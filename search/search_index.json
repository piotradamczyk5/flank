{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Flank Flank is a massively parallel Android and iOS test runner for Firebase Test Lab . Flank is YAML compatible with the gcloud CLI . Flank provides extra features to accelerate velocity and increase quality. Download https://github.com/Flank/flank/releases/latest/download/flank.jar Sponsors See docs/error_monitoring.md to disable Bugsnag error monitoring. Contributing Install JDK 15 (it works also correctly on the previous version, a newer version is not guaranteed to work properly): Oracle OpenJDK AdoptJDK Use JetBrains Toolbox to install IntelliJ IDEA Community Clone the repo git clone --recursive https://github.com/Flank/flank.git git submodule update --init --recursive updates the submodules Open build.gradle.kts in the main Flank base directory with IntelliJ IDEA Community , this will open the entire Flank mono repo test runner contributions can be made in the test_runner\\ subdirectory Features Test sharding Cost reporting Stability testing HTML report JUnit XML report Smart Flank Exit Codes Exit code Description 0 All tests passed 1 A general failure occurred. Possible causes include: a filename that does not exist or an HTTP/network error. 2 Usually indicates missing or wrong usage of flags, incorrect parameters, errors in config files. 10 At least one matrix not finished (usually a FTL internal error) or unexpected error occurred. 15 Firebase Test Lab could not determine if the test matrix passed or failed, because of an unexpected error. 18 The test environment for this test execution is not supported because of incompatible test dimensions. This error might occur if the selected Android API level is not supported by the selected device type. 19 The test matrix was canceled by the user. 20 A test infrastructure error occurred. CLI Flank supports CLI flags for each YAML parameter. The CLI flags are useful to selectively override YAML file values. Pass the --help flag to see the full documentation. For example: flank android run --help CLI flags work well with environment variables. You can override a value like this: flank android run --local-result-dir=$APP_NAME Flank configuration app, test, and xctestrun-file support ~ , environment variables, and globs ( , *) when resolving paths iOS example Run test_runner/flank.ios.yml with flank to verify iOS execution is working. ./gradlew clean test_runner:build test_runner:shadowJar java -jar ./test_runner/build/libs/flank-*.jar firebase test ios run # gcloud args match the official gcloud cli # https://cloud.google.com/sdk/gcloud/reference/alpha/firebase/test/ios/run gcloud : # -- GcloudYml -- ### Results Bucket ## The name of a Google Cloud Storage bucket where raw test results will be stored # results-bucket: tmp_flank ### Results Directory ## The name of a unique Google Cloud Storage object within the results bucket where raw test results will be stored ## (default: a timestamp with a random suffix). # results-dir: tmp ### Record Video flag ## Enable video recording during the test. Disabled by default. Use --record-video to enable. # record-video: true ### Timeout ## The max time this test execution can run before it is cancelled (default: 15m). ## It does not include any time necessary to prepare and clean up the target device. ## The maximum possible testing time is 45m on physical devices and 60m on virtual devices. ## The TIMEOUT units can be h, m, or s. If no unit is given, seconds are assumed. # timeout: 30m ### Asynchronous flag ## Invoke a test asynchronously without waiting for test results. # async: false ### Client Details ## A key-value map of additional details to attach to the test matrix. ## Arbitrary key-value pairs may be attached to a test matrix to provide additional context about the tests being run. ## When consuming the test results, such as in Cloud Functions or a CI system, ## these details can add additional context such as a link to the corresponding pull request. # client-details # key1: value1 # key2: value2 ### Network Profile ## The name of the network traffic profile, for example LTE, HSPA, etc, ## which consists of a set of parameters to emulate network conditions when running the test ## (default: no network shaping; see available profiles listed by the `flank test network-profiles list` command). ## This feature only works on physical devices. # network-profile: LTE ### Result History Name ## The history name for your test results (an arbitrary string label; default: the application's label from the APK manifest). ## All tests which use the same history name will have their results grouped together in the Firebase console in a time-ordered test history list. # results-history-name: android-history ### Number of Flaky Test Attempts ## The number of times a TestExecution should be re-attempted if one or more\\nof its test cases fail for any reason. ## The maximum number of reruns allowed is 10. Default is 0, which implies no reruns. # num-flaky-test-attempts: 0 ### Fail Fast ## If true, only a single attempt at most will be made to run each execution/shard in the matrix. ## Flaky test attempts are not affected. Normally, 2 or more attempts are made if a potential ## infrastructure issue is detected. This feature is for latency sensitive workloads. The ## incidence of execution failures may be significantly greater for fail-fast matrices and support ## is more limited because of that expectation. # fail-fast: false # -- IosGcloudYml -- ### IOS Test Package Path ## The path to the test package (a zip file containing the iOS app and XCTest files). ## The given path may be in the local filesystem or in Google Cloud Storage using a URL beginning with gs://. ## Note: any .xctestrun file in this zip file will be ignored if --xctestrun-file is specified. test : ./src/test/kotlin/ftl/fixtures/tmp/earlgrey_example.zip ### IOS XCTestrun File Path ## The path to an .xctestrun file that will override any .xctestrun file contained in the --test package. ## Because the .xctestrun file contains environment variables along with test methods to run and/or ignore, ## this can be useful for customizing or sharding test suites. The given path should be in the local filesystem. ## Note: this path should usually be pointing to the xctestrun file within the derived data folder ## For example ./derivedDataPath/Build/Products/EarlGreyExampleSwiftTests_iphoneos13.4-arm64e.xctestrun xctestrun-file : ./src/test/kotlin/ftl/fixtures/tmp/EarlGreyExampleSwiftTests_iphoneos13.4-arm64e.xctestrun ### Xcode Version ## The version of Xcode that should be used to run an XCTest. ## Defaults to the latest Xcode version supported in Firebase Test Lab. ## This Xcode version must be supported by all iOS versions selected in the test matrix. # xcode-version: 10.1 ### IOS Device Parameters ## A list of DIMENSION=VALUE pairs which specify a target device to test against. ## This flag may be repeated to specify multiple devices. ## The four device dimensions are: model, version, locale, and orientation. # device: # - model: iphone8 # version: 12.0 # locale: en # orientation: portrait # - model: iphonex # version: 12.0 # locale: es_ES # orientation: landscape ### Directories to Pull ## A list of paths that will be copied from the device's storage to the designated results bucket after the test ## is complete. These must be absolute paths under /private/var/mobile/Media or /Documents ## of the app under test. If the path is under an app's /Documents, it must be prefixed with the app's bundle id and a colon # directories-to-pull: # - /private/var/mobile/Media ### Other File paths ## A list of device-path=file-path pairs that specify the paths of the test device and the files you want pushed to the device prior to testing. ## Device paths should either be under the Media shared folder (e.g. prefixed with /private/var/mobile/Media) or ## within the documents directory of the filesystem of an app under test (e.g. /Documents). Device paths to app ## filesystems should be prefixed by the bundle ID and a colon. Source file paths may be in the local filesystem or in Google Cloud Storage (gs://\u2026). # other-files # com.my.app:/Documents/file.txt: local/file.txt # /private/var/mobile/Media/file.jpg: gs://bucket/file.jpg ### Additional IPA's ## List of up to 100 additional IPAs to install, in addition to the one being directly tested. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. # additional-ipas: # - gs://bucket/additional.ipa # - path/to/local/ipa/file.ipa ### Scenario Numbers ## A list of game-loop scenario numbers which will be run as part of the test (default: all scenarios). ## A maximum of 1024 scenarios may be specified in one test matrix, but the maximum number may also be limited by the overall test --timeout setting. # scenario-numbers: # - 1 # - 2 # - 3 ### Test type ## The type of iOS test to run. TYPE must be one of: xctest, game-loop. Default: xctest # type: xctest ### Application Path ## The path to the application archive (.ipa file) for game-loop testing. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## This flag is only valid when --type=game-loop is also set # app: # - gs://bucket/additional.ipa OR path/to/local/ipa/file.ipa ### Testing with Special Entitlements ## Enables testing special app entitlements. Re-signs an app having special entitlements with a new application-identifier. ## This currently supports testing Push Notifications (aps-environment) entitlement for up to one app in a project. ## Note: Because this changes the app's identifier, make sure none of the resources in your zip file contain direct references to the test app's bundle id. # test-special-entitlements: false flank : # -- FlankYml -- ### Max Test Shards ## test shards - the amount of groups to split the test suite into ## set to -1 to use one shard per test. default: 1 # max-test-shards: 1 ## Shard Time ## shard time - the amount of time tests within a shard should take ## when set to > 0, the shard count is dynamically set based on time up to the maximum limit defined by max-test-shards ## 2 minutes (120) is recommended. ## default: -1 (unlimited) # shard-time: -1 ### Number of Test Runs ## test runs - the amount of times to run the tests. ## 1 runs the tests once. 10 runs all the tests 10x # num-test-runs: 1 ### Smart Flank GCS Paths ## Google cloud storage path to store the JUnit XML results from the last run. ## NOTE: Empty results will not be uploaded # smart-flank-gcs-path: gs://tmp_flank/flank/test_app_ios.xml ### Smart Flank Disable Upload flag ## Disables smart flank JUnit XML uploading. Useful for preventing timing data from being updated. ## Default: false # smart-flank-disable-upload: false ### Use Average Test Time For New Tests flag ## Enable using average time from previous tests duration when using SmartShard and tests did not run before. ## Default: false # use-average-test-time-for-new-tests: true ### Default Test Time ## Set default test time used for calculating shards. ## Default: 120.0 # default-test-time: 15 ### Default Class Test Time ## Set default test time (in seconds) used for calculating shards of parametrized classes when previous tests results are not available. ## Default test time for classes should be different from the default time for test ## Default: 240.0 # default-class-test-time: 30 ### Disable Sharding flag ## Disables sharding. Useful for parameterized tests. # disable-sharding: false ### Test targets always Run ## always run - these tests are inserted at the beginning of every shard ## Execution order is not guaranteed by Flank. Users are responsible for configuring their own device test runner logic. # test-targets-always-run: # - className/testName ### Files to Download ## regex is matched against bucket paths, for example: 2019-01-09_00:18:07.314000_hCMY/shard_0/EarlGreyExampleSwiftTests_iphoneos12.1-arm64e.xctestrun # files-to-download: # - .*\\.mp4$ # -- IosFlankYml -- ### Test Targets ## test targets - a list of tests to run. omit to run all tests. # test-targets: # - className/testName ### Billing Project Name ## The billing enabled Google Cloud Platform project name to use # project: flank-open-source ### Local Result Directory Storage ## Local folder to store the test result. Folder is DELETED before each run to ensure only artifacts from the new run are saved. # local-result-dir: flank ### Run Timeout ## The max time this test run can execute before it is cancelled (default: unlimited). # run-timeout: 60m ### Keep File Path flag ## Keeps the full path of downloaded files. Required when file names are not unique. ## Default: false # keep-file-path: false ### Ignore Failed Tests flag ## Terminate with exit code 0 when there are failed tests. ## Useful for Fladle and other gradle plugins that don't expect the process to have a non-zero exit code. ## The JUnit XML is used to determine failure. (default: false) # ignore-failed-tests: true ### Output Style flag ## Output style of execution status. May be one of [verbose, multi, single, compact]. ## For runs with only one test execution the default value is 'verbose', in other cases ## 'multi' is used as the default. The output style 'multi' is not displayed correctly on consoles ## which don't support ansi codes, to avoid corrupted output use single or verbose. ## The output style `compact` is used to produce less detailed output, it prints just Args, test and matrix count, weblinks, cost, and result reports. # output-style: single ### Full Junit Result flag ## Enable create additional local junit result on local storage with failure nodes on passed flaky tests. # full-junit-result: false ### Disable Result Upload flag ## Disables flank results upload on gcloud storage. ## Default: false # disable-results-upload: false Android example Run test_runner/flank.yml with flank to verify Android execution is working. ./gradlew clean test_runner:build test_runner:shadowJar java -jar ./test_runner/build/libs/flank-*.jar firebase test android run # gcloud args match the official gcloud cli # See the docs for full gcloud details https://cloud.google.com/sdk/gcloud/reference/firebase/test/android/run gcloud : # -- GcloudYml -- ### Result Bucket ## The name of a Google Cloud Storage bucket where raw test results will be stored # results-bucket: tmp_flank ### Result Directory ## The name of a unique Google Cloud Storage object within the results bucket where raw test results will be stored ## (default: a timestamp with a random suffix). # results-dir: tmp ### Record Video flag ## Enable video recording during the test. Disabled by default. Use --record-video to enable. # record-video: true ### Timeout ## The max time this test execution can run before it is cancelled (default: 15m). ## It does not include any time necessary to prepare and clean up the target device. ## The maximum possible testing time is 45m on physical devices and 60m on virtual devices. ## The TIMEOUT units can be h, m, or s. If no unit is given, seconds are assumed. # timeout: 30m ### Asynchronous flag ## Invoke a test asynchronously without waiting for test results. # async: false ### Client Details ## A key-value map of additional details to attach to the test matrix. ## Arbitrary key-value pairs may be attached to a test matrix to provide additional context about the tests being run. ## When consuming the test results, such as in Cloud Functions or a CI system, ## these details can add additional context such as a link to the corresponding pull request. # client-details # key1: value1 # key2: value2 ### Network Profile ## The name of the network traffic profile, for example LTE, HSPA, etc, ## which consists of a set of parameters to emulate network conditions when running the test ## (default: no network shaping; see available profiles listed by the `flank test network-profiles list` command). ## This feature only works on physical devices. # network-profile: LTE ### Result History Name ## The history name for your test results (an arbitrary string label; default: the application's label from the APK manifest). ## All tests which use the same history name will have their results grouped together in the Firebase console in a time-ordered test history list. # results-history-name: android-history ### Number of Flaky Test Attempts ## The number of times a TestExecution should be re-attempted if one or more\\nof its test cases fail for any reason. ## The maximum number of reruns allowed is 10. Default is 0, which implies no reruns. # num-flaky-test-attempts: 0 ### Fail Fast ## If true, only a single attempt at most will be made to run each execution/shard in the matrix. ## Flaky test attempts are not affected. Normally, 2 or more attempts are made if a potential ## infrastructure issue is detected. This feature is for latency sensitive workloads. The ## incidence of execution failures may be significantly greater for fail-fast matrices and support ## is more limited because of that expectation. # fail-fast: false # -- AndroidGcloudYml -- ## Android Application Path ## The path to the application binary file. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## Android App Bundles are specified as .aab, all other files are assumed to be APKs. app : ../test_projects/android/apks/app-debug.apk ### Android Binary File Path ## The path to the binary file containing instrumentation tests. ## The given path may be in the local filesystem or in Google Cloud Storage using a URL beginning with gs://. test : ../test_projects/android/apks/app-debug-androidTest.apk ### Additional APK's ## A list of up to 100 additional APKs to install, in addition to those being directly tested. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. # additional-apks: additional-apk1.apk,additional-apk2.apk,additional-apk3.apk ### Auto Google Login flag ## Automatically log into the test device using a preconfigured Google account before beginning the test. ## Disabled by default. Use --auto-google-login to enable. # auto-google-login: true ### Use Orchestrator Flag ## Whether each test runs in its own Instrumentation instance with the Android Test Orchestrator ## (default: Orchestrator is used). Disable with --no-use-orchestrator. ## See https://developer.android.com/training/testing/junit-runner.html#using-android-test-orchestrator # use-orchestrator: true ### Environment Variables ## A comma-separated, key=value map of environment variables and their desired values. This flag is repeatable. ## The environment variables are mirrored as extra options to the am instrument -e KEY1 VALUE1 \u2026 command and ## passed to your test runner (typically AndroidJUnitRunner) # environment-variables: # coverage: true # coverageFilePath: /sdcard/ # clearPackageData: true ### Directories to Pull ## A list of paths that will be copied from the device's storage to the designated results bucket after the test ## is complete. These must be absolute paths under /sdcard or /data/local/tmp # directories-to-pull: # - /sdcard/ ### Grant Permissions flag ## Whether to grant runtime permissions on the device before the test begins. ## By default, all permissions are granted. PERMISSIONS must be one of: all, none # grant-permissions: all ### Test Type ## The type of test to run. TYPE must be one of: instrumentation, robo, game-loop. # type: instrumentation ### Other Files ## A list of device-path: file-path pairs that indicate the device paths to push files to the device before starting tests, and the paths of files to push. ## Device paths must be under absolute, whitelisted paths (${EXTERNAL_STORAGE}, or ${ANDROID_DATA}/local/tmp). ## Source file paths may be in the local filesystem or in Google Cloud Storage (gs://\u2026). # other-files # - /sdcard/dir1/file1.txt: local/file.txt # - /sdcard/dir2/file2.jpg: gs://bucket/file.jpg ### OBB Files ## A list of one or two Android OBB file names which will be copied to each test device before the tests will run (default: None). ## Each OBB file name must conform to the format as specified by Android (e.g. [main|patch].0300110.com.example.android.obb) and will be installed into <shared-storage>/Android/obb/<package-name>/ on the test device. # obb-files: # - local/file/path/test1.obb # - local/file/path/test2.obb ### Scenario Numbers ## A list of game-loop scenario numbers which will be run as part of the test (default: all scenarios). ## A maximum of 1024 scenarios may be specified in one test matrix, but the maximum number may also be limited by the overall test --timeout setting. # scenario-numbers: # - 1 # - 2 # - 3 ### Scenario Labels ## A list of game-loop scenario labels (default: None). Each game-loop scenario may be labeled in the APK manifest file with one or more arbitrary strings, creating logical groupings (e.g. GPU_COMPATIBILITY_TESTS). ## If --scenario-numbers and --scenario-labels are specified together, Firebase Test Lab will first execute each scenario from --scenario-numbers. ## It will then expand each given scenario label into a list of scenario numbers marked with that label, and execute those scenarios. # scenario-labels: # - label1 # - label2 ### OBB filenames ## A list of OBB required filenames. OBB file name must conform to the format as specified by Android e.g. ## [main|patch].0300110.com.example.android.obb which will be installed into <shared-storage>/Android/obb/<package-name>/ on the device. # obb-names: # - [main|patch].<VERSION>.com.example.android.obb ### Performance Metric flag ## Monitor and record performance metrics: CPU, memory, network usage, and FPS (game-loop only). ## Disabled by default. Use --performance-metrics to enable. # performance-metrics: true ### Number of Uniform Shards ## Specifies the number of shards into which you want to evenly distribute test cases. ## The shards are run in parallel on separate devices. For example, ## if your test execution contains 20 test cases and you specify four shards, each shard executes five test cases. ## The number of shards should be less than the total number of test cases. ## The number of shards specified must be >= 1 and <= 50. ## This option cannot be used along max-test-shards and is not compatible with smart sharding. ## If you want to take benefits of smart sharding use max-test-shards instead. ## default: null # num-uniform-shards: 50 ### Instrumentation Test Runner Class ## The fully-qualified Java class name of the instrumentation test runner ## (default: the last name extracted from the APK manifest). # test-runner-class: com.foo.TestRunner ### Test Targets ## A list of one or more test target filters to apply (default: run all test targets). ## Each target filter must be fully qualified with the package name, class name, or test annotation desired. ## Supported test filters by am instrument -e \u2026 include: ## class, notClass, size, annotation, notAnnotation, package, notPackage, testFile, notTestFile ## See https://developer.android.com/reference/android/support/test/runner/AndroidJUnitRunner for more information. # test-targets: # - class com.example.app.ExampleUiTest#testPasses ### Robo Directives ## A map of robo_directives that you can use to customize the behavior of Robo test. ## The type specifies the action type of the directive, which may take on values click, text or ignore. ## If no type is provided, text will be used by default. ## Each key should be the Android resource name of a target UI element and each value should be the text input for that element. ## Values are only permitted for text type elements, so no value should be specified for click and ignore type elements. # robo-directives: # \"text:input_resource_name\": message # \"click:button_resource_name\": \"\" ### Robo Scripts ## The path to a Robo Script JSON file. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## You can guide the Robo test to perform specific actions by recording a Robo Script in Android Studio and then specifying this argument. ## Learn more at https://firebase.google.com/docs/test-lab/robo-ux-test#scripting. # robo-script: path_to_robo_script ### Android Device Parameters ## A list of DIMENSION=VALUE pairs which specify a target device to test against. ## This flag may be repeated to specify multiple devices. ## The four device dimensions are: model, version, locale, and orientation. # device: # - model: NexusLowRes # version: 28 # locale: en # orientation: portrait # - model: NexusLowRes # version: 27 ### test-targets-for-shard ## Specifies a group of packages, classes, and/or test cases to run in each shard (a group of test cases). ## The shards are run in parallel on separate devices. You can repeat this flag up to 50 times to specify multiple shards when one or more physical devices are selected, ## or up to 500 times when no physical devices are selected. ## Note: If you include the flags environment-variable or test-targets when running test-targets-for-shard, the flags are applied to all the shards you create. # test-target-for-shard: # - package com.package1.for.shard1 # - class com.package2.for.shard2.Class flank : # -- FlankYml -- ### Max Test Shards ## test shards - the amount of groups to split the test suite into ## set to -1 to use one shard per test. default: 1 # max-test-shards: 1 ### Shard Time ## shard time - the amount of time tests within a shard should take ## when set to > 0, the shard count is dynamically set based on time up to the maximum limit defined by max-test-shards ## 2 minutes (120) is recommended. ## default: -1 (unlimited) # shard-time: -1 ### Number of Test Runs ## test runs - the amount of times to run the tests. ## 1 runs the tests once. 10 runs all the tests 10x # num-test-runs: 1 ### Smart Flank GCS Path ## Google cloud storage path where the JUnit XML results from the last run is stored. ## NOTE: Empty results will not be uploaded # smart-flank-gcs-path: gs://tmp_flank/tmp/JUnitReport.xml ### Smart Flank Upload Disable flag ## Disables smart flank JUnit XML uploading. Useful for preventing timing data from being updated. ## Default: false # smart-flank-disable-upload: false ### Use Average Test Time for New Tests flag ## Enable using average time from previous tests duration when using SmartShard and tests did not run before. ## Default: false # use-average-test-time-for-new-tests: true ### Default Test Time ## Set default test time used for calculating shards. ## Default: 120.0 # default-test-time: 15 ### Default Class Test Time ## Set default test time (in seconds) used for calculating shards of parametrized classes when previous tests results are not available. ## Default test time for classes should be different from the default time for test ## Default: 240.0 # default-class-test-time: 30 ### Disable Sharding flag ## Disables sharding. Useful for parameterized tests. # disable-sharding: false ### Test Targets Always Run ## always run - these tests are inserted at the beginning of every shard ## Execution order is not guaranteed by Flank. Users are responsible for configuring their own device test runner logic. # test-targets-always-run: # - class com.example.app.ExampleUiTest#testPasses ### Files to Download ## regex is matched against bucket paths, for example: 2019-01-09_00:13:06.106000_YCKl/shard_0/NexusLowRes-28-en-portrait/bugreport.txt # files-to-download: # - .*\\.mp4$ ### Billing Project Name ## The billing enabled Google Cloud Platform project name to use # project: flank-open-source ### Local Results Directory ## Local folder to store the test result. Folder is DELETED before each run to ensure only artifacts from the new run are saved. # local-result-dir: flank ### Keep File Path flag ## Keeps the full path of downloaded files. Required when file names are not unique. ## Default: false # keep-file-path: false ### Additional App/Test APKS ## Include additional app/test apk pairs in the run. Apks are unique by just filename and not by path! ## If app is omitted, then the top level app is used for that pair. # additional-app-test-apks: # - app: ../test_projects/android/apks/app-debug.apk # test: ../test_projects/android/apks/app1-debug-androidTest.apk # - test: ../test_projects/android/apks/app2-debug-androidTest.apk ### Run Timeout ## The max time this test run can execute before it is cancelled (default: unlimited). # run-timeout: 60m ### Ignore Failed Test flag ## Terminate with exit code 0 when there are failed tests. ## Useful for Fladle and other gradle plugins that don't expect the process to have a non-zero exit code. ## The JUnit XML is used to determine failure. (default: false) # ignore-failed-tests: true ### Legacy Junit Results flag ## Flank provides two ways for parsing junit xml results. ## New way uses google api instead of merging xml files, but can generate slightly different output format. ## This flag allows fallback for legacy xml junit results parsing ## Currently available for android, iOS still uses only legacy way. # legacy-junit-result: false ### Output Style flag ## Output style of execution status. May be one of [verbose, multi, single, compact]. ## For runs with only one test execution the default value is 'verbose', in other cases ## 'multi' is used as the default. The output style 'multi' is not displayed correctly on consoles ## which don't support ansi codes, to avoid corrupted output use single or verbose. ## The output style `compact` is used to produce less detailed output, it prints just Args, test and matrix count, weblinks, cost, and result reports. # output-style: single ### Full Junit Result flag ## Enable create additional local junit result on local storage with failure nodes on passed flaky tests. # full-junit-result: false ### Disable Results Upload flag ## Disables flank results upload on gcloud storage. ## Default: false # disable-results-upload: false Android code coverage Update your app's build.gradle to build with coverage and use orchestrator. A custom gradle task is defined to generate the coverage report. def coverageEnabled = project.hasProperty('coverage') android { defaultConfig { testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\" // runs pm clear after each test invocation testInstrumentationRunnerArguments clearPackageData: 'true' } buildTypes { debug { testCoverageEnabled true } } // https://google.github.io/android-gradle-dsl/current/com.android.build.gradle.internal.dsl.TestOptions.html#com.android.build.gradle.internal.dsl.TestOptions:animationsDisabled testOptions { execution 'ANDROIDX_TEST_ORCHESTRATOR' animationsDisabled = true } } dependencies { androidTestUtil 'androidx.test:orchestrator:1.1.1' androidTestImplementation(\"androidx.test:runner:1.1.1\") androidTestImplementation(\"androidx.test.ext:junit:1.1.0\") androidTestImplementation(\"androidx.test.ext:junit-ktx:1.1.0\") androidTestImplementation(\"androidx.test.ext:truth:1.1.0\") androidTestImplementation(\"androidx.test.espresso.idling:idling-concurrent:3.1.1\") androidTestImplementation(\"androidx.test.espresso.idling:idling-net:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-accessibility:3.1.1\") androidTestImplementation(\"androidx.test:rules:1.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-core:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-contrib:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-idling-resource:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-intents:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-web:3.1.1\") } if (coverageEnabled) { // gradle -Pcoverage firebaseJacoco task firebaseJacoco(type: JacocoReport) { group = \"Reporting\" description = \"Generate Jacoco coverage reports for Firebase test lab.\" def excludes = [ '**/R.class', '**/R$*.class', '**/BuildConfig.*', \"**/androidx\"] def javaClasses = fileTree(dir: \"${project.buildDir}/intermediates/javac/debug/classes\", excludes: excludes) def kotlinClasses = fileTree(dir: \"${project.buildDir}/tmp/kotlin-classes/debug\", excludes: excludes) getClassDirectories().setFrom(files([javaClasses, kotlinClasses])) getSourceDirectories().setFrom(files([ 'src/main/java', 'src/main/kotlin', 'src/androidTest/java', 'src/androidTest/kotlin'])) def ecFiles = project.fileTree(dir: '..', include: 'results/coverage_ec/**/sdcard/*.ec') ecFiles.forEach { println(\"Reading in $it\") } getExecutionData().setFrom(ecFiles) reports { html { enabled true } xml { enabled false } } } } Starting from Android Marshmallow we must grant runtime permissions to write to external storage. Following snippet in test class solves that issue. If you want to get coverage files when using orchestrator, you must set this Rule for each test class. import androidx.test.rule.GrantPermissionRule ; import static android.Manifest.permission.READ_EXTERNAL_STORAGE ; import static android.Manifest.permission.WRITE_EXTERNAL_STORAGE ; class MyEspressoTest { @Rule GrantPermissionRule grantPermissionRule = GrantPermissionRule . grant ( READ_EXTERNAL_STORAGE , WRITE_EXTERNAL_STORAGE ); // other configuration and tests } Here's an example flank.yml. Note that `coverage` and `coverageFilePath` must be set when using orchestrator with coverage. `coverageFile` is not used. Orchestrator will generate one coverage file per test. `coverageFilePath` must be a directory, not a file. gcloud : app : ./app/build/outputs/apk/debug/app-debug.apk test : ./app/build/outputs/apk/androidTest/debug/app-debug-androidTest.apk environment-variables : coverage : true coverageFilePath : /sdcard/ clearPackageData : true directories-to-pull : - /sdcard/ # use a named results dir that's used by the gradle task results-dir : coverage_ec flank : disableSharding : true files-to-download : - .*/sdcard/[^/]+\\.ec$ - Build the app with coverage: `./gradlew -Pcoverage build` - Run flank `flank android run` - Generate the report `./gradlew -Pcoverage firebaseJacoco` - Open the report in `./build/reports/jacoco/firebaseJacoco/html/index.html` CI integration Download Flank from GitHub releases. Stable. Get the latest stable version number and replace the XXX with the version number. wget --quiet https://github.com/Flank/flank/releases/download/vXXX/flank.jar -O ./flank.jar java -jar ./flank.jar android run Snapshot (published after every commit) wget --quiet https://github.com/Flank/flank/releases/download/flank_snapshot/flank.jar -O ./flank.jar java -jar ./flank.jar android run In CI, it may be useful to generate the file via a shell script: cat << 'EOF' > ./flank.yml gcloud: app: ../../test_projects/android/apks/app-debug.apk test: ../../test_projects/android/apks/app-debug-androidTest.apk EOF Circle CI Circle CI has a firebase testlab orb that supports Flank. Bitrise Bitrise has an official flank step . Gradle Plugin Fladle is a Gradle plugin for Flank that provides DSL configuration and task based execution. Flank on Windows In order to build or run Flank using Windows please follow guide of building/running it using Windows WSL. Native support is not currently supported. Authenticate with a Google account Run flank auth login . Flank will save the credential to ~/.flank . Google account authentication allows each person to have a unique non-shared credential. A service account is still recommended for CI. Authenticate with a service account Follow the test lab docs to create a service account. - Save the credential to $HOME/.config/gcloud/application_default_credentials.json or set GOOGLE_APPLICATION_CREDENTIALS when using a custom path. - Set the project id in flank.yml or set the GOOGLE_CLOUD_PROJECT environment variable. - (Since 21.01) if projectId is not set in a config yml file, flank uses the first available project ID among the following sources: 1. The project ID specified in the JSON credentials file pointed by the GOOGLE_APPLICATION_CREDENTIALS environment variable fladle 1. The project ID specified by the GOOGLE_CLOUD_PROJECT environment variable 1. The project ID specified in the JSON credentials file $HOME/.config/gcloud/application_default_credentials.json For continuous integration, base64 encode the credential as GCLOUD_KEY . Then write the file using a shell script. Note that gcloud CLI does not need to be installed. Flank works without any dependency on gcloud CLI. Encode JSON locally. base64 -i \" $HOME /.config/gcloud/application_default_credentials.json\" | pbcopy Then in CI decode the JSON. GCLOUD_DIR = \" $HOME /.config/gcloud/\" mkdir -p \" $GCLOUD_DIR \" echo \" $GCLOUD_KEY \" | base64 --decode > \" $GCLOUD_DIR /application_default_credentials.json\" Running with gcloud directly flank.yml is compatible with the gcloud CLI. gcloud firebase test android run flank.yml:gcloud gcloud alpha firebase test ios run flank.ios.yml:gcloud NOTE: You will need to activate gcloud's service account for the above commands to work. Doctor Use the doctor command to check for errors in the YAML. flank firebase test android doctor flank firebase test ios doctor Check version Flank supports printing the current version. $ flank -v v3.0-SNAPSHOT Maven You can consume Flank via maven. See the maven repo for all supported versions. repositories { maven(url = \"https://dl.bintray.com/flank/maven\") } dependencies { compile(\"flank:flank:flank_snapshot\") } or GitHub packages Groovy dependencies { implementation \"com.github.flank:flank:<latest version>\" } Kotlin dependencies { implementation ( \"com.github.flank:flank:<latest version>\" ) } Gradle Enterprise Export API It is possible to fetch metrics from Gradle builds. For detailed info please visit Gradle Export API and flank's example gradle-export-api . FAQ 1) > Access Not Configured. Cloud Tool Results API has not been used in project 764086051850 before or it is disabled. This error means authentication hasn't been setup properly. See `Authenticate with a service account` in this readme. 2) > How do I use Flank without typing long commands? Add Flank ' s [ bash helper folder ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / ) to your $PATH environment variable . This will allow you to call the shell scripts in that helper folder from anywhere . With the [ flank ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / flank ) shell script , you can use `flank` instead of `java -jar flank.jar` . Examples: - `flank android run` - `flank ios run` With the [ update_flank . sh ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / update_flank . sh ) shell script , you can rebuild `flank.jar` . 3) > Test run failed to complete. Expected 786 tests, received 660 Try setting `use-orchestrator: false`. Parameterized tests [are not compatible with orchestrator](https://stackoverflow.com/questions/48735268/unable-to-run-parameterized-tests-with-android-test-orchestrator). Flank uses [orchestrator by default on Android.](https://developer.android.com/training/testing/junit-runner) 4) > I have an issue when attempting to sync the Flank Gradle project Task 'prepareKotlinBuildScriptModel' not found in project ':test_runner'. or similar - Make sure you do not change any module specific settings for Gradle - Clear IDE cache using `File > Invalidate Caches / Restart` - Re - import project using root `build.gradle.kts` - Sync project again 5) > Does Flank support Cucumber? Please check document for more info Resources Instrumenting Firebase Test Lab","title":"Home"},{"location":"#flank","text":"Flank is a massively parallel Android and iOS test runner for Firebase Test Lab . Flank is YAML compatible with the gcloud CLI . Flank provides extra features to accelerate velocity and increase quality.","title":"Flank"},{"location":"#download","text":"https://github.com/Flank/flank/releases/latest/download/flank.jar","title":"Download"},{"location":"#sponsors","text":"See docs/error_monitoring.md to disable Bugsnag error monitoring.","title":"Sponsors"},{"location":"#contributing","text":"Install JDK 15 (it works also correctly on the previous version, a newer version is not guaranteed to work properly): Oracle OpenJDK AdoptJDK Use JetBrains Toolbox to install IntelliJ IDEA Community Clone the repo git clone --recursive https://github.com/Flank/flank.git git submodule update --init --recursive updates the submodules Open build.gradle.kts in the main Flank base directory with IntelliJ IDEA Community , this will open the entire Flank mono repo test runner contributions can be made in the test_runner\\ subdirectory","title":"Contributing"},{"location":"#features","text":"Test sharding Cost reporting Stability testing HTML report JUnit XML report Smart Flank","title":"Features"},{"location":"#exit-codes","text":"Exit code Description 0 All tests passed 1 A general failure occurred. Possible causes include: a filename that does not exist or an HTTP/network error. 2 Usually indicates missing or wrong usage of flags, incorrect parameters, errors in config files. 10 At least one matrix not finished (usually a FTL internal error) or unexpected error occurred. 15 Firebase Test Lab could not determine if the test matrix passed or failed, because of an unexpected error. 18 The test environment for this test execution is not supported because of incompatible test dimensions. This error might occur if the selected Android API level is not supported by the selected device type. 19 The test matrix was canceled by the user. 20 A test infrastructure error occurred.","title":"Exit Codes"},{"location":"#cli","text":"Flank supports CLI flags for each YAML parameter. The CLI flags are useful to selectively override YAML file values. Pass the --help flag to see the full documentation. For example: flank android run --help CLI flags work well with environment variables. You can override a value like this: flank android run --local-result-dir=$APP_NAME","title":"CLI"},{"location":"#flank-configuration","text":"app, test, and xctestrun-file support ~ , environment variables, and globs ( , *) when resolving paths","title":"Flank configuration"},{"location":"#ios-example","text":"Run test_runner/flank.ios.yml with flank to verify iOS execution is working. ./gradlew clean test_runner:build test_runner:shadowJar java -jar ./test_runner/build/libs/flank-*.jar firebase test ios run # gcloud args match the official gcloud cli # https://cloud.google.com/sdk/gcloud/reference/alpha/firebase/test/ios/run gcloud : # -- GcloudYml -- ### Results Bucket ## The name of a Google Cloud Storage bucket where raw test results will be stored # results-bucket: tmp_flank ### Results Directory ## The name of a unique Google Cloud Storage object within the results bucket where raw test results will be stored ## (default: a timestamp with a random suffix). # results-dir: tmp ### Record Video flag ## Enable video recording during the test. Disabled by default. Use --record-video to enable. # record-video: true ### Timeout ## The max time this test execution can run before it is cancelled (default: 15m). ## It does not include any time necessary to prepare and clean up the target device. ## The maximum possible testing time is 45m on physical devices and 60m on virtual devices. ## The TIMEOUT units can be h, m, or s. If no unit is given, seconds are assumed. # timeout: 30m ### Asynchronous flag ## Invoke a test asynchronously without waiting for test results. # async: false ### Client Details ## A key-value map of additional details to attach to the test matrix. ## Arbitrary key-value pairs may be attached to a test matrix to provide additional context about the tests being run. ## When consuming the test results, such as in Cloud Functions or a CI system, ## these details can add additional context such as a link to the corresponding pull request. # client-details # key1: value1 # key2: value2 ### Network Profile ## The name of the network traffic profile, for example LTE, HSPA, etc, ## which consists of a set of parameters to emulate network conditions when running the test ## (default: no network shaping; see available profiles listed by the `flank test network-profiles list` command). ## This feature only works on physical devices. # network-profile: LTE ### Result History Name ## The history name for your test results (an arbitrary string label; default: the application's label from the APK manifest). ## All tests which use the same history name will have their results grouped together in the Firebase console in a time-ordered test history list. # results-history-name: android-history ### Number of Flaky Test Attempts ## The number of times a TestExecution should be re-attempted if one or more\\nof its test cases fail for any reason. ## The maximum number of reruns allowed is 10. Default is 0, which implies no reruns. # num-flaky-test-attempts: 0 ### Fail Fast ## If true, only a single attempt at most will be made to run each execution/shard in the matrix. ## Flaky test attempts are not affected. Normally, 2 or more attempts are made if a potential ## infrastructure issue is detected. This feature is for latency sensitive workloads. The ## incidence of execution failures may be significantly greater for fail-fast matrices and support ## is more limited because of that expectation. # fail-fast: false # -- IosGcloudYml -- ### IOS Test Package Path ## The path to the test package (a zip file containing the iOS app and XCTest files). ## The given path may be in the local filesystem or in Google Cloud Storage using a URL beginning with gs://. ## Note: any .xctestrun file in this zip file will be ignored if --xctestrun-file is specified. test : ./src/test/kotlin/ftl/fixtures/tmp/earlgrey_example.zip ### IOS XCTestrun File Path ## The path to an .xctestrun file that will override any .xctestrun file contained in the --test package. ## Because the .xctestrun file contains environment variables along with test methods to run and/or ignore, ## this can be useful for customizing or sharding test suites. The given path should be in the local filesystem. ## Note: this path should usually be pointing to the xctestrun file within the derived data folder ## For example ./derivedDataPath/Build/Products/EarlGreyExampleSwiftTests_iphoneos13.4-arm64e.xctestrun xctestrun-file : ./src/test/kotlin/ftl/fixtures/tmp/EarlGreyExampleSwiftTests_iphoneos13.4-arm64e.xctestrun ### Xcode Version ## The version of Xcode that should be used to run an XCTest. ## Defaults to the latest Xcode version supported in Firebase Test Lab. ## This Xcode version must be supported by all iOS versions selected in the test matrix. # xcode-version: 10.1 ### IOS Device Parameters ## A list of DIMENSION=VALUE pairs which specify a target device to test against. ## This flag may be repeated to specify multiple devices. ## The four device dimensions are: model, version, locale, and orientation. # device: # - model: iphone8 # version: 12.0 # locale: en # orientation: portrait # - model: iphonex # version: 12.0 # locale: es_ES # orientation: landscape ### Directories to Pull ## A list of paths that will be copied from the device's storage to the designated results bucket after the test ## is complete. These must be absolute paths under /private/var/mobile/Media or /Documents ## of the app under test. If the path is under an app's /Documents, it must be prefixed with the app's bundle id and a colon # directories-to-pull: # - /private/var/mobile/Media ### Other File paths ## A list of device-path=file-path pairs that specify the paths of the test device and the files you want pushed to the device prior to testing. ## Device paths should either be under the Media shared folder (e.g. prefixed with /private/var/mobile/Media) or ## within the documents directory of the filesystem of an app under test (e.g. /Documents). Device paths to app ## filesystems should be prefixed by the bundle ID and a colon. Source file paths may be in the local filesystem or in Google Cloud Storage (gs://\u2026). # other-files # com.my.app:/Documents/file.txt: local/file.txt # /private/var/mobile/Media/file.jpg: gs://bucket/file.jpg ### Additional IPA's ## List of up to 100 additional IPAs to install, in addition to the one being directly tested. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. # additional-ipas: # - gs://bucket/additional.ipa # - path/to/local/ipa/file.ipa ### Scenario Numbers ## A list of game-loop scenario numbers which will be run as part of the test (default: all scenarios). ## A maximum of 1024 scenarios may be specified in one test matrix, but the maximum number may also be limited by the overall test --timeout setting. # scenario-numbers: # - 1 # - 2 # - 3 ### Test type ## The type of iOS test to run. TYPE must be one of: xctest, game-loop. Default: xctest # type: xctest ### Application Path ## The path to the application archive (.ipa file) for game-loop testing. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## This flag is only valid when --type=game-loop is also set # app: # - gs://bucket/additional.ipa OR path/to/local/ipa/file.ipa ### Testing with Special Entitlements ## Enables testing special app entitlements. Re-signs an app having special entitlements with a new application-identifier. ## This currently supports testing Push Notifications (aps-environment) entitlement for up to one app in a project. ## Note: Because this changes the app's identifier, make sure none of the resources in your zip file contain direct references to the test app's bundle id. # test-special-entitlements: false flank : # -- FlankYml -- ### Max Test Shards ## test shards - the amount of groups to split the test suite into ## set to -1 to use one shard per test. default: 1 # max-test-shards: 1 ## Shard Time ## shard time - the amount of time tests within a shard should take ## when set to > 0, the shard count is dynamically set based on time up to the maximum limit defined by max-test-shards ## 2 minutes (120) is recommended. ## default: -1 (unlimited) # shard-time: -1 ### Number of Test Runs ## test runs - the amount of times to run the tests. ## 1 runs the tests once. 10 runs all the tests 10x # num-test-runs: 1 ### Smart Flank GCS Paths ## Google cloud storage path to store the JUnit XML results from the last run. ## NOTE: Empty results will not be uploaded # smart-flank-gcs-path: gs://tmp_flank/flank/test_app_ios.xml ### Smart Flank Disable Upload flag ## Disables smart flank JUnit XML uploading. Useful for preventing timing data from being updated. ## Default: false # smart-flank-disable-upload: false ### Use Average Test Time For New Tests flag ## Enable using average time from previous tests duration when using SmartShard and tests did not run before. ## Default: false # use-average-test-time-for-new-tests: true ### Default Test Time ## Set default test time used for calculating shards. ## Default: 120.0 # default-test-time: 15 ### Default Class Test Time ## Set default test time (in seconds) used for calculating shards of parametrized classes when previous tests results are not available. ## Default test time for classes should be different from the default time for test ## Default: 240.0 # default-class-test-time: 30 ### Disable Sharding flag ## Disables sharding. Useful for parameterized tests. # disable-sharding: false ### Test targets always Run ## always run - these tests are inserted at the beginning of every shard ## Execution order is not guaranteed by Flank. Users are responsible for configuring their own device test runner logic. # test-targets-always-run: # - className/testName ### Files to Download ## regex is matched against bucket paths, for example: 2019-01-09_00:18:07.314000_hCMY/shard_0/EarlGreyExampleSwiftTests_iphoneos12.1-arm64e.xctestrun # files-to-download: # - .*\\.mp4$ # -- IosFlankYml -- ### Test Targets ## test targets - a list of tests to run. omit to run all tests. # test-targets: # - className/testName ### Billing Project Name ## The billing enabled Google Cloud Platform project name to use # project: flank-open-source ### Local Result Directory Storage ## Local folder to store the test result. Folder is DELETED before each run to ensure only artifacts from the new run are saved. # local-result-dir: flank ### Run Timeout ## The max time this test run can execute before it is cancelled (default: unlimited). # run-timeout: 60m ### Keep File Path flag ## Keeps the full path of downloaded files. Required when file names are not unique. ## Default: false # keep-file-path: false ### Ignore Failed Tests flag ## Terminate with exit code 0 when there are failed tests. ## Useful for Fladle and other gradle plugins that don't expect the process to have a non-zero exit code. ## The JUnit XML is used to determine failure. (default: false) # ignore-failed-tests: true ### Output Style flag ## Output style of execution status. May be one of [verbose, multi, single, compact]. ## For runs with only one test execution the default value is 'verbose', in other cases ## 'multi' is used as the default. The output style 'multi' is not displayed correctly on consoles ## which don't support ansi codes, to avoid corrupted output use single or verbose. ## The output style `compact` is used to produce less detailed output, it prints just Args, test and matrix count, weblinks, cost, and result reports. # output-style: single ### Full Junit Result flag ## Enable create additional local junit result on local storage with failure nodes on passed flaky tests. # full-junit-result: false ### Disable Result Upload flag ## Disables flank results upload on gcloud storage. ## Default: false # disable-results-upload: false","title":"iOS example"},{"location":"#android-example","text":"Run test_runner/flank.yml with flank to verify Android execution is working. ./gradlew clean test_runner:build test_runner:shadowJar java -jar ./test_runner/build/libs/flank-*.jar firebase test android run # gcloud args match the official gcloud cli # See the docs for full gcloud details https://cloud.google.com/sdk/gcloud/reference/firebase/test/android/run gcloud : # -- GcloudYml -- ### Result Bucket ## The name of a Google Cloud Storage bucket where raw test results will be stored # results-bucket: tmp_flank ### Result Directory ## The name of a unique Google Cloud Storage object within the results bucket where raw test results will be stored ## (default: a timestamp with a random suffix). # results-dir: tmp ### Record Video flag ## Enable video recording during the test. Disabled by default. Use --record-video to enable. # record-video: true ### Timeout ## The max time this test execution can run before it is cancelled (default: 15m). ## It does not include any time necessary to prepare and clean up the target device. ## The maximum possible testing time is 45m on physical devices and 60m on virtual devices. ## The TIMEOUT units can be h, m, or s. If no unit is given, seconds are assumed. # timeout: 30m ### Asynchronous flag ## Invoke a test asynchronously without waiting for test results. # async: false ### Client Details ## A key-value map of additional details to attach to the test matrix. ## Arbitrary key-value pairs may be attached to a test matrix to provide additional context about the tests being run. ## When consuming the test results, such as in Cloud Functions or a CI system, ## these details can add additional context such as a link to the corresponding pull request. # client-details # key1: value1 # key2: value2 ### Network Profile ## The name of the network traffic profile, for example LTE, HSPA, etc, ## which consists of a set of parameters to emulate network conditions when running the test ## (default: no network shaping; see available profiles listed by the `flank test network-profiles list` command). ## This feature only works on physical devices. # network-profile: LTE ### Result History Name ## The history name for your test results (an arbitrary string label; default: the application's label from the APK manifest). ## All tests which use the same history name will have their results grouped together in the Firebase console in a time-ordered test history list. # results-history-name: android-history ### Number of Flaky Test Attempts ## The number of times a TestExecution should be re-attempted if one or more\\nof its test cases fail for any reason. ## The maximum number of reruns allowed is 10. Default is 0, which implies no reruns. # num-flaky-test-attempts: 0 ### Fail Fast ## If true, only a single attempt at most will be made to run each execution/shard in the matrix. ## Flaky test attempts are not affected. Normally, 2 or more attempts are made if a potential ## infrastructure issue is detected. This feature is for latency sensitive workloads. The ## incidence of execution failures may be significantly greater for fail-fast matrices and support ## is more limited because of that expectation. # fail-fast: false # -- AndroidGcloudYml -- ## Android Application Path ## The path to the application binary file. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## Android App Bundles are specified as .aab, all other files are assumed to be APKs. app : ../test_projects/android/apks/app-debug.apk ### Android Binary File Path ## The path to the binary file containing instrumentation tests. ## The given path may be in the local filesystem or in Google Cloud Storage using a URL beginning with gs://. test : ../test_projects/android/apks/app-debug-androidTest.apk ### Additional APK's ## A list of up to 100 additional APKs to install, in addition to those being directly tested. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. # additional-apks: additional-apk1.apk,additional-apk2.apk,additional-apk3.apk ### Auto Google Login flag ## Automatically log into the test device using a preconfigured Google account before beginning the test. ## Disabled by default. Use --auto-google-login to enable. # auto-google-login: true ### Use Orchestrator Flag ## Whether each test runs in its own Instrumentation instance with the Android Test Orchestrator ## (default: Orchestrator is used). Disable with --no-use-orchestrator. ## See https://developer.android.com/training/testing/junit-runner.html#using-android-test-orchestrator # use-orchestrator: true ### Environment Variables ## A comma-separated, key=value map of environment variables and their desired values. This flag is repeatable. ## The environment variables are mirrored as extra options to the am instrument -e KEY1 VALUE1 \u2026 command and ## passed to your test runner (typically AndroidJUnitRunner) # environment-variables: # coverage: true # coverageFilePath: /sdcard/ # clearPackageData: true ### Directories to Pull ## A list of paths that will be copied from the device's storage to the designated results bucket after the test ## is complete. These must be absolute paths under /sdcard or /data/local/tmp # directories-to-pull: # - /sdcard/ ### Grant Permissions flag ## Whether to grant runtime permissions on the device before the test begins. ## By default, all permissions are granted. PERMISSIONS must be one of: all, none # grant-permissions: all ### Test Type ## The type of test to run. TYPE must be one of: instrumentation, robo, game-loop. # type: instrumentation ### Other Files ## A list of device-path: file-path pairs that indicate the device paths to push files to the device before starting tests, and the paths of files to push. ## Device paths must be under absolute, whitelisted paths (${EXTERNAL_STORAGE}, or ${ANDROID_DATA}/local/tmp). ## Source file paths may be in the local filesystem or in Google Cloud Storage (gs://\u2026). # other-files # - /sdcard/dir1/file1.txt: local/file.txt # - /sdcard/dir2/file2.jpg: gs://bucket/file.jpg ### OBB Files ## A list of one or two Android OBB file names which will be copied to each test device before the tests will run (default: None). ## Each OBB file name must conform to the format as specified by Android (e.g. [main|patch].0300110.com.example.android.obb) and will be installed into <shared-storage>/Android/obb/<package-name>/ on the test device. # obb-files: # - local/file/path/test1.obb # - local/file/path/test2.obb ### Scenario Numbers ## A list of game-loop scenario numbers which will be run as part of the test (default: all scenarios). ## A maximum of 1024 scenarios may be specified in one test matrix, but the maximum number may also be limited by the overall test --timeout setting. # scenario-numbers: # - 1 # - 2 # - 3 ### Scenario Labels ## A list of game-loop scenario labels (default: None). Each game-loop scenario may be labeled in the APK manifest file with one or more arbitrary strings, creating logical groupings (e.g. GPU_COMPATIBILITY_TESTS). ## If --scenario-numbers and --scenario-labels are specified together, Firebase Test Lab will first execute each scenario from --scenario-numbers. ## It will then expand each given scenario label into a list of scenario numbers marked with that label, and execute those scenarios. # scenario-labels: # - label1 # - label2 ### OBB filenames ## A list of OBB required filenames. OBB file name must conform to the format as specified by Android e.g. ## [main|patch].0300110.com.example.android.obb which will be installed into <shared-storage>/Android/obb/<package-name>/ on the device. # obb-names: # - [main|patch].<VERSION>.com.example.android.obb ### Performance Metric flag ## Monitor and record performance metrics: CPU, memory, network usage, and FPS (game-loop only). ## Disabled by default. Use --performance-metrics to enable. # performance-metrics: true ### Number of Uniform Shards ## Specifies the number of shards into which you want to evenly distribute test cases. ## The shards are run in parallel on separate devices. For example, ## if your test execution contains 20 test cases and you specify four shards, each shard executes five test cases. ## The number of shards should be less than the total number of test cases. ## The number of shards specified must be >= 1 and <= 50. ## This option cannot be used along max-test-shards and is not compatible with smart sharding. ## If you want to take benefits of smart sharding use max-test-shards instead. ## default: null # num-uniform-shards: 50 ### Instrumentation Test Runner Class ## The fully-qualified Java class name of the instrumentation test runner ## (default: the last name extracted from the APK manifest). # test-runner-class: com.foo.TestRunner ### Test Targets ## A list of one or more test target filters to apply (default: run all test targets). ## Each target filter must be fully qualified with the package name, class name, or test annotation desired. ## Supported test filters by am instrument -e \u2026 include: ## class, notClass, size, annotation, notAnnotation, package, notPackage, testFile, notTestFile ## See https://developer.android.com/reference/android/support/test/runner/AndroidJUnitRunner for more information. # test-targets: # - class com.example.app.ExampleUiTest#testPasses ### Robo Directives ## A map of robo_directives that you can use to customize the behavior of Robo test. ## The type specifies the action type of the directive, which may take on values click, text or ignore. ## If no type is provided, text will be used by default. ## Each key should be the Android resource name of a target UI element and each value should be the text input for that element. ## Values are only permitted for text type elements, so no value should be specified for click and ignore type elements. # robo-directives: # \"text:input_resource_name\": message # \"click:button_resource_name\": \"\" ### Robo Scripts ## The path to a Robo Script JSON file. ## The path may be in the local filesystem or in Google Cloud Storage using gs:// notation. ## You can guide the Robo test to perform specific actions by recording a Robo Script in Android Studio and then specifying this argument. ## Learn more at https://firebase.google.com/docs/test-lab/robo-ux-test#scripting. # robo-script: path_to_robo_script ### Android Device Parameters ## A list of DIMENSION=VALUE pairs which specify a target device to test against. ## This flag may be repeated to specify multiple devices. ## The four device dimensions are: model, version, locale, and orientation. # device: # - model: NexusLowRes # version: 28 # locale: en # orientation: portrait # - model: NexusLowRes # version: 27 ### test-targets-for-shard ## Specifies a group of packages, classes, and/or test cases to run in each shard (a group of test cases). ## The shards are run in parallel on separate devices. You can repeat this flag up to 50 times to specify multiple shards when one or more physical devices are selected, ## or up to 500 times when no physical devices are selected. ## Note: If you include the flags environment-variable or test-targets when running test-targets-for-shard, the flags are applied to all the shards you create. # test-target-for-shard: # - package com.package1.for.shard1 # - class com.package2.for.shard2.Class flank : # -- FlankYml -- ### Max Test Shards ## test shards - the amount of groups to split the test suite into ## set to -1 to use one shard per test. default: 1 # max-test-shards: 1 ### Shard Time ## shard time - the amount of time tests within a shard should take ## when set to > 0, the shard count is dynamically set based on time up to the maximum limit defined by max-test-shards ## 2 minutes (120) is recommended. ## default: -1 (unlimited) # shard-time: -1 ### Number of Test Runs ## test runs - the amount of times to run the tests. ## 1 runs the tests once. 10 runs all the tests 10x # num-test-runs: 1 ### Smart Flank GCS Path ## Google cloud storage path where the JUnit XML results from the last run is stored. ## NOTE: Empty results will not be uploaded # smart-flank-gcs-path: gs://tmp_flank/tmp/JUnitReport.xml ### Smart Flank Upload Disable flag ## Disables smart flank JUnit XML uploading. Useful for preventing timing data from being updated. ## Default: false # smart-flank-disable-upload: false ### Use Average Test Time for New Tests flag ## Enable using average time from previous tests duration when using SmartShard and tests did not run before. ## Default: false # use-average-test-time-for-new-tests: true ### Default Test Time ## Set default test time used for calculating shards. ## Default: 120.0 # default-test-time: 15 ### Default Class Test Time ## Set default test time (in seconds) used for calculating shards of parametrized classes when previous tests results are not available. ## Default test time for classes should be different from the default time for test ## Default: 240.0 # default-class-test-time: 30 ### Disable Sharding flag ## Disables sharding. Useful for parameterized tests. # disable-sharding: false ### Test Targets Always Run ## always run - these tests are inserted at the beginning of every shard ## Execution order is not guaranteed by Flank. Users are responsible for configuring their own device test runner logic. # test-targets-always-run: # - class com.example.app.ExampleUiTest#testPasses ### Files to Download ## regex is matched against bucket paths, for example: 2019-01-09_00:13:06.106000_YCKl/shard_0/NexusLowRes-28-en-portrait/bugreport.txt # files-to-download: # - .*\\.mp4$ ### Billing Project Name ## The billing enabled Google Cloud Platform project name to use # project: flank-open-source ### Local Results Directory ## Local folder to store the test result. Folder is DELETED before each run to ensure only artifacts from the new run are saved. # local-result-dir: flank ### Keep File Path flag ## Keeps the full path of downloaded files. Required when file names are not unique. ## Default: false # keep-file-path: false ### Additional App/Test APKS ## Include additional app/test apk pairs in the run. Apks are unique by just filename and not by path! ## If app is omitted, then the top level app is used for that pair. # additional-app-test-apks: # - app: ../test_projects/android/apks/app-debug.apk # test: ../test_projects/android/apks/app1-debug-androidTest.apk # - test: ../test_projects/android/apks/app2-debug-androidTest.apk ### Run Timeout ## The max time this test run can execute before it is cancelled (default: unlimited). # run-timeout: 60m ### Ignore Failed Test flag ## Terminate with exit code 0 when there are failed tests. ## Useful for Fladle and other gradle plugins that don't expect the process to have a non-zero exit code. ## The JUnit XML is used to determine failure. (default: false) # ignore-failed-tests: true ### Legacy Junit Results flag ## Flank provides two ways for parsing junit xml results. ## New way uses google api instead of merging xml files, but can generate slightly different output format. ## This flag allows fallback for legacy xml junit results parsing ## Currently available for android, iOS still uses only legacy way. # legacy-junit-result: false ### Output Style flag ## Output style of execution status. May be one of [verbose, multi, single, compact]. ## For runs with only one test execution the default value is 'verbose', in other cases ## 'multi' is used as the default. The output style 'multi' is not displayed correctly on consoles ## which don't support ansi codes, to avoid corrupted output use single or verbose. ## The output style `compact` is used to produce less detailed output, it prints just Args, test and matrix count, weblinks, cost, and result reports. # output-style: single ### Full Junit Result flag ## Enable create additional local junit result on local storage with failure nodes on passed flaky tests. # full-junit-result: false ### Disable Results Upload flag ## Disables flank results upload on gcloud storage. ## Default: false # disable-results-upload: false","title":"Android example"},{"location":"#android-code-coverage","text":"Update your app's build.gradle to build with coverage and use orchestrator. A custom gradle task is defined to generate the coverage report. def coverageEnabled = project.hasProperty('coverage') android { defaultConfig { testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\" // runs pm clear after each test invocation testInstrumentationRunnerArguments clearPackageData: 'true' } buildTypes { debug { testCoverageEnabled true } } // https://google.github.io/android-gradle-dsl/current/com.android.build.gradle.internal.dsl.TestOptions.html#com.android.build.gradle.internal.dsl.TestOptions:animationsDisabled testOptions { execution 'ANDROIDX_TEST_ORCHESTRATOR' animationsDisabled = true } } dependencies { androidTestUtil 'androidx.test:orchestrator:1.1.1' androidTestImplementation(\"androidx.test:runner:1.1.1\") androidTestImplementation(\"androidx.test.ext:junit:1.1.0\") androidTestImplementation(\"androidx.test.ext:junit-ktx:1.1.0\") androidTestImplementation(\"androidx.test.ext:truth:1.1.0\") androidTestImplementation(\"androidx.test.espresso.idling:idling-concurrent:3.1.1\") androidTestImplementation(\"androidx.test.espresso.idling:idling-net:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-accessibility:3.1.1\") androidTestImplementation(\"androidx.test:rules:1.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-core:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-contrib:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-idling-resource:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-intents:3.1.1\") androidTestImplementation(\"androidx.test.espresso:espresso-web:3.1.1\") } if (coverageEnabled) { // gradle -Pcoverage firebaseJacoco task firebaseJacoco(type: JacocoReport) { group = \"Reporting\" description = \"Generate Jacoco coverage reports for Firebase test lab.\" def excludes = [ '**/R.class', '**/R$*.class', '**/BuildConfig.*', \"**/androidx\"] def javaClasses = fileTree(dir: \"${project.buildDir}/intermediates/javac/debug/classes\", excludes: excludes) def kotlinClasses = fileTree(dir: \"${project.buildDir}/tmp/kotlin-classes/debug\", excludes: excludes) getClassDirectories().setFrom(files([javaClasses, kotlinClasses])) getSourceDirectories().setFrom(files([ 'src/main/java', 'src/main/kotlin', 'src/androidTest/java', 'src/androidTest/kotlin'])) def ecFiles = project.fileTree(dir: '..', include: 'results/coverage_ec/**/sdcard/*.ec') ecFiles.forEach { println(\"Reading in $it\") } getExecutionData().setFrom(ecFiles) reports { html { enabled true } xml { enabled false } } } } Starting from Android Marshmallow we must grant runtime permissions to write to external storage. Following snippet in test class solves that issue. If you want to get coverage files when using orchestrator, you must set this Rule for each test class. import androidx.test.rule.GrantPermissionRule ; import static android.Manifest.permission.READ_EXTERNAL_STORAGE ; import static android.Manifest.permission.WRITE_EXTERNAL_STORAGE ; class MyEspressoTest { @Rule GrantPermissionRule grantPermissionRule = GrantPermissionRule . grant ( READ_EXTERNAL_STORAGE , WRITE_EXTERNAL_STORAGE ); // other configuration and tests } Here's an example flank.yml. Note that `coverage` and `coverageFilePath` must be set when using orchestrator with coverage. `coverageFile` is not used. Orchestrator will generate one coverage file per test. `coverageFilePath` must be a directory, not a file. gcloud : app : ./app/build/outputs/apk/debug/app-debug.apk test : ./app/build/outputs/apk/androidTest/debug/app-debug-androidTest.apk environment-variables : coverage : true coverageFilePath : /sdcard/ clearPackageData : true directories-to-pull : - /sdcard/ # use a named results dir that's used by the gradle task results-dir : coverage_ec flank : disableSharding : true files-to-download : - .*/sdcard/[^/]+\\.ec$ - Build the app with coverage: `./gradlew -Pcoverage build` - Run flank `flank android run` - Generate the report `./gradlew -Pcoverage firebaseJacoco` - Open the report in `./build/reports/jacoco/firebaseJacoco/html/index.html`","title":"Android code coverage"},{"location":"#ci-integration","text":"Download Flank from GitHub releases. Stable. Get the latest stable version number and replace the XXX with the version number. wget --quiet https://github.com/Flank/flank/releases/download/vXXX/flank.jar -O ./flank.jar java -jar ./flank.jar android run Snapshot (published after every commit) wget --quiet https://github.com/Flank/flank/releases/download/flank_snapshot/flank.jar -O ./flank.jar java -jar ./flank.jar android run In CI, it may be useful to generate the file via a shell script: cat << 'EOF' > ./flank.yml gcloud: app: ../../test_projects/android/apks/app-debug.apk test: ../../test_projects/android/apks/app-debug-androidTest.apk EOF","title":"CI integration"},{"location":"#circle-ci","text":"Circle CI has a firebase testlab orb that supports Flank.","title":"Circle CI"},{"location":"#bitrise","text":"Bitrise has an official flank step .","title":"Bitrise"},{"location":"#gradle-plugin","text":"Fladle is a Gradle plugin for Flank that provides DSL configuration and task based execution.","title":"Gradle Plugin"},{"location":"#flank-on-windows","text":"In order to build or run Flank using Windows please follow guide of building/running it using Windows WSL. Native support is not currently supported.","title":"Flank on Windows"},{"location":"#authenticate-with-a-google-account","text":"Run flank auth login . Flank will save the credential to ~/.flank . Google account authentication allows each person to have a unique non-shared credential. A service account is still recommended for CI.","title":"Authenticate with a Google account"},{"location":"#authenticate-with-a-service-account","text":"Follow the test lab docs to create a service account. - Save the credential to $HOME/.config/gcloud/application_default_credentials.json or set GOOGLE_APPLICATION_CREDENTIALS when using a custom path. - Set the project id in flank.yml or set the GOOGLE_CLOUD_PROJECT environment variable. - (Since 21.01) if projectId is not set in a config yml file, flank uses the first available project ID among the following sources: 1. The project ID specified in the JSON credentials file pointed by the GOOGLE_APPLICATION_CREDENTIALS environment variable fladle 1. The project ID specified by the GOOGLE_CLOUD_PROJECT environment variable 1. The project ID specified in the JSON credentials file $HOME/.config/gcloud/application_default_credentials.json For continuous integration, base64 encode the credential as GCLOUD_KEY . Then write the file using a shell script. Note that gcloud CLI does not need to be installed. Flank works without any dependency on gcloud CLI. Encode JSON locally. base64 -i \" $HOME /.config/gcloud/application_default_credentials.json\" | pbcopy Then in CI decode the JSON. GCLOUD_DIR = \" $HOME /.config/gcloud/\" mkdir -p \" $GCLOUD_DIR \" echo \" $GCLOUD_KEY \" | base64 --decode > \" $GCLOUD_DIR /application_default_credentials.json\"","title":"Authenticate with a service account"},{"location":"#running-with-gcloud-directly","text":"flank.yml is compatible with the gcloud CLI. gcloud firebase test android run flank.yml:gcloud gcloud alpha firebase test ios run flank.ios.yml:gcloud NOTE: You will need to activate gcloud's service account for the above commands to work.","title":"Running with gcloud directly"},{"location":"#doctor","text":"Use the doctor command to check for errors in the YAML. flank firebase test android doctor flank firebase test ios doctor","title":"Doctor"},{"location":"#check-version","text":"Flank supports printing the current version. $ flank -v v3.0-SNAPSHOT","title":"Check version"},{"location":"#maven","text":"You can consume Flank via maven. See the maven repo for all supported versions. repositories { maven(url = \"https://dl.bintray.com/flank/maven\") } dependencies { compile(\"flank:flank:flank_snapshot\") } or GitHub packages Groovy dependencies { implementation \"com.github.flank:flank:<latest version>\" } Kotlin dependencies { implementation ( \"com.github.flank:flank:<latest version>\" ) }","title":"Maven"},{"location":"#gradle-enterprise-export-api","text":"It is possible to fetch metrics from Gradle builds. For detailed info please visit Gradle Export API and flank's example gradle-export-api .","title":"Gradle Enterprise Export API"},{"location":"#faq","text":"1) > Access Not Configured. Cloud Tool Results API has not been used in project 764086051850 before or it is disabled. This error means authentication hasn't been setup properly. See `Authenticate with a service account` in this readme. 2) > How do I use Flank without typing long commands? Add Flank ' s [ bash helper folder ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / ) to your $PATH environment variable . This will allow you to call the shell scripts in that helper folder from anywhere . With the [ flank ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / flank ) shell script , you can use `flank` instead of `java -jar flank.jar` . Examples: - `flank android run` - `flank ios run` With the [ update_flank . sh ]( https: //gi thub . com /Flank/ flank /blob/m aster /test_runner/ bash / update_flank . sh ) shell script , you can rebuild `flank.jar` . 3) > Test run failed to complete. Expected 786 tests, received 660 Try setting `use-orchestrator: false`. Parameterized tests [are not compatible with orchestrator](https://stackoverflow.com/questions/48735268/unable-to-run-parameterized-tests-with-android-test-orchestrator). Flank uses [orchestrator by default on Android.](https://developer.android.com/training/testing/junit-runner) 4) > I have an issue when attempting to sync the Flank Gradle project Task 'prepareKotlinBuildScriptModel' not found in project ':test_runner'. or similar - Make sure you do not change any module specific settings for Gradle - Clear IDE cache using `File > Invalidate Caches / Restart` - Re - import project using root `build.gradle.kts` - Sync project again 5) > Does Flank support Cucumber? Please check document for more info","title":"FAQ"},{"location":"#resources","text":"Instrumenting Firebase Test Lab","title":"Resources"},{"location":"binaries_used_in_flank_ios_testing/","text":"Binaries used in Flank's iOS testing Location Binaries are placed in Flank binaries repository Usage The binaries are downloaded at runtime when they needed for Linux and Windows from Flank binaries repository . They are unpacked to <user directory>/.flank . If they already exist on this path, they are not downloaded again. Updating In order to update binaries just follow below steps: 1. checkout binaries repository 1. update them using: - updateBinariesWithFlankBash will update binaries for Linux and Windows using flank-scripts - update.sh (old method). It will update binaries for Linux OS 1. commit and push files (create PR with changes) 1. once they will be on master branch. CI job will update artifacts with proper files based on OS","title":"Binaries used in Flank's iOS testing"},{"location":"binaries_used_in_flank_ios_testing/#binaries-used-in-flanks-ios-testing","text":"","title":"Binaries used in Flank's iOS testing"},{"location":"binaries_used_in_flank_ios_testing/#location","text":"Binaries are placed in Flank binaries repository","title":"Location"},{"location":"binaries_used_in_flank_ios_testing/#usage","text":"The binaries are downloaded at runtime when they needed for Linux and Windows from Flank binaries repository . They are unpacked to <user directory>/.flank . If they already exist on this path, they are not downloaded again.","title":"Usage"},{"location":"binaries_used_in_flank_ios_testing/#updating","text":"In order to update binaries just follow below steps: 1. checkout binaries repository 1. update them using: - updateBinariesWithFlankBash will update binaries for Linux and Windows using flank-scripts - update.sh (old method). It will update binaries for Linux OS 1. commit and push files (create PR with changes) 1. once they will be on master branch. CI job will update artifacts with proper files based on OS","title":"Updating"},{"location":"building_updating_flank/","text":"Building and Updating Flank Ensure that all steps taken for contributing and building Flank have been followed, which are found here Building an updated flank To build an updated version of flank from source simply run (This assumes you are in the root Flank directory) ./gradlew flankFullRun The flank full run task, builds a clean Flan, runs all tests and runs the updateFlank gradle task. This will create the Flank.jar file and place it in /test_runner/bash Building a minimized and optimized version of Flank (Proguard) To build a proguarded version of Flank ./gradlew applyProguard This will generate a second Flank.jar , named Flank-proguard.jar found at /test_runner/bash To make use of this jar copy and rename it to Flank.jar","title":"Building & updating Flank"},{"location":"building_updating_flank/#building-and-updating-flank","text":"Ensure that all steps taken for contributing and building Flank have been followed, which are found here","title":"Building and Updating Flank"},{"location":"building_updating_flank/#building-an-updated-flank","text":"To build an updated version of flank from source simply run (This assumes you are in the root Flank directory) ./gradlew flankFullRun The flank full run task, builds a clean Flan, runs all tests and runs the updateFlank gradle task. This will create the Flank.jar file and place it in /test_runner/bash","title":"Building an updated flank"},{"location":"building_updating_flank/#building-a-minimized-and-optimized-version-of-flank-proguard","text":"To build a proguarded version of Flank ./gradlew applyProguard This will generate a second Flank.jar , named Flank-proguard.jar found at /test_runner/bash To make use of this jar copy and rename it to Flank.jar","title":"Building a minimized and optimized version of Flank (Proguard)"},{"location":"client_generation/","text":"Overview The google-api-*-client projects are officially marked as maintenance mode . They don't have a consistent code generation approach. Each binding appears to do something different. apis-client-generator is used to generate REST bindings. The new clients are google-cloud-* . They are in the process of adopting the latest code generation tech googleapis/toolkit which is gRPC based. These handwritten packages are going to migrate to live on top of clients generated from the Google API Code Generator https://github.com/GoogleCloudPlatform/google-cloud-go/issues/266#issuecomment-221083266 API client summary Google API clients ( google-api-*-client ) - Low level - Auto generated - Made for JSON REST APIs - Older project - maintence mode Google APIs toolkit ( googleapis/toolkit ) - Low level - Auto generated - Made for gRPC APIs - New project - actively developed Google Cloud libraries ( google-cloud-* ) - High level - Hand written - Built on top of the low level clients google-api-client - maintenance mode google-api-nodejs-client google-api-php-client google-api-python-client google-api-ruby-client google-api-go-client google-api-go-generator google-api-java-client google-api-javascript-client - not offically marked as maintence, however no new updates since May 2017. google-api-dotnet-client google-api-objectivec-client Deprecated. Replaced entirely by google-api-objectivec-client-for-rest google-api-cpp-client apis-client-generator - active apis-client-generator Java, C++, C#, GWT, PHP, Dart Used to generate google-api-java-client Recommended way to generate bindings for REST APIs apitools - maintenance mode apitools Generates Python only. Used by gcloud CLI Writes protobuf files from API discovery google-cloud - active Actively developed. Google Cloud APIs only. google-cloud-ruby google-cloud-node google-cloud-python google-cloud-go google-cloud-java google-cloud-php google-cloud-dotnet googleapis/toolkit - active The latest code generation tech from Google. googleapis/toolkit googleapis/api-compiler Generating Java APIs with apis-client-generator Install Google API client generator from source. git clone https://github.com/google/apis-client-generator.git pip install . Generate the library manually: generate_library \\ --input=./testing_v1.json \\ --language=java \\ --output_dir=./testing Alternatively, generate the library by running generate.sh","title":"Client generation"},{"location":"client_generation/#overview","text":"The google-api-*-client projects are officially marked as maintenance mode . They don't have a consistent code generation approach. Each binding appears to do something different. apis-client-generator is used to generate REST bindings. The new clients are google-cloud-* . They are in the process of adopting the latest code generation tech googleapis/toolkit which is gRPC based. These handwritten packages are going to migrate to live on top of clients generated from the Google API Code Generator https://github.com/GoogleCloudPlatform/google-cloud-go/issues/266#issuecomment-221083266","title":"Overview"},{"location":"client_generation/#api-client-summary","text":"Google API clients ( google-api-*-client ) - Low level - Auto generated - Made for JSON REST APIs - Older project - maintence mode Google APIs toolkit ( googleapis/toolkit ) - Low level - Auto generated - Made for gRPC APIs - New project - actively developed Google Cloud libraries ( google-cloud-* ) - High level - Hand written - Built on top of the low level clients","title":"API client summary"},{"location":"client_generation/#google-api-client-maintenance-mode","text":"google-api-nodejs-client google-api-php-client google-api-python-client google-api-ruby-client google-api-go-client google-api-go-generator google-api-java-client google-api-javascript-client - not offically marked as maintence, however no new updates since May 2017. google-api-dotnet-client google-api-objectivec-client Deprecated. Replaced entirely by google-api-objectivec-client-for-rest google-api-cpp-client","title":"google-api-client - maintenance mode"},{"location":"client_generation/#apis-client-generator-active","text":"apis-client-generator Java, C++, C#, GWT, PHP, Dart Used to generate google-api-java-client Recommended way to generate bindings for REST APIs","title":"apis-client-generator - active"},{"location":"client_generation/#apitools-maintenance-mode","text":"apitools Generates Python only. Used by gcloud CLI Writes protobuf files from API discovery","title":"apitools - maintenance mode"},{"location":"client_generation/#google-cloud-active","text":"Actively developed. Google Cloud APIs only. google-cloud-ruby google-cloud-node google-cloud-python google-cloud-go google-cloud-java google-cloud-php google-cloud-dotnet","title":"google-cloud - active"},{"location":"client_generation/#googleapistoolkit-active","text":"The latest code generation tech from Google. googleapis/toolkit googleapis/api-compiler","title":"googleapis/toolkit - active"},{"location":"client_generation/#generating-java-apis-with-apis-client-generator","text":"Install Google API client generator from source. git clone https://github.com/google/apis-client-generator.git pip install . Generate the library manually: generate_library \\ --input=./testing_v1.json \\ --language=java \\ --output_dir=./testing Alternatively, generate the library by running generate.sh","title":"Generating Java APIs with apis-client-generator"},{"location":"cucumber_support/","text":"Cucumber support Firebase test lab and Flank do not support Cucumber. However, you could run these tests. - To make them work properly please disable sharding using .yml options: flank : disable-sharding : true or by using command-line option shell script --disable-sharding If you would like to use orchestrator please make sure that you are using at least version 1.3.0 of it. Currently , Flank will run Cucumber tests only if there are other Instrumented tests to run in your test apk. In other cases Flank will fast fail with There are no tests to run message.","title":"Cucumber"},{"location":"cucumber_support/#cucumber-support","text":"Firebase test lab and Flank do not support Cucumber. However, you could run these tests. - To make them work properly please disable sharding using .yml options: flank : disable-sharding : true or by using command-line option shell script --disable-sharding If you would like to use orchestrator please make sure that you are using at least version 1.3.0 of it. Currently , Flank will run Cucumber tests only if there are other Instrumented tests to run in your test apk. In other cases Flank will fast fail with There are no tests to run message.","title":"Cucumber support"},{"location":"dependencies_update_process/","text":"Dependencies update process Description Process run commands and update files with defined versions in the provided file and create PR with changes. Modules Gradle Versions Plugin which check dependencies version and generate report Command in flank-scripts which update dependencies versions Github action job which runs dependencies check every Monday at 5 AM UTC or on-demand Usage Manually (root directory) Generate report using command ./gradlew dependencyUpdates -DoutputFormatter=json -DoutputDir=. Build flank scripts using script ./flank-scripts/bash/buildFlankScripts.sh Run ./flank-scripts/bash/flankScripts dependencies update Github action Run Update dependencies job using Github action menu by clicking Run workflow button Merging to master Success path If all PR jobs will succeed it means that dependencies update will not break the current code base and pull requests could be successfully merged. Failure path If any of PR job will fail, it means that dependencies update will break our codebase and code should be aligned before merging","title":"Dependencies update"},{"location":"dependencies_update_process/#dependencies-update-process","text":"","title":"Dependencies update process"},{"location":"dependencies_update_process/#description","text":"Process run commands and update files with defined versions in the provided file and create PR with changes.","title":"Description"},{"location":"dependencies_update_process/#modules","text":"Gradle Versions Plugin which check dependencies version and generate report Command in flank-scripts which update dependencies versions Github action job which runs dependencies check every Monday at 5 AM UTC or on-demand","title":"Modules"},{"location":"dependencies_update_process/#usage","text":"","title":"Usage"},{"location":"dependencies_update_process/#manually-root-directory","text":"Generate report using command ./gradlew dependencyUpdates -DoutputFormatter=json -DoutputDir=. Build flank scripts using script ./flank-scripts/bash/buildFlankScripts.sh Run ./flank-scripts/bash/flankScripts dependencies update","title":"Manually (root directory)"},{"location":"dependencies_update_process/#github-action","text":"Run Update dependencies job using Github action menu by clicking Run workflow button","title":"Github action"},{"location":"dependencies_update_process/#merging-to-master","text":"","title":"Merging to master"},{"location":"dependencies_update_process/#success-path","text":"If all PR jobs will succeed it means that dependencies update will not break the current code base and pull requests could be successfully merged.","title":"Success path"},{"location":"dependencies_update_process/#failure-path","text":"If any of PR job will fail, it means that dependencies update will break our codebase and code should be aligned before merging","title":"Failure path"},{"location":"error_monitoring/","text":"Flank Error Monitoring Flank uses Bugsnag to monitor test runner stability. Bugsnag enables data driven decisions when prioritizing bug fixes. https://www.bugsnag.com/ https://docs.bugsnag.com/ Data Captured Bugsnag captures the following error data: Flank - Stacktrace - releaseStage of Flank (production or snapshot) - version of Flank (git commit Flank was built from) Device - hostname - locale - osArch - osName - osVersion - runtimeVersions of Java Disable Bugsnag Flank respects the same analytics opt out as gcloud CLI. echo \"DISABLED\" > ~/.gsutil/analytics-uuid More information To see how Bugsnag is integrated within the Flank project please see the Flank Bugsnag testcase and the actual Bugsnag implementation Bugsnag data policy","title":"Flank Error Monitoring"},{"location":"error_monitoring/#flank-error-monitoring","text":"Flank uses Bugsnag to monitor test runner stability. Bugsnag enables data driven decisions when prioritizing bug fixes. https://www.bugsnag.com/ https://docs.bugsnag.com/","title":"Flank Error Monitoring"},{"location":"error_monitoring/#data-captured","text":"Bugsnag captures the following error data: Flank - Stacktrace - releaseStage of Flank (production or snapshot) - version of Flank (git commit Flank was built from) Device - hostname - locale - osArch - osName - osVersion - runtimeVersions of Java","title":"Data Captured"},{"location":"error_monitoring/#disable-bugsnag","text":"Flank respects the same analytics opt out as gcloud CLI. echo \"DISABLED\" > ~/.gsutil/analytics-uuid","title":"Disable Bugsnag"},{"location":"error_monitoring/#more-information","text":"To see how Bugsnag is integrated within the Flank project please see the Flank Bugsnag testcase and the actual Bugsnag implementation Bugsnag data policy","title":"More information"},{"location":"exit_codes_and_exceptions/","text":"Flank exit codes and exceptions Exit code | Returned by exception | Description -- | -- | -- | 0 | | All tests passed 1 | FlankGeneralError, FlankTimeoutError | A general failure occurred. Possible causes include: a filename that does not exist or an HTTP/network error. 2 | FlankConfigurationError, YmlValidationError | Usually indicates missing or wrong usage of flags, incorrect parameters, errors in config files. 10 | FailedMatrixError | At least one matrix not finished (usually a FTL internal error) or unexpected error occurred. 15 | FTLError | Firebase Test Lab could not determine if the test matrix passed or failed, because of an unexpected error. 18 | IncompatibleTestDimensionError | The test environment for this test execution is not supported because of incompatible test dimensions. This error might occur if the selected Android API level is not supported by the selected device type. 19 | MatrixCanceledError | The test matrix was canceled by the user. 20 | InfrastructureError | A test infrastructure error occurred.","title":"Flank exit codes and exceptions"},{"location":"exit_codes_and_exceptions/#flank-exit-codes-and-exceptions","text":"Exit code | Returned by exception | Description -- | -- | -- | 0 | | All tests passed 1 | FlankGeneralError, FlankTimeoutError | A general failure occurred. Possible causes include: a filename that does not exist or an HTTP/network error. 2 | FlankConfigurationError, YmlValidationError | Usually indicates missing or wrong usage of flags, incorrect parameters, errors in config files. 10 | FailedMatrixError | At least one matrix not finished (usually a FTL internal error) or unexpected error occurred. 15 | FTLError | Firebase Test Lab could not determine if the test matrix passed or failed, because of an unexpected error. 18 | IncompatibleTestDimensionError | The test environment for this test execution is not supported because of incompatible test dimensions. This error might occur if the selected Android API level is not supported by the selected device type. 19 | MatrixCanceledError | The test matrix was canceled by the user. 20 | InfrastructureError | A test infrastructure error occurred.","title":"Flank exit codes and exceptions"},{"location":"flank_secrets/","text":"Flank Secrets Flank securely communicates to Firebase Test Lab using Google's official Java SDKs for authorization. 2FA Two-factor authentication is required for everyone in the Flank organization. 2FA is also required on Bugsnag. Running Flank Flank runs require authorization with either a Google user account or a service account. Service accounts are the recommended way to authenticate to Flank instead of using a personal account. The authorization credential is saved by default to: $HOME/.config/gcloud/application_default_credentials.json Developing Flank The Flank release job requires secrets as part of continuous delivery. A flankbot account with Member level permission is used to release artifacts on bintray. GITHUB_TOKEN - Provided by GitHub Actions BUGSNAG_API_KEY - Notifier API key for Bugsnag used to report crashes. JFROG_USER - Username for jfrog authentication JFROG_API_KEY - API key for jfrog used to publish releases. The API key is found in https://bintray.com/profile/edit","title":"Secrets"},{"location":"flank_secrets/#flank-secrets","text":"Flank securely communicates to Firebase Test Lab using Google's official Java SDKs for authorization.","title":"Flank Secrets"},{"location":"flank_secrets/#2fa","text":"Two-factor authentication is required for everyone in the Flank organization. 2FA is also required on Bugsnag.","title":"2FA"},{"location":"flank_secrets/#running-flank","text":"Flank runs require authorization with either a Google user account or a service account. Service accounts are the recommended way to authenticate to Flank instead of using a personal account. The authorization credential is saved by default to: $HOME/.config/gcloud/application_default_credentials.json","title":"Running Flank"},{"location":"flank_secrets/#developing-flank","text":"The Flank release job requires secrets as part of continuous delivery. A flankbot account with Member level permission is used to release artifacts on bintray. GITHUB_TOKEN - Provided by GitHub Actions BUGSNAG_API_KEY - Notifier API key for Bugsnag used to report crashes. JFROG_USER - Username for jfrog authentication JFROG_API_KEY - API key for jfrog used to publish releases. The API key is found in https://bintray.com/profile/edit","title":"Developing Flank"},{"location":"flank_vision/","text":"Flank Vision Flank is a massively parallel Android and iOS test runner for Firebase Test Lab. GCloud Compatible Flank is a Kotlin reimplementation of gcloud firebase test commands. Flank strives to implement all gcloud test commands using compatible YAML syntax. The CLI flags match when possible. The same flank.yml file can be run with both gcloud and Flank. gcloud firebase test android run flank.yml:gcloud flank firebase test android run -c flank.yml Industry enabling features Flank adds features on top of gcloud CLI, such as test sharding for iOS, to improve the developer experience. The goal of Flank's features is to be implemented in the server so that all test lab customers may benefit, not just those who use Flank. By upstreaming Flank features, a new set of even more awesome capabilities can be developed on top of the server API. Vision Today the Flank team is focused on bug fixes and stabilization in both the test runner and the Fladle plugin. In the future Flank may adopt a gRPC API to enable other software to easily be built on top of Flank. Examples include test analytics and test flakiness management.","title":"Vision"},{"location":"flank_vision/#flank-vision","text":"Flank is a massively parallel Android and iOS test runner for Firebase Test Lab.","title":"Flank Vision"},{"location":"flank_vision/#gcloud-compatible","text":"Flank is a Kotlin reimplementation of gcloud firebase test commands. Flank strives to implement all gcloud test commands using compatible YAML syntax. The CLI flags match when possible. The same flank.yml file can be run with both gcloud and Flank. gcloud firebase test android run flank.yml:gcloud flank firebase test android run -c flank.yml","title":"GCloud Compatible"},{"location":"flank_vision/#industry-enabling-features","text":"Flank adds features on top of gcloud CLI, such as test sharding for iOS, to improve the developer experience. The goal of Flank's features is to be implemented in the server so that all test lab customers may benefit, not just those who use Flank. By upstreaming Flank features, a new set of even more awesome capabilities can be developed on top of the server API.","title":"Industry enabling features"},{"location":"flank_vision/#vision","text":"Today the Flank team is focused on bug fixes and stabilization in both the test runner and the Fladle plugin. In the future Flank may adopt a gRPC API to enable other software to easily be built on top of Flank. Examples include test analytics and test flakiness management.","title":"Vision"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/","text":"Getting delete permission exception when using results-dir option #790 Changelog Date Who? Action 7th July 2020 pawelpasterz created Description I am trying enable code-coverage and seeing this error java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug-androidTest.apk. Does the service account need higher permissions? wondering why delete is needed though (edited) I also tried with orchestrator, that didn't change anything. User's yml file gcloud: results-bucket: cloud-test-sofi-test record-video: true app: ../app/build/outputs/apk/debug/app-debug.apk test: ../app/build/outputs/apk/androidTest/debug/app-debug-androidTest.apk use-orchestrator: false environment-variables: endpoint: https://testendponit.com/ performance-metrics: false num-flaky-test-attempts: 1 device: - model: Nexus5X version: 26 locale: en orientation: portrait flank: local-result-dir: ../app/build/test-results/ui-tests/ Stacktrace: RunTests Uploading app-debug-androidTest.apk . Uploading app-debug.apk . java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug-androidTest.apk. at ftl.gc.GcStorage.upload(GcStorage.kt:144) at ftl.gc.GcStorage.upload(GcStorage.kt:50) at ftl.run.AndroidTestRunner$resolveApks$2$invokeSuspend$$inlined$forEach$lambda$2.invokeSuspend(AndroidTestRunner.kt:77) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:594) at kotlinx.coroutines.scheduling.CoroutineScheduler.access$runSafely(CoroutineScheduler.kt:60) at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:740) Suppressed: java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug.apk. at ftl.gc.GcStorage.upload(GcStorage.kt:144) at ftl.gc.GcStorage.upload(GcStorage.kt:50) at ftl.run.AndroidTestRunner$resolveApks$2$invokeSuspend$$inlined$forEach$lambda$1.invokeSuspend(AndroidTestRunner.kt:76) ... 5 more Caused by: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug.apk. Steps to reproduce Unfortunately I was unable to reproduce this error (state for 7th July 2020). If any new information will be available -- this paragraph should be updated. What was done: 1. new FTL project with admin account + one service account (default editor permissions) 1. two flank's run with the same apks and result-dir (for both admin and service accounts) 2. first flank run as admin account and second as service account 2. new FTL project with two service accounts (default editor permissions) -- let's call them A & B 1. first flank run as A and second as B 2. repeat above but with reversed order 3. all above with the same apks and results-dir Comments and thoughts According to google docs --results-dir it is recommended use unique value for each run. Google Docs Caution: if specified, this argument must be unique for each test matrix you create, otherwise results from multiple test matrices will be overwritten or intermingled. -- that indicates there is deletion action performed when file with the same name is being uploaded. Therefor storage.object.delete is required to do so. Google Cloud IAM Permissions : Note: In order to overwrite existing objects, both storage.objects.create and storage.objects.delete permissions are required. -- as confirmation of above Service account with Editor role does not have storage.object.delete Permissions Flank uploads files to storage with the Google client and its logic is rather simple (no additional logic with creating/changing/modifying roles and permissions): val fileBlob = BlobInfo.newBuilder(rootGcsBucket, validGcsPath).build() storage.create(fileBlob, fileBytes) There might have been some changes on the Google Cloud Storage side with permissions (guessing) There were lots of changes since flank 8.1.0 . We know 8.1.0 version had some issues with upload,caching and overlapping results. Conclusion Some time was spent on this issue but no results, with given input and info, were achieved. Team should track any future issues similar to this one and update doc if any new input will be available.","title":"Getting delete permission exception when using results-dir option [#790](https://github.com/Flank/flank/issues/790)"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#getting-delete-permission-exception-when-using-results-dir-option-790","text":"","title":"Getting delete permission exception when using results-dir option #790"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#changelog","text":"Date Who? Action 7th July 2020 pawelpasterz created","title":"Changelog"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#description","text":"I am trying enable code-coverage and seeing this error java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug-androidTest.apk. Does the service account need higher permissions? wondering why delete is needed though (edited) I also tried with orchestrator, that didn't change anything. User's yml file gcloud: results-bucket: cloud-test-sofi-test record-video: true app: ../app/build/outputs/apk/debug/app-debug.apk test: ../app/build/outputs/apk/androidTest/debug/app-debug-androidTest.apk use-orchestrator: false environment-variables: endpoint: https://testendponit.com/ performance-metrics: false num-flaky-test-attempts: 1 device: - model: Nexus5X version: 26 locale: en orientation: portrait flank: local-result-dir: ../app/build/test-results/ui-tests/ Stacktrace: RunTests Uploading app-debug-androidTest.apk . Uploading app-debug.apk . java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug-androidTest.apk. at ftl.gc.GcStorage.upload(GcStorage.kt:144) at ftl.gc.GcStorage.upload(GcStorage.kt:50) at ftl.run.AndroidTestRunner$resolveApks$2$invokeSuspend$$inlined$forEach$lambda$2.invokeSuspend(AndroidTestRunner.kt:77) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:594) at kotlinx.coroutines.scheduling.CoroutineScheduler.access$runSafely(CoroutineScheduler.kt:60) at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:740) Suppressed: java.lang.RuntimeException: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug.apk. at ftl.gc.GcStorage.upload(GcStorage.kt:144) at ftl.gc.GcStorage.upload(GcStorage.kt:50) at ftl.run.AndroidTestRunner$resolveApks$2$invokeSuspend$$inlined$forEach$lambda$1.invokeSuspend(AndroidTestRunner.kt:76) ... 5 more Caused by: com.google.cloud.storage.StorageException: firebase-test-lab@sofi-test.iam.gserviceaccount.com does not have storage.objects.delete access to cloud-test-sofi-test/coverage_ec/app-debug.apk.","title":"Description"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#steps-to-reproduce","text":"Unfortunately I was unable to reproduce this error (state for 7th July 2020). If any new information will be available -- this paragraph should be updated. What was done: 1. new FTL project with admin account + one service account (default editor permissions) 1. two flank's run with the same apks and result-dir (for both admin and service accounts) 2. first flank run as admin account and second as service account 2. new FTL project with two service accounts (default editor permissions) -- let's call them A & B 1. first flank run as A and second as B 2. repeat above but with reversed order 3. all above with the same apks and results-dir","title":"Steps to reproduce"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#comments-and-thoughts","text":"According to google docs --results-dir it is recommended use unique value for each run. Google Docs Caution: if specified, this argument must be unique for each test matrix you create, otherwise results from multiple test matrices will be overwritten or intermingled. -- that indicates there is deletion action performed when file with the same name is being uploaded. Therefor storage.object.delete is required to do so. Google Cloud IAM Permissions : Note: In order to overwrite existing objects, both storage.objects.create and storage.objects.delete permissions are required. -- as confirmation of above Service account with Editor role does not have storage.object.delete Permissions Flank uploads files to storage with the Google client and its logic is rather simple (no additional logic with creating/changing/modifying roles and permissions): val fileBlob = BlobInfo.newBuilder(rootGcsBucket, validGcsPath).build() storage.create(fileBlob, fileBytes) There might have been some changes on the Google Cloud Storage side with permissions (guessing) There were lots of changes since flank 8.1.0 . We know 8.1.0 version had some issues with upload,caching and overlapping results.","title":"Comments and thoughts"},{"location":"getting_delete_permission_exception_when_using_results-dir_option/#conclusion","text":"Some time was spent on this issue but no results, with given input and info, were achieved. Team should track any future issues similar to this one and update doc if any new input will be available.","title":"Conclusion"},{"location":"host_binaries_solutions_comparison/","text":"Host binaries solutions comparison git-lfs Docs can be found here Costs about-storage-and-bandwidth-usage Each data pack cost is $5 per month. It contains: 50gb bandwidth 50gb storage Is git lfs versioning files? Yes. Git LFS versioning files and show changed md5 sum and size. On main repository git diff looks like: diff --git a/app/build/outputs/apk/release/app-release-unsigned.apk b/app/build/outputs/apk/release/app-release-unsigned.apk index bba31d0..d165ca1 100644 --- a/app/build/outputs/apk/release/app-release-unsigned.apk +++ b/app/build/outputs/apk/release/app-release-unsigned.apk @@ -1,3 +1,3 @@ version https://git-lfs.github.com/spec/v1 -oid sha256:bf48d577836f07a3d625ee30f04eb356c0b1158770f613df0d82b6ef40d300d3 -size 1938709 +oid sha256:3349370934f8a1695b6ace6db53e58c0f24a1d9c02ce703b4a78870175c8c066 +size 1938769 How to configure git lfs? To add a file extension to git lfs you should execute the following command: git lfs track \"*.apk\" Now all files with .apk extensions will be added to git lfs How to add file The file should be added in normal way to git git add app-debug.apk git commit -m \"Add apk file\" git push origin master How to play with branches There is nothing to configure. When you modify files on branch changes not shown on the master. git-submodules Docs can be found here Is git submodules versioning files? Git submodule is another git repository linked to the main repository. On main repository git diff looks like: diff --git a/apks b/apks --- a/apks +++ b/apks @@ -1 +1 @@ -Subproject commit 7036bcb56be19490b4445a7e31e821e80b9ff870 +Subproject commit 7036bcb56be19490b4445a7e31e821e80b9ff870-dirty So from the main repository, we don't see what files changed. To check details we need to execute git diff from the submodule. Is git submodule versioning files? Yes. Git Submodule versioning binary files like standard git repository. How to configure git submodule? Create a submodule repository On main execute repository git submodule add git@github.com:url_to/awesome_submodule.git path_to_awesome_submodule. Execute git submodule init How to add file Goto submodule directory git add app-debug.apk git commit -m \"Add apk file\" git push origin master How to play with branches Actually there is an option to track specific branch on submodule. https://stackoverflow.com/questions/9189575/git-submodule-tracking-latest git-annex Docs can be found here Docs for github here We can configure git-annex to store files in places like ftp, amazon s3, etc. Probably it can reduce costs on many large files. link Costs Depends on the chosen storage provider Google drive Limitation to 15gb (free quota) After 15gb need to switch to google one or use Gsuite (10$/month and quota 100gb per user) OneDrive Pricing table: https://www.opendrive.com/pricing Free quota 5gb From table: Custom Plan: 500gb storage and 25gb daily bandwidth : 5$/month 50$/year Droplet/VPS - depend on the provider FTP/SFTP - depend on provider Is git annex versioning files? Git-annex like git lfs versioning control sum Git diff looks like: @ -1 +1 @@ ../../../../../.git/annex/objects/QJ/ZX/SHA256E-s1938733--a5bd978c2a6a9ff32bdf0ad5bd94c1362d6904c80c8f6d7890a40303a5d1d703.apk/SHA256E-s1938733--a5bd978c2a6a9ff32bdf0ad5bd94c1362d6904c80c8f6d7890a40303a5d1d703.apk ../../../../../.git/annex/objects/xG/5q/SHA256E-s1938761--038902c65338873f5936f7c5d764fc98839746036a9fffb46223bb742fd1556f.apk/SHA256E-s1938761--038902c65338873f5936f7c5d764fc98839746036a9fffb46223bb742fd1556f.apk How to configure git annex? First, you need to install git-annex brew install git-annex If you don't have brew installed. Install it from this page Installing on other systems can be found here Initialize On root repository git annex init How to add file git annex add app-debug.apk git commit -m \"Add apk file\" git push origin master How to override file Unlock file git annex unlock app-debug.apk Change file Add file to annex git annex add app-debug.apk Where to host Git-annex allows to host binary files in different locations, after a little discussion with @jan-gogo we chose top 3: Google Drive OneDrive Droplet/VPS FTP/SFTP This list is only our idea feel free to suggest another one. All of the supported storage types can be here How to play with branches When you change branch you can override file and commit changes (remember to unlock file first). File will be uploaded on configured storage. How to configure new git-annex repository with google drive Install rclone from here Install git-annex-rclone from here Go to repository directory and in console enter rclone config Init remote by git annex initremote CONFIG_NAME type=external externaltype=rclone target=CONFIG_NAME prefix=git-annex chunk=50MiB encryption=shared mac=HMACSHA512 rclone_layout=lower You can test remote by git annex testremote How to push changes to remote Unlock file git-annex unlock file_name Modify file Add file git annex add file_name --force Sync changes git annex sync file_name --content How to sync changes from remote If you don't have configured git clone and git-annex with rclone check 1 to 3 points from How to configure new git-annex repository with google drive section execute command git annex enableremote CONFIG_NAME After clone repo sync with git-annex git annex sync --content conclusion GIT LFS GIT SUBMODULES GIT ANNEX Costs 5$/month per pack (50gb) Cannot find clear answer Depend on storage provider Versioning Files Yes Yes Yes Branch support Yes Now there is an option to track specific branch on submodule. https://stackoverflow.com/questions/9189575/git-submodule-tracking-latest Yes Flexibility Yes, you can set your lfs server. Check here No Yes, can use different storage providers. Our requirement is to have remote storage for test artifacts that should cover the following points: * Allow having dedicated artifacts for specific branches. * Free to use * Public read access and restricted write Git annex was the most promising but we didn't find remote storage that will meet all our requirements. By comparing these three solutions, we decide to stay with GitHub releases and automate the release process using custom Gradle tasks.","title":"Host binaries solutions comparison"},{"location":"host_binaries_solutions_comparison/#host-binaries-solutions-comparison","text":"","title":"Host binaries solutions comparison"},{"location":"host_binaries_solutions_comparison/#git-lfs","text":"Docs can be found here","title":"git-lfs"},{"location":"host_binaries_solutions_comparison/#costs","text":"about-storage-and-bandwidth-usage Each data pack cost is $5 per month. It contains: 50gb bandwidth 50gb storage","title":"Costs"},{"location":"host_binaries_solutions_comparison/#is-git-lfs-versioning-files","text":"Yes. Git LFS versioning files and show changed md5 sum and size. On main repository git diff looks like: diff --git a/app/build/outputs/apk/release/app-release-unsigned.apk b/app/build/outputs/apk/release/app-release-unsigned.apk index bba31d0..d165ca1 100644 --- a/app/build/outputs/apk/release/app-release-unsigned.apk +++ b/app/build/outputs/apk/release/app-release-unsigned.apk @@ -1,3 +1,3 @@ version https://git-lfs.github.com/spec/v1 -oid sha256:bf48d577836f07a3d625ee30f04eb356c0b1158770f613df0d82b6ef40d300d3 -size 1938709 +oid sha256:3349370934f8a1695b6ace6db53e58c0f24a1d9c02ce703b4a78870175c8c066 +size 1938769","title":"Is git lfs versioning files?"},{"location":"host_binaries_solutions_comparison/#how-to-configure-git-lfs","text":"To add a file extension to git lfs you should execute the following command: git lfs track \"*.apk\" Now all files with .apk extensions will be added to git lfs","title":"How to configure git lfs?"},{"location":"host_binaries_solutions_comparison/#how-to-add-file","text":"The file should be added in normal way to git git add app-debug.apk git commit -m \"Add apk file\" git push origin master","title":"How to add file"},{"location":"host_binaries_solutions_comparison/#how-to-play-with-branches","text":"There is nothing to configure. When you modify files on branch changes not shown on the master.","title":"How to play with branches"},{"location":"host_binaries_solutions_comparison/#git-submodules","text":"Docs can be found here","title":"git-submodules"},{"location":"host_binaries_solutions_comparison/#is-git-submodules-versioning-files","text":"Git submodule is another git repository linked to the main repository. On main repository git diff looks like: diff --git a/apks b/apks --- a/apks +++ b/apks @@ -1 +1 @@ -Subproject commit 7036bcb56be19490b4445a7e31e821e80b9ff870 +Subproject commit 7036bcb56be19490b4445a7e31e821e80b9ff870-dirty So from the main repository, we don't see what files changed. To check details we need to execute git diff from the submodule.","title":"Is git submodules versioning files?"},{"location":"host_binaries_solutions_comparison/#is-git-submodule-versioning-files","text":"Yes. Git Submodule versioning binary files like standard git repository.","title":"Is git submodule versioning files?"},{"location":"host_binaries_solutions_comparison/#how-to-configure-git-submodule","text":"Create a submodule repository On main execute repository git submodule add git@github.com:url_to/awesome_submodule.git path_to_awesome_submodule. Execute git submodule init","title":"How to configure git submodule?"},{"location":"host_binaries_solutions_comparison/#how-to-add-file_1","text":"Goto submodule directory git add app-debug.apk git commit -m \"Add apk file\" git push origin master","title":"How to add file"},{"location":"host_binaries_solutions_comparison/#how-to-play-with-branches_1","text":"Actually there is an option to track specific branch on submodule. https://stackoverflow.com/questions/9189575/git-submodule-tracking-latest","title":"How to play with branches"},{"location":"host_binaries_solutions_comparison/#git-annex","text":"Docs can be found here Docs for github here We can configure git-annex to store files in places like ftp, amazon s3, etc. Probably it can reduce costs on many large files. link","title":"git-annex"},{"location":"host_binaries_solutions_comparison/#costs_1","text":"Depends on the chosen storage provider Google drive Limitation to 15gb (free quota) After 15gb need to switch to google one or use Gsuite (10$/month and quota 100gb per user) OneDrive Pricing table: https://www.opendrive.com/pricing Free quota 5gb From table: Custom Plan: 500gb storage and 25gb daily bandwidth : 5$/month 50$/year Droplet/VPS - depend on the provider FTP/SFTP - depend on provider","title":"Costs"},{"location":"host_binaries_solutions_comparison/#is-git-annex-versioning-files","text":"Git-annex like git lfs versioning control sum Git diff looks like: @ -1 +1 @@ ../../../../../.git/annex/objects/QJ/ZX/SHA256E-s1938733--a5bd978c2a6a9ff32bdf0ad5bd94c1362d6904c80c8f6d7890a40303a5d1d703.apk/SHA256E-s1938733--a5bd978c2a6a9ff32bdf0ad5bd94c1362d6904c80c8f6d7890a40303a5d1d703.apk ../../../../../.git/annex/objects/xG/5q/SHA256E-s1938761--038902c65338873f5936f7c5d764fc98839746036a9fffb46223bb742fd1556f.apk/SHA256E-s1938761--038902c65338873f5936f7c5d764fc98839746036a9fffb46223bb742fd1556f.apk","title":"Is git annex versioning files?"},{"location":"host_binaries_solutions_comparison/#how-to-configure-git-annex","text":"First, you need to install git-annex brew install git-annex If you don't have brew installed. Install it from this page Installing on other systems can be found here Initialize On root repository git annex init","title":"How to configure git annex?"},{"location":"host_binaries_solutions_comparison/#how-to-add-file_2","text":"git annex add app-debug.apk git commit -m \"Add apk file\" git push origin master","title":"How to add file"},{"location":"host_binaries_solutions_comparison/#how-to-override-file","text":"Unlock file git annex unlock app-debug.apk Change file Add file to annex git annex add app-debug.apk","title":"How to override file"},{"location":"host_binaries_solutions_comparison/#where-to-host","text":"Git-annex allows to host binary files in different locations, after a little discussion with @jan-gogo we chose top 3: Google Drive OneDrive Droplet/VPS FTP/SFTP This list is only our idea feel free to suggest another one. All of the supported storage types can be here","title":"Where to host"},{"location":"host_binaries_solutions_comparison/#how-to-play-with-branches_2","text":"When you change branch you can override file and commit changes (remember to unlock file first). File will be uploaded on configured storage.","title":"How to play with branches"},{"location":"host_binaries_solutions_comparison/#how-to-configure-new-git-annex-repository-with-google-drive","text":"Install rclone from here Install git-annex-rclone from here Go to repository directory and in console enter rclone config Init remote by git annex initremote CONFIG_NAME type=external externaltype=rclone target=CONFIG_NAME prefix=git-annex chunk=50MiB encryption=shared mac=HMACSHA512 rclone_layout=lower You can test remote by git annex testremote","title":"How to configure new git-annex repository with google drive"},{"location":"host_binaries_solutions_comparison/#how-to-push-changes-to-remote","text":"Unlock file git-annex unlock file_name Modify file Add file git annex add file_name --force Sync changes git annex sync file_name --content","title":"How to push changes to remote"},{"location":"host_binaries_solutions_comparison/#how-to-sync-changes-from-remote","text":"If you don't have configured git clone and git-annex with rclone check 1 to 3 points from How to configure new git-annex repository with google drive section execute command git annex enableremote CONFIG_NAME After clone repo sync with git-annex git annex sync --content","title":"How to sync changes from remote"},{"location":"host_binaries_solutions_comparison/#conclusion","text":"GIT LFS GIT SUBMODULES GIT ANNEX Costs 5$/month per pack (50gb) Cannot find clear answer Depend on storage provider Versioning Files Yes Yes Yes Branch support Yes Now there is an option to track specific branch on submodule. https://stackoverflow.com/questions/9189575/git-submodule-tracking-latest Yes Flexibility Yes, you can set your lfs server. Check here No Yes, can use different storage providers. Our requirement is to have remote storage for test artifacts that should cover the following points: * Allow having dedicated artifacts for specific branches. * Free to use * Public read access and restricted write Git annex was the most promising but we didn't find remote storage that will meet all our requirements. By comparing these three solutions, we decide to stay with GitHub releases and automate the release process using custom Gradle tasks.","title":"conclusion"},{"location":"instrumentation_tests/","text":"Flank instrumentation tests Flank contains a project for instrumentation tests placed on flank_tests directory. Tests can be run with Gradle wrapper and parametrized by command-line arguments Commands flank-path location of flank.jar yml-path location of test yml run-params optional additional run parameters, default parameters depend on the platform for iOS is firebase, test, ios, run for android firebase, test, ios, run working-directory optional parameter for set working directory default is: ./ output-pattern optional parameter for set regex to compare output expected-output-code optional parameter for for veryfi output code default: 0 Example of run android test ./gradlew test --tests IntegrationTests.shouldMatchAndroidSuccessExitCodeAndPattern -Dflank-path=../test_runner/build/libs/flank.jar -Dyml-path=./src/test/resources/flank_android.yml Example of run ios test ./gradlew test --tests IntegrationTests.shouldMatchIosSuccessExitCodeAndPattern -Dflank-path=../test_runner/build/libs/flank.jar -Dyml-path=./src/test/resources/flank_ios.yml","title":"Flank instrumentation tests"},{"location":"instrumentation_tests/#flank-instrumentation-tests","text":"Flank contains a project for instrumentation tests placed on flank_tests directory. Tests can be run with Gradle wrapper and parametrized by command-line arguments","title":"Flank instrumentation tests"},{"location":"instrumentation_tests/#commands","text":"flank-path location of flank.jar yml-path location of test yml run-params optional additional run parameters, default parameters depend on the platform for iOS is firebase, test, ios, run for android firebase, test, ios, run working-directory optional parameter for set working directory default is: ./ output-pattern optional parameter for set regex to compare output expected-output-code optional parameter for for veryfi output code default: 0","title":"Commands"},{"location":"instrumentation_tests/#example-of-run-android-test","text":"./gradlew test --tests IntegrationTests.shouldMatchAndroidSuccessExitCodeAndPattern -Dflank-path=../test_runner/build/libs/flank.jar -Dyml-path=./src/test/resources/flank_android.yml","title":"Example of run android test"},{"location":"instrumentation_tests/#example-of-run-ios-test","text":"./gradlew test --tests IntegrationTests.shouldMatchIosSuccessExitCodeAndPattern -Dflank-path=../test_runner/build/libs/flank.jar -Dyml-path=./src/test/resources/flank_ios.yml","title":"Example of run ios test"},{"location":"investigate_flank_options/","text":"Investigate flank options List of options android gcloud app test additional-apks auto-google-login no-auto-google-login use-orchestrator no-use-orchestrator environment-variables directories-to-pull other-files performance-metrics no-performance-metrics num-uniform-shards test-runner-class test-targets robo-directives robo-script results-bucket results-dir record-video no-record-video timeout async client-details network-profile results-history-name num-flaky-test-attempts device flank additional-app-test-apks legacy-junit-result max-test-shards shard-time num-test-runs smart-flank-gcs-path smart-flank-disable-upload disable-sharding test-targets-always-run files-to-download project local-result-dir run-timeout full-junit-result ignore-failed-tests keep-file-path output-style disable-results-upload default-test-time default-class-test-time use-average-test-time-for-new-tests List of options ios gcloud test xctestrun-file xcode-version results-bucket results-dir record-video no-record-video timeout async client-details network-profile results-history-name num-flaky-test-attempts device flank test-targets max-test-shards shard-time num-test-runs smart-flank-gcs-path smart-flank-disable-upload disable-sharding test-targets-always-run files-to-download project local-result-dir run-timeout full-junit-result ignore-failed-tests keep-file-path output-style disable-results-upload default-test-time default-class-test-time use-average-test-time-for-new-tests Investigation report environment-variables (Android) Set the directories-to-pull variable to pull from the device directory with coverage report. There will be no warnings or failure messages when environment-variables is set without directories-to-pull A warning has been added about this. files-to-download (Android) In the case where coverage reports need to be downloaded set the directories-to-pull variable. There will be no warnings or failures when files-to-download is set without directories-to-pull . A warning is added regarding this. disable-sharding (Common) Can be enabled by setting max-test-shards to greater than one. In this case flank will disable sharding A warning is added regarding this. num-uniform-shards (Android) When set with max-test-shards Flank will fail fast. When set with disable-sharding , Flank will disable sharding without any warning Warning added about this.","title":"Investigate flank options"},{"location":"investigate_flank_options/#investigate-flank-options","text":"","title":"Investigate flank options"},{"location":"investigate_flank_options/#list-of-options-android","text":"","title":"List of options android"},{"location":"investigate_flank_options/#gcloud","text":"app test additional-apks auto-google-login no-auto-google-login use-orchestrator no-use-orchestrator environment-variables directories-to-pull other-files performance-metrics no-performance-metrics num-uniform-shards test-runner-class test-targets robo-directives robo-script results-bucket results-dir record-video no-record-video timeout async client-details network-profile results-history-name num-flaky-test-attempts device","title":"gcloud"},{"location":"investigate_flank_options/#flank","text":"additional-app-test-apks legacy-junit-result max-test-shards shard-time num-test-runs smart-flank-gcs-path smart-flank-disable-upload disable-sharding test-targets-always-run files-to-download project local-result-dir run-timeout full-junit-result ignore-failed-tests keep-file-path output-style disable-results-upload default-test-time default-class-test-time use-average-test-time-for-new-tests","title":"flank"},{"location":"investigate_flank_options/#list-of-options-ios","text":"","title":"List of options ios"},{"location":"investigate_flank_options/#gcloud_1","text":"test xctestrun-file xcode-version results-bucket results-dir record-video no-record-video timeout async client-details network-profile results-history-name num-flaky-test-attempts device","title":"gcloud"},{"location":"investigate_flank_options/#flank_1","text":"test-targets max-test-shards shard-time num-test-runs smart-flank-gcs-path smart-flank-disable-upload disable-sharding test-targets-always-run files-to-download project local-result-dir run-timeout full-junit-result ignore-failed-tests keep-file-path output-style disable-results-upload default-test-time default-class-test-time use-average-test-time-for-new-tests","title":"flank"},{"location":"investigate_flank_options/#investigation-report","text":"","title":"Investigation report"},{"location":"investigate_flank_options/#environment-variables-android","text":"Set the directories-to-pull variable to pull from the device directory with coverage report. There will be no warnings or failure messages when environment-variables is set without directories-to-pull A warning has been added about this.","title":"environment-variables (Android)"},{"location":"investigate_flank_options/#files-to-download-android","text":"In the case where coverage reports need to be downloaded set the directories-to-pull variable. There will be no warnings or failures when files-to-download is set without directories-to-pull . A warning is added regarding this.","title":"files-to-download (Android)"},{"location":"investigate_flank_options/#disable-sharding-common","text":"Can be enabled by setting max-test-shards to greater than one. In this case flank will disable sharding A warning is added regarding this.","title":"disable-sharding (Common)"},{"location":"investigate_flank_options/#num-uniform-shards-android","text":"When set with max-test-shards Flank will fail fast. When set with disable-sharding , Flank will disable sharding without any warning Warning added about this.","title":"num-uniform-shards (Android)"},{"location":"junit_xml_comparison/","text":"iOS pass: <testsuite name= \"EarlGreyExampleSwiftTests\" hostname= \"localhost\" tests= \"16\" failures= \"0\" errors= \"0\" time= \"25.892\" > <testcase name= \"testBasicSelection()\" classname= \"EarlGreyExampleSwiftTests\" time= \"2.0\" /> <system-out/> <system-err/> iOS fail: <testsuite name= \"EarlGreyExampleSwiftTests\" hostname= \"localhost\" tests= \"17\" failures= \"1\" errors= \"0\" time= \"25.881\" > <properties/> <testcase name= \"testBasicSelectionAndAction()\" classname= \"EarlGreyExampleSwiftTests\" time= \"0.584\" > <failure> ... Android fail: <testsuite name= \"\" tests= \"2\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"3.87\" timestamp= \"2018-09-09T00:16:36\" hostname= \"localhost\" > <properties/> <testcase name= \"testFails\" classname= \"com.example.app.ExampleUiTest\" time= \"0.857\" > <failure> ... Android pass: <testsuite name= \"\" tests= \"1\" failures= \"0\" errors= \"0\" skipped= \"0\" time= \"2.278\" timestamp= \"2018-09-14T20:45:55\" hostname= \"localhost\" > <properties/> <testcase name= \"testPasses\" classname= \"com.example.app.ExampleUiTest\" time= \"0.328\" /> </testsuite> results / analysis: testsuite name - test target name tests - count of total tests failures - count of failures (test assertion failed) errors - count of errors (unhandled exceptions) time - overall time of test suite in seconds hostname - always localhost testcase name - name of test method classname - name of class that defines the test time - time in seconds Android testsuite name - always empty string tests failures errors skipped* Android only. time timestamp* Android only. hostname - always localhost testcase name classname time","title":"Junit xml comparison"},{"location":"logging/","text":"Logs in Flank Log level depends on the output style. Simple, multi and verbose output style prints logs from SIMPLE and DETAILED levels. Compact style prints log only from SIMPLE level. If you want a print message for all output styles uses log or logLn with only message parameter. If you want print message more detailed message use log or logLn and set level to OutputLogLevel.DETAILED","title":"Logs in Flank"},{"location":"logging/#logs-in-flank","text":"Log level depends on the output style. Simple, multi and verbose output style prints logs from SIMPLE and DETAILED levels. Compact style prints log only from SIMPLE level. If you want a print message for all output styles uses log or logLn with only message parameter. If you want print message more detailed message use log or logLn and set level to OutputLogLevel.DETAILED","title":"Logs in Flank"},{"location":"mock_server/","text":"Mock Server Goal: Test Flank using a mock server so we don't have to spend $$ to verify the test runner works. Proof of concepts prism prism consumes Swagger v2 and uses json-schema-faker to return generated default responses. The fields are randomly null which crashes the runner. Prism is exceptionally slow and closed sourced. There's no way to customize the behavior. Not a viable option. mock_server_inflector swagger-codegen consumes OpenAPI v3 and generates a fake server using swagger-inflector . Static canned responses are used based on the OpenAPI description. The responses aren't useful since they don't mimic a real server. Swagger generates a massive amount of code. The build system lacks gradle support . mock_server_vertx vertx-web-api-contract consumes OpenAPI v3 and generates a fake server using slush-vertx . Stub implementations for routes are provided. There are no precanned responses. Unfortunately the code generation is incompatible with the variable naming used in Firebase Test Lab. I opened an issue upstream . Of the three generated options, this looks like the nicest one. However the Google API surface is enormous and we don't need stubs for everything. mock_server ktor is a nice Kotlin server based on Netty. In a single file, all the necessary routes are implemented for each API. The routes are easy to define manually. The complexity of OpenAPI / code generation is avoided. This is the approach that's working on the kotlin_poc runner. Known issues Using a flag for mocked mode isn't ideal. Dependency injection with interfaces would be more robust.","title":"Mock Server"},{"location":"mock_server/#mock-server","text":"Goal: Test Flank using a mock server so we don't have to spend $$ to verify the test runner works.","title":"Mock Server"},{"location":"mock_server/#proof-of-concepts","text":"","title":"Proof of concepts"},{"location":"mock_server/#prism","text":"prism consumes Swagger v2 and uses json-schema-faker to return generated default responses. The fields are randomly null which crashes the runner. Prism is exceptionally slow and closed sourced. There's no way to customize the behavior. Not a viable option.","title":"prism"},{"location":"mock_server/#mock_server_inflector","text":"swagger-codegen consumes OpenAPI v3 and generates a fake server using swagger-inflector . Static canned responses are used based on the OpenAPI description. The responses aren't useful since they don't mimic a real server. Swagger generates a massive amount of code. The build system lacks gradle support .","title":"mock_server_inflector"},{"location":"mock_server/#mock_server_vertx","text":"vertx-web-api-contract consumes OpenAPI v3 and generates a fake server using slush-vertx . Stub implementations for routes are provided. There are no precanned responses. Unfortunately the code generation is incompatible with the variable naming used in Firebase Test Lab. I opened an issue upstream . Of the three generated options, this looks like the nicest one. However the Google API surface is enormous and we don't need stubs for everything.","title":"mock_server_vertx"},{"location":"mock_server/#mock_server","text":"ktor is a nice Kotlin server based on Netty. In a single file, all the necessary routes are implemented for each API. The routes are easy to define manually. The complexity of OpenAPI / code generation is avoided. This is the approach that's working on the kotlin_poc runner.","title":"mock_server"},{"location":"mock_server/#known-issues","text":"Using a flag for mocked mode isn't ideal. Dependency injection with interfaces would be more robust.","title":"Known issues"},{"location":"permissions_denied_behavior/","text":"Flank permissions denied behavior Reported on: Clean flank not authorized error messages #874 Changed on: Enhance permission denied exception logs #875 1. User don't have permission to project (403) When user don't have permission to project Flank should returns message like: Fla n k e n cou ntere d a 403 error whe n ru nn i n g o n projec t $projec t _ na me. Please veri f y t his crede nt ial is au t horized f or t he projec t . Co ns ider au t he nt ica t io n a wi t h Service Accou nt h tt ps : //gi t hub.com/Fla n k/ flan k#au t he nt ica te - wi t h - a - service - accou nt or wi t h a Google accou nt h tt ps : //gi t hub.com/Fla n k/ flan k#au t he nt ica te - wi t h - a - google - accou nt Caused by : com.google.api.clie nt .googleapis.jso n .GoogleJso n Respo nse Excep t io n : 403 Forbidde n { \"code\" : 403 , \"errors\" : [ { \"domain\" : \"global\" , \"message\" : \"The caller does not have permission\" , \"reason\" : \"forbidden\" } ], \"message\" : \"The caller does not have permission\" , \"status\" : \"PERMISSION_DENIED\" } You can reproduce the error by setting PROJECT_ID to a project that the firebase account doesn't have permission to access. 2. Project not found (404) When project not found on firebase Flank should return message like: Fla n k was u na ble t o f i n d projec t $projec t _ na me. Please veri f y t he projec t id. Caused by : com.google.api.clie nt .googleapis.jso n .GoogleJso n Respo nse Excep t io n : 404 No t Fou n d { \"code\" : 404 , \"errors\" : [ { \"domain\" : \"global\" , \"message\" : \"Project not found: $project_name\" , \"reason\" : \"notFound\" } ], \"message\" : \"Project not found: $project_name\" , \"status\" : \"NOT_FOUND\" } You can reproduce the error by setting PROJECT_ID to a project that doesn't exist. 3. On this two cases Flank throws FlankCommonException and exit with code: 1","title":"Flank permissions denied behavior"},{"location":"permissions_denied_behavior/#flank-permissions-denied-behavior","text":"Reported on: Clean flank not authorized error messages #874 Changed on: Enhance permission denied exception logs #875","title":"Flank permissions denied behavior"},{"location":"permissions_denied_behavior/#1-user-dont-have-permission-to-project-403","text":"When user don't have permission to project Flank should returns message like: Fla n k e n cou ntere d a 403 error whe n ru nn i n g o n projec t $projec t _ na me. Please veri f y t his crede nt ial is au t horized f or t he projec t . Co ns ider au t he nt ica t io n a wi t h Service Accou nt h tt ps : //gi t hub.com/Fla n k/ flan k#au t he nt ica te - wi t h - a - service - accou nt or wi t h a Google accou nt h tt ps : //gi t hub.com/Fla n k/ flan k#au t he nt ica te - wi t h - a - google - accou nt Caused by : com.google.api.clie nt .googleapis.jso n .GoogleJso n Respo nse Excep t io n : 403 Forbidde n { \"code\" : 403 , \"errors\" : [ { \"domain\" : \"global\" , \"message\" : \"The caller does not have permission\" , \"reason\" : \"forbidden\" } ], \"message\" : \"The caller does not have permission\" , \"status\" : \"PERMISSION_DENIED\" } You can reproduce the error by setting PROJECT_ID to a project that the firebase account doesn't have permission to access.","title":"1. User don't have permission to project (403)"},{"location":"permissions_denied_behavior/#2-project-not-found-404","text":"When project not found on firebase Flank should return message like: Fla n k was u na ble t o f i n d projec t $projec t _ na me. Please veri f y t he projec t id. Caused by : com.google.api.clie nt .googleapis.jso n .GoogleJso n Respo nse Excep t io n : 404 No t Fou n d { \"code\" : 404 , \"errors\" : [ { \"domain\" : \"global\" , \"message\" : \"Project not found: $project_name\" , \"reason\" : \"notFound\" } ], \"message\" : \"Project not found: $project_name\" , \"status\" : \"NOT_FOUND\" } You can reproduce the error by setting PROJECT_ID to a project that doesn't exist.","title":"2. Project not found (404)"},{"location":"permissions_denied_behavior/#3-on-this-two-cases-flank-throws-flankcommonexception-and-exit-with-code-1","text":"","title":"3. On this two cases Flank throws FlankCommonException and exit with code: 1"},{"location":"pr_titles/","text":"PR title using conventional commits Introduction The Conventional Commits specification is a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of. Usage Every PR which is not in draft mode should follow conventional commit convention for PR title. It allows us to generate release notes and avoid merge conflicts in release_notes.md file PR title Pull request title should be: <type>([optional scope]): <description> where <type> - one of following [optional scope] - additional information <description> - description of pr Type build - Changes that affect the build system or external dependencies (dependencies update) ci - Changes to our CI configuration files and scripts (basically directory .github/workflows ) docs - Documentation only changes feat - A new feature fix - A bug fix chore - Changes which does not touch the code (ex. manual update of release notes). It will not generate release notes changes refactor - A code change that contains refactor style - Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) test - Adding missing tests or correcting existing tests and also changes for our test app perf - A code change that improves performance (I do not think we will use it) Examples feat: Add locales description command for ios and android -> https://github.com/Flank/flank/pull/969 fix: rate limit exceeded -> https://github.com/Flank/flank/pull/919 ci: Added leading V to version name -> https://github.com/Flank/flank/pull/980 refactor: config entities and arguments -> https://github.com/Flank/flank/pull/831 docs: Add secrets and vision doc -> https://github.com/Flank/flank/pull/922 build: Disable Auto Doc Generation -> https://github.com/Flank/flank/pull/942 test: added multi modules to test app -> https://github.com/Flank/flank/pull/857 chore: Release v20.08.1 -> https://github.com/Flank/flank/pull/982","title":"Pull request"},{"location":"pr_titles/#pr-title-using-conventional-commits","text":"","title":"PR title using conventional commits"},{"location":"pr_titles/#introduction","text":"The Conventional Commits specification is a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of.","title":"Introduction"},{"location":"pr_titles/#usage","text":"Every PR which is not in draft mode should follow conventional commit convention for PR title. It allows us to generate release notes and avoid merge conflicts in release_notes.md file","title":"Usage"},{"location":"pr_titles/#pr-title","text":"Pull request title should be: <type>([optional scope]): <description> where <type> - one of following [optional scope] - additional information <description> - description of pr","title":"PR title"},{"location":"pr_titles/#type","text":"build - Changes that affect the build system or external dependencies (dependencies update) ci - Changes to our CI configuration files and scripts (basically directory .github/workflows ) docs - Documentation only changes feat - A new feature fix - A bug fix chore - Changes which does not touch the code (ex. manual update of release notes). It will not generate release notes changes refactor - A code change that contains refactor style - Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) test - Adding missing tests or correcting existing tests and also changes for our test app perf - A code change that improves performance (I do not think we will use it)","title":"Type"},{"location":"pr_titles/#examples","text":"feat: Add locales description command for ios and android -> https://github.com/Flank/flank/pull/969 fix: rate limit exceeded -> https://github.com/Flank/flank/pull/919 ci: Added leading V to version name -> https://github.com/Flank/flank/pull/980 refactor: config entities and arguments -> https://github.com/Flank/flank/pull/831 docs: Add secrets and vision doc -> https://github.com/Flank/flank/pull/922 build: Disable Auto Doc Generation -> https://github.com/Flank/flank/pull/942 test: added multi modules to test app -> https://github.com/Flank/flank/pull/857 chore: Release v20.08.1 -> https://github.com/Flank/flank/pull/982","title":"Examples"},{"location":"release_process/","text":"Release process Requirements A release process should be run withing macOS environment The machine should contain: homebrew - Package manager hub - Github CLI tool Current setup Current scripts run on GitHub actions environment with macos-latest os. Script could be found on path Each push: - to master branch run Snapshot release - of tag v* run regular release Triggering release Manually Navigate to Github Actions Run job Generate release notes for next commit by using Run Workflow button After merging PR, the next tag will be pushed to repository Wait for CI job to finish Automatically Release job will run each 1st day of month After merging PR, the next tag will be pushed to repository Wait for CI job to finish CI Steps Gradle Build flankScripts and add it to PATH Set environment variables Update bugsnag Get Jfrog CLI Delete old snapshot Gradle Build Flank Gradle Upload to bintray Authenticate to hub Remove old release Rename old tag Release flank Snapshot for snapshot flow (push to master) Stable for regular flow (push tag v* ) Sync bintray to maven central","title":"Releasing"},{"location":"release_process/#release-process","text":"","title":"Release process"},{"location":"release_process/#requirements","text":"A release process should be run withing macOS environment The machine should contain: homebrew - Package manager hub - Github CLI tool","title":"Requirements"},{"location":"release_process/#current-setup","text":"Current scripts run on GitHub actions environment with macos-latest os. Script could be found on path Each push: - to master branch run Snapshot release - of tag v* run regular release","title":"Current setup"},{"location":"release_process/#triggering-release","text":"","title":"Triggering release"},{"location":"release_process/#manually","text":"Navigate to Github Actions Run job Generate release notes for next commit by using Run Workflow button After merging PR, the next tag will be pushed to repository Wait for CI job to finish","title":"Manually"},{"location":"release_process/#automatically","text":"Release job will run each 1st day of month After merging PR, the next tag will be pushed to repository Wait for CI job to finish","title":"Automatically"},{"location":"release_process/#ci-steps","text":"Gradle Build flankScripts and add it to PATH Set environment variables Update bugsnag Get Jfrog CLI Delete old snapshot Gradle Build Flank Gradle Upload to bintray Authenticate to hub Remove old release Rename old tag Release flank Snapshot for snapshot flow (push to master) Stable for regular flow (push tag v* ) Sync bintray to maven central","title":"CI Steps"},{"location":"smart_flank/","text":"Smart Flank Design Doc Smart Flank is a sharding algorithm that groups tests into equally sized buckets. Current implementation At the start of a run, Flank checks to see if there's a JUnit XML with timing info from the previous run. If there's no previous test time recorded, then the test is estimated to be 10 seconds. The tests are then grouped into equally timed buckets. The bucket count is set by the user provided maxTestShards count. After each test run, the aggregated JUnit XML from the new run is merged with the old run. Any tests not in the new run are discarded. Tests that were skipped, errored, failed, or empty are discarded. The test time value of successful tests is set using the time from the latest run. If a test failed in the new run and passed in the old run, the timing info from the old run is carried over. The merged XML is uploaded to the user defined smartFlankGcsPath . If smartFlankGcsPath is not defined, then the smart flank feature will not activate. Empty results are not uploaded. Example: <!-- Old Run --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"4\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"51.773\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"5.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"6.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"7.0\" /> <testcase name= \"d()\" classname= \"d\" time= \"8.0\" /> </testsuite> </testsuites> <!-- New run --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"4\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"51.773\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"1.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"2.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"0.584\" > <failure> Exception: NoMatchingElementException </failure> <failure> failed: caught \"EarlGreyInternalTestInterruptException\", \"Immediately halt execution of testcase\" </failure> </testcase> <testcase name= \"d()\" classname= \"d\" time= \"0.0\" > <skipped/> </testcase> </testsuite> </testsuites> <!-- Merged --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"3\" failures= \"0\" errors= \"0\" skipped= \"0\" time= \"10.0\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"1.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"2.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"7.0\" /> </testsuite> </testsuites> Issues Merging XML files is complicated A local run of 1 test will upload a new XML file that contains only that one test. That will discard timing info for all other tests. Does not integrate properly with the typical local/PR/master workflow. Ideas for improvement Keep a user configurable rolling number of aggregated xmls (1.xml, 2.xml, 3.xml) and shard based on the average time. Average time is expected to be more reliable than always using the last time in isolation. Identify a way of translating app binary to a default xml name (bundle id/package name) so that smart flank works out of the box for users. Talk with Firebase on how to do this locally and/or expose an API.","title":"Smart Flank"},{"location":"smart_flank/#smart-flank-design-doc","text":"Smart Flank is a sharding algorithm that groups tests into equally sized buckets.","title":"Smart Flank Design Doc"},{"location":"smart_flank/#current-implementation","text":"At the start of a run, Flank checks to see if there's a JUnit XML with timing info from the previous run. If there's no previous test time recorded, then the test is estimated to be 10 seconds. The tests are then grouped into equally timed buckets. The bucket count is set by the user provided maxTestShards count. After each test run, the aggregated JUnit XML from the new run is merged with the old run. Any tests not in the new run are discarded. Tests that were skipped, errored, failed, or empty are discarded. The test time value of successful tests is set using the time from the latest run. If a test failed in the new run and passed in the old run, the timing info from the old run is carried over. The merged XML is uploaded to the user defined smartFlankGcsPath . If smartFlankGcsPath is not defined, then the smart flank feature will not activate. Empty results are not uploaded. Example: <!-- Old Run --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"4\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"51.773\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"5.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"6.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"7.0\" /> <testcase name= \"d()\" classname= \"d\" time= \"8.0\" /> </testsuite> </testsuites> <!-- New run --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"4\" failures= \"1\" errors= \"0\" skipped= \"0\" time= \"51.773\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"1.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"2.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"0.584\" > <failure> Exception: NoMatchingElementException </failure> <failure> failed: caught \"EarlGreyInternalTestInterruptException\", \"Immediately halt execution of testcase\" </failure> </testcase> <testcase name= \"d()\" classname= \"d\" time= \"0.0\" > <skipped/> </testcase> </testsuite> </testsuites> <!-- Merged --> <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"EarlGreyExampleSwiftTests\" tests= \"3\" failures= \"0\" errors= \"0\" skipped= \"0\" time= \"10.0\" hostname= \"localhost\" > <testcase name= \"a()\" classname= \"a\" time= \"1.0\" /> <testcase name= \"b()\" classname= \"b\" time= \"2.0\" /> <testcase name= \"c()\" classname= \"c\" time= \"7.0\" /> </testsuite> </testsuites>","title":"Current implementation"},{"location":"smart_flank/#issues","text":"Merging XML files is complicated A local run of 1 test will upload a new XML file that contains only that one test. That will discard timing info for all other tests. Does not integrate properly with the typical local/PR/master workflow.","title":"Issues"},{"location":"smart_flank/#ideas-for-improvement","text":"Keep a user configurable rolling number of aggregated xmls (1.xml, 2.xml, 3.xml) and shard based on the average time. Average time is expected to be more reliable than always using the last time in isolation. Identify a way of translating app binary to a default xml name (bundle id/package name) so that smart flank works out of the box for users. Talk with Firebase on how to do this locally and/or expose an API.","title":"Ideas for improvement"},{"location":"test_artifacts/","text":"Test artifacts Test artifacts are precompiled binaries necessary for CI and local testing. They can change over time so developers should be able to maintain and share them easy as possible. Overview Storage Local Local working copy of test artifacts can be find in test_artifacts directory inside local flank repository. However, this directory MUST be added to .gitignore to prevent committing binaries to git. We accept few restrictions about storing test artifacts in test_artifacts directory. * test_artifacts directory CAN contain directories with binaries required for testing. * Names of directories inside test_artifacts SHOULD reflect names of working branches. * test_artifacts directory CAN contain test artifacts archives bundles. * The name of artifact archive should match following format branchName-unixTimestamp.zip Remote The remote copy of test artifacts can be find at test_artifacts/releases We accept few restrictions about using github releases as storage for test artifacts. * The release name and tag should reflect the name of related working branch. * Each release should contain only one asset with artifacts. * The name of test artifact assets archive should be [ ~~mdp5 checksum~~ | unix timestamp ] suffixed with zip extensions. Linking artifacts For convenient switching between test artifacts we are using symbolic links. Ensure you have symbolic link to correct directory inside test_artifacts directory, otherwise the unit tests will fail because of lack of binaries. Test projects All source code of test artifacts binaries can be find in test_projects directory. Generating test artifacts source .env update_test_artifacts android ios # [ android | go | ios | all ] Working with artifacts All testArtifacts subcommands can be configured using base options. * testArtifacts -b {name} used to specify branch name for test artifacts. For example if you want to run any subcommand on artifacts dedicated for your working branch feature123 run testArtifacts -b feature123 {subcommand} . By default, it is the name of current working git branch. testArtifacts -p {path} used to specify the path to local flank repository. By default, the path is read from FLANK_ROOT env variable. To export path to your local flank repository just source .env file. Developer scenarios As a developer I want to download test artifacts before test run. Just run ./gradlew test command, this should trigger resolveArtifacts task which will update artifacts if needed. As a developer I want to switch between local test artifacts Run flankScripts testArtifacts link to create a symbolic link to test artifacts for the current branch. Run flankScripts testArtifacts -b {name} link to create a symbolic link to test artifacts for the specific branch. As a developer I want to edit existing test artifacts Edit required project in test_artifacts/releases directory. Ensure you have sourced .env file. Build required project and copy binaries using update_test_artifacts shell function or do it manually. Ensure you have linked a correct directory with artifacts. Your local tests now should you use updated artifacts. As a developer I want to upload new test artifacts to remote repository. Make sure you have directory with artifacts in test_artifacts and the name same as working branch. Run flankScripts testArtifacts zip to create zip archive. Run flankScripts testArtifacts upload to upload zip as remote copy. The script will automatically resolve the latest archive and upload it. As a developer I want to remove test artifacts Run flankScripts testArtifacts remove this will remove the remote copy of test artifacts for current working branch. iOS test artifacts Currently we have 4 different iOS test projects: EarlGreyExample FlankExample FlankGameLoopExample FlankTestPlansExample Source code of each of them is located under: test_projects/ios Test artifacts for each project contains: * build output in Debug-iphoneos * zipped build output in PROJECT_NAME.zip, * .xctestrun file for each test target EarlGreyExample This project is basically clone of EarlGrey . Source project contains two test targets: EarlGreyExampleSwiftTests, EarlGreyExampleTests. Generate Run: flankScripts shell ops build_earl_grey_example . Source Code test_projects/ios/EarlGreyExample FlankExample Simple project with two test targets: FlankExampleTests, FlankExampleSecondTests. Generate Run: flankScripts shell ops build_flank_example . Source Code test_projects/ios/EarlGreyExample FlankGameLoopExample Simple SpriteKit app to test gameloop mode. It doesn't contain any test target, so test artifacts contains only IPA file. Generate Run: flankScripts shell ops build_ios_gameloop_example . Source Code test_projects/ios/EarlGreyExample \u26a0\ufe0f NOTE: Generating IPA requires Apple distribution certificate therefore for now it's not possible to generate it without correct Apple Developer Account. build_ios_gameloop_example is excluded when building all iOS artifacts: update_test_artifacts ios FlankTestPlansExample iOS project with XCTestPlans. Contains AllTests test plan. Generated .xctestrun is using V2 format. More details about test plans: docs/feature/ios_test_plans.md Generate Run: flankScripts shell ops build_ios_testplans_example . Source Code test_projects/ios/EarlGreyExample","title":"Test artifacts"},{"location":"test_artifacts/#test-artifacts","text":"Test artifacts are precompiled binaries necessary for CI and local testing. They can change over time so developers should be able to maintain and share them easy as possible.","title":"Test artifacts"},{"location":"test_artifacts/#overview","text":"","title":"Overview"},{"location":"test_artifacts/#storage","text":"","title":"Storage"},{"location":"test_artifacts/#local","text":"Local working copy of test artifacts can be find in test_artifacts directory inside local flank repository. However, this directory MUST be added to .gitignore to prevent committing binaries to git. We accept few restrictions about storing test artifacts in test_artifacts directory. * test_artifacts directory CAN contain directories with binaries required for testing. * Names of directories inside test_artifacts SHOULD reflect names of working branches. * test_artifacts directory CAN contain test artifacts archives bundles. * The name of artifact archive should match following format branchName-unixTimestamp.zip","title":"Local"},{"location":"test_artifacts/#remote","text":"The remote copy of test artifacts can be find at test_artifacts/releases We accept few restrictions about using github releases as storage for test artifacts. * The release name and tag should reflect the name of related working branch. * Each release should contain only one asset with artifacts. * The name of test artifact assets archive should be [ ~~mdp5 checksum~~ | unix timestamp ] suffixed with zip extensions.","title":"Remote"},{"location":"test_artifacts/#linking-artifacts","text":"For convenient switching between test artifacts we are using symbolic links. Ensure you have symbolic link to correct directory inside test_artifacts directory, otherwise the unit tests will fail because of lack of binaries.","title":"Linking artifacts"},{"location":"test_artifacts/#test-projects","text":"All source code of test artifacts binaries can be find in test_projects directory.","title":"Test projects"},{"location":"test_artifacts/#generating-test-artifacts","text":"source .env update_test_artifacts android ios # [ android | go | ios | all ]","title":"Generating test artifacts"},{"location":"test_artifacts/#working-with-artifacts","text":"All testArtifacts subcommands can be configured using base options. * testArtifacts -b {name} used to specify branch name for test artifacts. For example if you want to run any subcommand on artifacts dedicated for your working branch feature123 run testArtifacts -b feature123 {subcommand} . By default, it is the name of current working git branch. testArtifacts -p {path} used to specify the path to local flank repository. By default, the path is read from FLANK_ROOT env variable. To export path to your local flank repository just source .env file.","title":"Working with artifacts"},{"location":"test_artifacts/#developer-scenarios","text":"As a developer I want to download test artifacts before test run. Just run ./gradlew test command, this should trigger resolveArtifacts task which will update artifacts if needed. As a developer I want to switch between local test artifacts Run flankScripts testArtifacts link to create a symbolic link to test artifacts for the current branch. Run flankScripts testArtifacts -b {name} link to create a symbolic link to test artifacts for the specific branch. As a developer I want to edit existing test artifacts Edit required project in test_artifacts/releases directory. Ensure you have sourced .env file. Build required project and copy binaries using update_test_artifacts shell function or do it manually. Ensure you have linked a correct directory with artifacts. Your local tests now should you use updated artifacts. As a developer I want to upload new test artifacts to remote repository. Make sure you have directory with artifacts in test_artifacts and the name same as working branch. Run flankScripts testArtifacts zip to create zip archive. Run flankScripts testArtifacts upload to upload zip as remote copy. The script will automatically resolve the latest archive and upload it. As a developer I want to remove test artifacts Run flankScripts testArtifacts remove this will remove the remote copy of test artifacts for current working branch.","title":"Developer scenarios"},{"location":"test_artifacts/#ios-test-artifacts","text":"Currently we have 4 different iOS test projects: EarlGreyExample FlankExample FlankGameLoopExample FlankTestPlansExample Source code of each of them is located under: test_projects/ios Test artifacts for each project contains: * build output in Debug-iphoneos * zipped build output in PROJECT_NAME.zip, * .xctestrun file for each test target","title":"iOS test artifacts"},{"location":"test_artifacts/#earlgreyexample","text":"This project is basically clone of EarlGrey . Source project contains two test targets: EarlGreyExampleSwiftTests, EarlGreyExampleTests.","title":"EarlGreyExample"},{"location":"test_artifacts/#generate","text":"Run: flankScripts shell ops build_earl_grey_example .","title":"Generate"},{"location":"test_artifacts/#source-code","text":"test_projects/ios/EarlGreyExample","title":"Source Code"},{"location":"test_artifacts/#flankexample","text":"Simple project with two test targets: FlankExampleTests, FlankExampleSecondTests.","title":"FlankExample"},{"location":"test_artifacts/#generate_1","text":"Run: flankScripts shell ops build_flank_example .","title":"Generate"},{"location":"test_artifacts/#source-code_1","text":"test_projects/ios/EarlGreyExample","title":"Source Code"},{"location":"test_artifacts/#flankgameloopexample","text":"Simple SpriteKit app to test gameloop mode. It doesn't contain any test target, so test artifacts contains only IPA file.","title":"FlankGameLoopExample"},{"location":"test_artifacts/#generate_2","text":"Run: flankScripts shell ops build_ios_gameloop_example .","title":"Generate"},{"location":"test_artifacts/#source-code_2","text":"test_projects/ios/EarlGreyExample \u26a0\ufe0f NOTE: Generating IPA requires Apple distribution certificate therefore for now it's not possible to generate it without correct Apple Developer Account. build_ios_gameloop_example is excluded when building all iOS artifacts: update_test_artifacts ios","title":"Source Code"},{"location":"test_artifacts/#flanktestplansexample","text":"iOS project with XCTestPlans. Contains AllTests test plan. Generated .xctestrun is using V2 format. More details about test plans: docs/feature/ios_test_plans.md","title":"FlankTestPlansExample"},{"location":"test_artifacts/#generate_3","text":"Run: flankScripts shell ops build_ios_testplans_example .","title":"Generate"},{"location":"test_artifacts/#source-code_3","text":"test_projects/ios/EarlGreyExample","title":"Source Code"},{"location":"test_sharding/","text":"Test Sharding Orchestrator Android Test Orchestrator removes shared state and isolates crashes. Orchestrator trades performance for stability. Tests run slower when orchestrator is enabled. Orchestrator ensures each tests runs Each test runs in a new Instrumentation instance to ensure there's no shared state. It's recommended to use clearPackageData as well to remove file system state. When a test crashes, only that tests instrumentation instance crashes which enables other tests in the suite to continue execution. Without orchestrator, a test crash will break the entire test suite. gcloud : use-orchestrator : true Orchestrator + Firebase Test lab Orchestrator is enabled by default in Flank and disabled by default in gcloud CLI. AndroidX Test 1.3.0 Beta02 or later is required to run parameterized tests with orchestrator. Both the runner and orchestrator must be updated. Local execution will use the version of Orchestrator defined in your gradle file. Firebase Test Lab currently uses Orchestrator v1.3.0. Firebase must manually upgrade Orchestrator on their devices for customers to use the new version. GCloud Sharding The gcloud CLI supports two types of sharding, uniform sharding and manual sharding via test targets for shard. --num-uniform-shards is translated to \u201c-e numShard\u201d \u201c-e shardIndex\u201d AndroidJUnitRunner arguments. This uses the native adb am instrument sharding feature which randomly shards all tests. When using uniform shards, it's possible to have shards with empty tests. Firebase will mark shards as failed when execution consists of skipped/empty tests, as this is likely an indication the tests are not configured correctly. This firebase design choice is incompatable with num-uniform-shards as you will randomly get failures when shards are empty. Using num-uniform-shards is therefor not recommended. --test-targets-for-shard allows manually specifying tests to be run. On FTL --test-targets or --test-targets-for-shard are passed as arguments to adb shell am instrument . adb shell am instrument -r -w -e class com.example.test_app.ParameterizedTest#shouldHopefullyPass com.example.test_app.test/androidx.test.runner.AndroidJUnitRunner Flank The Flank supports two types of sharding, automatic sharding and uniform sharding. Automatic sharding is enabled by default and can be disabled by --disable-sharding=true . Under the hood automatic sharding uses same API as gcloud's --test-targets-for-shard but in difference to gcloud it creates shards automatically. --dump-shards can help with verifying if calculates shards are correct. Automated sharding can work with --test-targets option. --num-uniform-shards provides same functionality as gcloud's option. Is not compatible with automatic sharding. Parameterized tests Flank v20.06.1 fix some compatibility issues with named parameterized tests, when running with sharding. \\ Table below bases on report from running parameterized tests with different configurations. Flank uses same API as gcloud so everything supported by gcloud should be also supported by flank. orchestrator disabled disabled 1.3.0-rc01 1.3.0-rc01 sharding disabled enabled disabled enabled local @RunWith(Parameterized::class) OK OK OK OK local @RunWith(Parameterized::class) {named} OK OK OK OK local @RunWith(JUnitParamsRunner::class) OK OK OK OK gcloud @RunWith(Parameterized::class) OK OK OK OK gcloud @RunWith(Parameterized::class) {named} OK OK OK OK gcloud @RunWith(JUnitParamsRunner::class) OK OK null null flank v20.06.0 @RunWith(Parameterized::class) OK OK OK OK flank v20.06.0 @RunWith(Parameterized::class) {named} OK missing OK missing flank v20.06.0 @RunWith(JUnitParamsRunner::class) OK missing null missing flank v20.06.1 @RunWith(Parameterized::class) OK OK OK OK flank v20.06.1 @RunWith(Parameterized::class) {named} OK OK OK OK flank v20.06.1 @RunWith(JUnitParamsRunner::class) OK OK null null JUnit5 Android instrumented tests has no official support for JUnit5. There is third-party support","title":"Sharding"},{"location":"test_sharding/#test-sharding","text":"","title":"Test Sharding"},{"location":"test_sharding/#orchestrator","text":"Android Test Orchestrator removes shared state and isolates crashes. Orchestrator trades performance for stability. Tests run slower when orchestrator is enabled. Orchestrator ensures each tests runs Each test runs in a new Instrumentation instance to ensure there's no shared state. It's recommended to use clearPackageData as well to remove file system state. When a test crashes, only that tests instrumentation instance crashes which enables other tests in the suite to continue execution. Without orchestrator, a test crash will break the entire test suite. gcloud : use-orchestrator : true","title":"Orchestrator"},{"location":"test_sharding/#orchestrator-firebase-test-lab","text":"Orchestrator is enabled by default in Flank and disabled by default in gcloud CLI. AndroidX Test 1.3.0 Beta02 or later is required to run parameterized tests with orchestrator. Both the runner and orchestrator must be updated. Local execution will use the version of Orchestrator defined in your gradle file. Firebase Test Lab currently uses Orchestrator v1.3.0. Firebase must manually upgrade Orchestrator on their devices for customers to use the new version.","title":"Orchestrator + Firebase Test lab"},{"location":"test_sharding/#gcloud-sharding","text":"The gcloud CLI supports two types of sharding, uniform sharding and manual sharding via test targets for shard. --num-uniform-shards is translated to \u201c-e numShard\u201d \u201c-e shardIndex\u201d AndroidJUnitRunner arguments. This uses the native adb am instrument sharding feature which randomly shards all tests. When using uniform shards, it's possible to have shards with empty tests. Firebase will mark shards as failed when execution consists of skipped/empty tests, as this is likely an indication the tests are not configured correctly. This firebase design choice is incompatable with num-uniform-shards as you will randomly get failures when shards are empty. Using num-uniform-shards is therefor not recommended. --test-targets-for-shard allows manually specifying tests to be run. On FTL --test-targets or --test-targets-for-shard are passed as arguments to adb shell am instrument . adb shell am instrument -r -w -e class com.example.test_app.ParameterizedTest#shouldHopefullyPass com.example.test_app.test/androidx.test.runner.AndroidJUnitRunner","title":"GCloud Sharding"},{"location":"test_sharding/#flank","text":"The Flank supports two types of sharding, automatic sharding and uniform sharding. Automatic sharding is enabled by default and can be disabled by --disable-sharding=true . Under the hood automatic sharding uses same API as gcloud's --test-targets-for-shard but in difference to gcloud it creates shards automatically. --dump-shards can help with verifying if calculates shards are correct. Automated sharding can work with --test-targets option. --num-uniform-shards provides same functionality as gcloud's option. Is not compatible with automatic sharding.","title":"Flank"},{"location":"test_sharding/#parameterized-tests","text":"Flank v20.06.1 fix some compatibility issues with named parameterized tests, when running with sharding. \\ Table below bases on report from running parameterized tests with different configurations. Flank uses same API as gcloud so everything supported by gcloud should be also supported by flank. orchestrator disabled disabled 1.3.0-rc01 1.3.0-rc01 sharding disabled enabled disabled enabled local @RunWith(Parameterized::class) OK OK OK OK local @RunWith(Parameterized::class) {named} OK OK OK OK local @RunWith(JUnitParamsRunner::class) OK OK OK OK gcloud @RunWith(Parameterized::class) OK OK OK OK gcloud @RunWith(Parameterized::class) {named} OK OK OK OK gcloud @RunWith(JUnitParamsRunner::class) OK OK null null flank v20.06.0 @RunWith(Parameterized::class) OK OK OK OK flank v20.06.0 @RunWith(Parameterized::class) {named} OK missing OK missing flank v20.06.0 @RunWith(JUnitParamsRunner::class) OK missing null missing flank v20.06.1 @RunWith(Parameterized::class) OK OK OK OK flank v20.06.1 @RunWith(Parameterized::class) {named} OK OK OK OK flank v20.06.1 @RunWith(JUnitParamsRunner::class) OK OK null null","title":"Parameterized tests"},{"location":"test_sharding/#junit5","text":"Android instrumented tests has no official support for JUnit5. There is third-party support","title":"JUnit5"},{"location":"update_gradle/","text":"Gradle Updating Gradle brew upgrade gradle gradle --version gradle wrapper --distribution-type all Specifiy the gradle distribution type in build.gradle : wrapper { distributionType = Wrapper.DistributionType.ALL }","title":"Gradle"},{"location":"update_gradle/#gradle","text":"","title":"Gradle"},{"location":"update_gradle/#updating-gradle","text":"brew upgrade gradle gradle --version gradle wrapper --distribution-type all Specifiy the gradle distribution type in build.gradle : wrapper { distributionType = Wrapper.DistributionType.ALL }","title":"Updating Gradle"},{"location":"using_different_environment_variables_in_different_matrices/","text":"Using different environment variables per test apk The problem Environment variables are used to configure test coverage. When you configure this in the global scope (gcloud:environment-variables) all of the matrices have the same test coverage file name. Example: gcloud: app: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environment-variables: coverage: true coverageFile: /sdcard/coverage.ec clearPackageData: true directories-to-pull: - /sdcard/ use-orchestrator: false In the case where you have configured additional test apks, all of the matrices have a coverage file named coverage.ec Solution You can override environment variables by configuring it in additional-app-test-apks Example: gcloud: app: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environment-variables: coverage: true coverageFile: /sdcard/main.ec clearPackageData: true directories-to-pull: - /sdcard/ use-orchestrator: false flank: disable-sharding: false max-test-shards: 2 files-to-download: - .*/sdcard/[^/]+\\.ec$ additional-app-test-apks: - test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environmentVariables: coverageFile: /sdcard/module_1.ec - test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environmentVariables: coverageFile: /sdcard/module_2.ec Now Flank override coverageFile in matrices and you can identify what matrix run what test","title":"Using different environment variables per test apk"},{"location":"using_different_environment_variables_in_different_matrices/#using-different-environment-variables-per-test-apk","text":"","title":"Using different environment variables per test apk"},{"location":"using_different_environment_variables_in_different_matrices/#the-problem","text":"Environment variables are used to configure test coverage. When you configure this in the global scope (gcloud:environment-variables) all of the matrices have the same test coverage file name. Example: gcloud: app: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environment-variables: coverage: true coverageFile: /sdcard/coverage.ec clearPackageData: true directories-to-pull: - /sdcard/ use-orchestrator: false In the case where you have configured additional test apks, all of the matrices have a coverage file named coverage.ec","title":"The problem"},{"location":"using_different_environment_variables_in_different_matrices/#solution","text":"You can override environment variables by configuring it in additional-app-test-apks Example: gcloud: app: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-debug.apk test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environment-variables: coverage: true coverageFile: /sdcard/main.ec clearPackageData: true directories-to-pull: - /sdcard/ use-orchestrator: false flank: disable-sharding: false max-test-shards: 2 files-to-download: - .*/sdcard/[^/]+\\.ec$ additional-app-test-apks: - test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environmentVariables: coverageFile: /sdcard/module_1.ec - test: ./src/test/kotlin/ftl/fixtures/tmp/apk/app-multiple-success-debug-androidTest.apk environmentVariables: coverageFile: /sdcard/module_2.ec Now Flank override coverageFile in matrices and you can identify what matrix run what test","title":"Solution"},{"location":"windows_and_github_actions/","text":"Working with Windows on Github actions Differences between system property user.home and environment variable HOMEPATH %HOMEPATH% in a batch scripts returns D:\\Users\\runneradmin\\ In Java System.getProperty(\"user.home\") returns C:\\Users\\runneradmin\\ For Windows recommended is using System.getenv(\"HOMEPATH\") instead of System.getProperty(\"user.home\")","title":"Working with Windows on Github actions"},{"location":"windows_and_github_actions/#working-with-windows-on-github-actions","text":"","title":"Working with Windows on Github actions"},{"location":"windows_and_github_actions/#differences-between-system-property-userhome-and-environment-variable-homepath","text":"%HOMEPATH% in a batch scripts returns D:\\Users\\runneradmin\\ In Java System.getProperty(\"user.home\") returns C:\\Users\\runneradmin\\ For Windows recommended is using System.getenv(\"HOMEPATH\") instead of System.getProperty(\"user.home\")","title":"Differences between system property user.home and environment variable HOMEPATH"},{"location":"windows_wsl_guide/","text":"Building and running Flank on Windows WSL It is possible to build and run Flank on Windows through WSL. You could configure it using your own Windows machine or you could use GitHub actions to do this. Building Own Windows machine Install WSL on Windows using Microsoft Guide Launch WSL console Install JDK if you do not have any installed sudo apt-get install openjdk-15-jre sudo apt-get install openjdk-15-jdk export JAVA_HOME=/usr/lib/jvm/openjdk-15-jdk export PATH=$PATH:$JAVA_HOME/bin Install dos2unix using the command sudo apt-get install dos2unix Convert files to UNIX in your Flank repository directory find . -type f -print0 | xargs -0 -n 1 -P 4 dos2unix Make Gradlew wrapper executable chmod +x gradlew Build Flank using command ./gradlew clean build GitHub actions Setup WSL on windows-2019 runner runs-on: windows-2019 Setup default shell to wsl-bash {0} this will allow avoiding typing wsl-bash on start each command defaults: run: shell: wsl-bash {0} Use GitHub action for setup WSL - uses: Vampire/setup-wsl@v1 with: distribution: Ubuntu-18.04 additional-packages: dos2unix In order to build flank you should install java, add permission to Gradle wrapper file and prepare each file using dos2unix - name: Configure WSL for flank run: | find . -type f -print0 | xargs -0 -n 1 -P 4 dos2unix chmod +x gradlew sudo apt-get -y install openjdk-8-jdk After this step you could build flank like in every UNIX machine ./gradlew clean build For reference please check Flank team implementation of WSL workflow Running After building using the above steps or downloading using the command wget --quiet https://github.com/Flank/flank/releases/download/XXX/flank.jar -O ./flank.jar where XXX is the latest version of flank from Flank releases on GitHub You could run Flank both on your own machine or GitHub actions typing the command: java -jar <PATH TO FLANK> <COMMANDS> <OPTIONS> You could also add Flank's bash helper folder to your $PATH environment variable. This will allow you to call the shell scripts in that helper folder from anywhere.","title":"Windows WSL"},{"location":"windows_wsl_guide/#building-and-running-flank-on-windows-wsl","text":"It is possible to build and run Flank on Windows through WSL. You could configure it using your own Windows machine or you could use GitHub actions to do this.","title":"Building and running Flank on Windows WSL"},{"location":"windows_wsl_guide/#building","text":"","title":"Building"},{"location":"windows_wsl_guide/#own-windows-machine","text":"Install WSL on Windows using Microsoft Guide Launch WSL console Install JDK if you do not have any installed sudo apt-get install openjdk-15-jre sudo apt-get install openjdk-15-jdk export JAVA_HOME=/usr/lib/jvm/openjdk-15-jdk export PATH=$PATH:$JAVA_HOME/bin Install dos2unix using the command sudo apt-get install dos2unix Convert files to UNIX in your Flank repository directory find . -type f -print0 | xargs -0 -n 1 -P 4 dos2unix Make Gradlew wrapper executable chmod +x gradlew Build Flank using command ./gradlew clean build","title":"Own Windows machine"},{"location":"windows_wsl_guide/#github-actions","text":"Setup WSL on windows-2019 runner runs-on: windows-2019 Setup default shell to wsl-bash {0} this will allow avoiding typing wsl-bash on start each command defaults: run: shell: wsl-bash {0} Use GitHub action for setup WSL - uses: Vampire/setup-wsl@v1 with: distribution: Ubuntu-18.04 additional-packages: dos2unix In order to build flank you should install java, add permission to Gradle wrapper file and prepare each file using dos2unix - name: Configure WSL for flank run: | find . -type f -print0 | xargs -0 -n 1 -P 4 dos2unix chmod +x gradlew sudo apt-get -y install openjdk-8-jdk After this step you could build flank like in every UNIX machine ./gradlew clean build For reference please check Flank team implementation of WSL workflow","title":"GitHub actions"},{"location":"windows_wsl_guide/#running","text":"After building using the above steps or downloading using the command wget --quiet https://github.com/Flank/flank/releases/download/XXX/flank.jar -O ./flank.jar where XXX is the latest version of flank from Flank releases on GitHub You could run Flank both on your own machine or GitHub actions typing the command: java -jar <PATH TO FLANK> <COMMANDS> <OPTIONS> You could also add Flank's bash helper folder to your $PATH environment variable. This will allow you to call the shell scripts in that helper folder from anywhere.","title":"Running"},{"location":"bugs/1374_invalid_class/","text":"[Parametrized tests] #1374 - java.lang.ClassNotFoundException: Invalid name: Bug description Test code with parametrized code and custom name @RunWith ( Parameterized :: class ) class BrokenTestName2 ( private val testCase : TestCase ) { @Test fun test () { val expectedRemainder = if ( testCase . odd ) 1 else 0 for ( value in testCase . values ) { assertEquals ( expectedRemainder , abs ( value % 2 )) } } data class TestCase ( val values : List < Int >, val odd : Boolean , ) companion object { @ [ JvmStatic Parameterized . Parameters ( name = \"{0}\" )] fun parameters (): List < TestCase > = listOf ( TestCase ( listOf (- 4 , - 2 , 0 , 2 , 4 ), odd = false ), TestCase ( listOf (- 3 , - 5 , - 1 , 1 , 3 , 5 ), odd = true ), ) } } Will produce failures such as: java.lang.ClassNotFoundException: Invalid name: 3 at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:324) at androidx.test.internal.runner.TestLoader.doCreateRunner(TestLoader.java:72) at androidx.test.internal.runner.TestLoader.getRunnersFor(TestLoader.java:105) at androidx.test.internal.runner.TestRequestBuilder.build(TestRequestBuilder.java:804) at androidx.test.runner.AndroidJUnitRunner.buildRequest(AndroidJUnitRunner.java:575) at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:393) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1879) and similar failures for other values interspersed with commas, e.g. java.lang.ClassNotFoundException: Invalid name: -2 , etc. Such failures appear like this in the FTL UI: Test details No matter if sharding is disabled or not, tests will always fail both on GCloud and Flank Solution Disable test orchestrator with yaml option: gcloud : ... use-orchestrator : false ... flank : ... or CLI command: --no-use-orchestrator More info Please take a look at documentation about sharding for more info","title":"[Parametrized tests] #1374 - java.lang.ClassNotFoundException: Invalid name:"},{"location":"bugs/1374_invalid_class/#parametrized-tests-1374-javalangclassnotfoundexception-invalid-name","text":"","title":"[Parametrized tests] #1374 - java.lang.ClassNotFoundException: Invalid name:"},{"location":"bugs/1374_invalid_class/#bug-description","text":"Test code with parametrized code and custom name @RunWith ( Parameterized :: class ) class BrokenTestName2 ( private val testCase : TestCase ) { @Test fun test () { val expectedRemainder = if ( testCase . odd ) 1 else 0 for ( value in testCase . values ) { assertEquals ( expectedRemainder , abs ( value % 2 )) } } data class TestCase ( val values : List < Int >, val odd : Boolean , ) companion object { @ [ JvmStatic Parameterized . Parameters ( name = \"{0}\" )] fun parameters (): List < TestCase > = listOf ( TestCase ( listOf (- 4 , - 2 , 0 , 2 , 4 ), odd = false ), TestCase ( listOf (- 3 , - 5 , - 1 , 1 , 3 , 5 ), odd = true ), ) } } Will produce failures such as: java.lang.ClassNotFoundException: Invalid name: 3 at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:324) at androidx.test.internal.runner.TestLoader.doCreateRunner(TestLoader.java:72) at androidx.test.internal.runner.TestLoader.getRunnersFor(TestLoader.java:105) at androidx.test.internal.runner.TestRequestBuilder.build(TestRequestBuilder.java:804) at androidx.test.runner.AndroidJUnitRunner.buildRequest(AndroidJUnitRunner.java:575) at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:393) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1879) and similar failures for other values interspersed with commas, e.g. java.lang.ClassNotFoundException: Invalid name: -2 , etc. Such failures appear like this in the FTL UI:","title":"Bug description"},{"location":"bugs/1374_invalid_class/#test-details","text":"No matter if sharding is disabled or not, tests will always fail both on GCloud and Flank","title":"Test details"},{"location":"bugs/1374_invalid_class/#solution","text":"Disable test orchestrator with yaml option: gcloud : ... use-orchestrator : false ... flank : ... or CLI command: --no-use-orchestrator","title":"Solution"},{"location":"bugs/1374_invalid_class/#more-info","text":"Please take a look at documentation about sharding for more info","title":"More info"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/","text":"Non-deterministic issue when parsing JUnitReport.xml generated by Flank 20.06.2 #872 Description After updating from Flank 8.0.1 to 20.06.2 and switching to server-side sharding sometimes on our CI (TeamCity) we see following error when parsing JUnitReport.xml : [17:00:00]Ant JUnit report watcher [17:00:00][Ant JUnit report watcher] 1 report found for paths: [17:00:00][Ant JUnit report watcher] results/**/JUnitReport.xml [17:00:00][Ant JUnit report watcher] Parsing errors [17:00:00][Parsing errors] Failed to parse 1 report [17:00:00][Parsing errors] results/2020-06-25_16-47-33.675000_QnCW/JUnitReport.xml: Content is not allowed in prolog. [17:00:00][Parsing errors] jetbrains.buildServer.util.XmlXppAbstractParser$3: Content is not allowed in prolog. at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:39) at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:31) at jetbrains.buildServer.xmlReportPlugin.parsers.antJUnit.AntJUnitReportParser.parse(AntJUnitReportParser.java:179) at jetbrains.buildServer.xmlReportPlugin.ParseReportCommand.run(ParseReportCommand.java:62) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog. at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source) at jetbrains.buildServer.util.XmlXppAbstractParser.parseWithSAX(XmlXppAbstractParser.java:240) at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:37) ... 8 more The issue is non-deterministic, in most cases the report gets parsed, but this never happened to us when using Flank 8. Did anyone already run into the problem? Any ideas what could be a cause? The XML files look correct for me, I compared two files from failed and successful runs, the difference is only in reported time of running tests, both files do not contain BOM marker (checked using vim). To Reproduce Run tests using Flank and then try to parse the report using TeamCity's report parsing feature. Expected behavior No error raised by TeamCity when parsing the JUnitReport.xml . Details (please complete the following information): Flank 20.06.2 Additional info Asked user for providing raport which fails on TeamCity CI and he replied with failing report fail: <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"Pixel2-27-en-portrait\" tests= \"2\" failures= \"0\" flakes= \"0\" errors= \"0\" skipped= \"0\" time= \"39.151\" timestamp= \"2020-07-01T16:46:41\" hostname= \"localhost\" > <testcase name= \"loginSuccessBetaEnvTest\" classname= \"com.miquido.play360.login.LoginBetaEnvTest\" time= \"19.133\" /> <testcase name= \"testPaymentsCard\" classname= \"com.miquido.play360.dashboard.PaymentCardBetaEnvTest\" time= \"20.019\" /> </testsuite> </testsuites> Problem investigation techniques Validate XML Description Attached XML report was checked using some online tools to validate XML files Execution Following sites were used to validate XML (https://www.xmlvalidation.com/, https://www.w3schools.com/xml/xml_validator.asp, https://www.freeformatter.com/xml-validator-xsd.html, https://codebeautify.org/xmlvalidator, https://www.liquid-technologies.com/online-xml-validator) Results All tools confirm that provided XML report is valid Clear XML buffer unnecessary characters Description During research about stack trace provided in the task description, user suggest on stack overflow that some XML files could contain bad whitespace characters which are bytes order marked Execution Change JUnitXML.kt file to trim characters suggested in post fun JUnitTestResult ?. xmlToString (): String { if ( this == null ) return \"\" val prefix = \"<?xml version='1.0' encoding='UTF-8'?>\" return ( prefix + System . lineSeparator () + xmlPrettyWriter . writeValueAsString ( this )). clearByteOrderMarkersFromBuffer () } // According to occasional problem with parsing byte order markers should be cleared from buffer // https://stackoverflow.com/a/3030913 private fun String . clearByteOrderMarkersFromBuffer () = trim (). replaceFirst ( \"^([\\\\W]+)<\" , \"<\" ) Try following code when reading file and compare it without using option to clear buffer Results No characters were deleted, XML has the same length with and without suggested \"fix\" Checking hex code of provided raport Description XML report hex code was checked to make sure that no unnecessary characters are added to the file. Execution The report was parsed to hex representation of chars and then in revert side. Used websites (https://tomeko.net/online_tools/file_to_hex.php, https://tomeko.net/online_tools/hex_to_file.php) The file was checked using online hex viewer Results No unnecessary characters found on file. Compare reading report with using Kotlin file loading Description By using java.nio.File(\"JUnitReport.xml\") methods readLines() / length() / readText() checking is provided file is different than generated similar one (also with clearing byte order markers) Results No issues found in the report Checking git history Description Checked git history of XML reporting code. Results No breaking changes were introduced between Flank 8.0.1 and 20.06.2 Ask user about TeamCity configuration Description User was asked about agents count and configuration Results All agents are docker's containers so they are always the same and the problem isn't here. User have 2 types of tests Using MockWebServer and have ~140 tests executed on multiple shards Using real backend, not many tests executed on single shards - this type of tests sometime falling When user back to old configuration, he can still reproduce problem Between versions user changed config from: num-uniform-shards: 1 to num-test-runs: 1 But when user back to old configuration, he can still reproduce problem. Current situation Issue reported to TeamCity https://youtrack.jetbrains.com/issue/TW-66753 TeamCity team cannot reproduce the problem, thay asked about additional logs, but User could deliver them in next week (starting 13-07-2020), until that we must wait. User is testing TeamCity CI with Flank 8 to make sure that the problem is related to Flank","title":"Non-deterministic issue when parsing JUnitReport.xml generated by Flank 20.06.2 [#872](https://github.com/Flank/flank/issues/872)"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#non-deterministic-issue-when-parsing-junitreportxml-generated-by-flank-20062-872","text":"","title":"Non-deterministic issue when parsing JUnitReport.xml generated by Flank 20.06.2 #872"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description","text":"After updating from Flank 8.0.1 to 20.06.2 and switching to server-side sharding sometimes on our CI (TeamCity) we see following error when parsing JUnitReport.xml : [17:00:00]Ant JUnit report watcher [17:00:00][Ant JUnit report watcher] 1 report found for paths: [17:00:00][Ant JUnit report watcher] results/**/JUnitReport.xml [17:00:00][Ant JUnit report watcher] Parsing errors [17:00:00][Parsing errors] Failed to parse 1 report [17:00:00][Parsing errors] results/2020-06-25_16-47-33.675000_QnCW/JUnitReport.xml: Content is not allowed in prolog. [17:00:00][Parsing errors] jetbrains.buildServer.util.XmlXppAbstractParser$3: Content is not allowed in prolog. at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:39) at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:31) at jetbrains.buildServer.xmlReportPlugin.parsers.antJUnit.AntJUnitReportParser.parse(AntJUnitReportParser.java:179) at jetbrains.buildServer.xmlReportPlugin.ParseReportCommand.run(ParseReportCommand.java:62) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog. at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source) at jetbrains.buildServer.util.XmlXppAbstractParser.parseWithSAX(XmlXppAbstractParser.java:240) at jetbrains.buildServer.util.XmlXppAbstractParser.parse(XmlXppAbstractParser.java:37) ... 8 more The issue is non-deterministic, in most cases the report gets parsed, but this never happened to us when using Flank 8. Did anyone already run into the problem? Any ideas what could be a cause? The XML files look correct for me, I compared two files from failed and successful runs, the difference is only in reported time of running tests, both files do not contain BOM marker (checked using vim). To Reproduce Run tests using Flank and then try to parse the report using TeamCity's report parsing feature. Expected behavior No error raised by TeamCity when parsing the JUnitReport.xml . Details (please complete the following information): Flank 20.06.2","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#additional-info","text":"Asked user for providing raport which fails on TeamCity CI and he replied with failing report fail: <?xml version='1.0' encoding='UTF-8' ?> <testsuites> <testsuite name= \"Pixel2-27-en-portrait\" tests= \"2\" failures= \"0\" flakes= \"0\" errors= \"0\" skipped= \"0\" time= \"39.151\" timestamp= \"2020-07-01T16:46:41\" hostname= \"localhost\" > <testcase name= \"loginSuccessBetaEnvTest\" classname= \"com.miquido.play360.login.LoginBetaEnvTest\" time= \"19.133\" /> <testcase name= \"testPaymentsCard\" classname= \"com.miquido.play360.dashboard.PaymentCardBetaEnvTest\" time= \"20.019\" /> </testsuite> </testsuites>","title":"Additional info"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#problem-investigation-techniques","text":"","title":"Problem investigation techniques"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#validate-xml","text":"","title":"Validate XML"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_1","text":"Attached XML report was checked using some online tools to validate XML files","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#execution","text":"Following sites were used to validate XML (https://www.xmlvalidation.com/, https://www.w3schools.com/xml/xml_validator.asp, https://www.freeformatter.com/xml-validator-xsd.html, https://codebeautify.org/xmlvalidator, https://www.liquid-technologies.com/online-xml-validator)","title":"Execution"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results","text":"All tools confirm that provided XML report is valid","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#clear-xml-buffer-unnecessary-characters","text":"","title":"Clear XML buffer unnecessary characters"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_2","text":"During research about stack trace provided in the task description, user suggest on stack overflow that some XML files could contain bad whitespace characters which are bytes order marked","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#execution_1","text":"Change JUnitXML.kt file to trim characters suggested in post fun JUnitTestResult ?. xmlToString (): String { if ( this == null ) return \"\" val prefix = \"<?xml version='1.0' encoding='UTF-8'?>\" return ( prefix + System . lineSeparator () + xmlPrettyWriter . writeValueAsString ( this )). clearByteOrderMarkersFromBuffer () } // According to occasional problem with parsing byte order markers should be cleared from buffer // https://stackoverflow.com/a/3030913 private fun String . clearByteOrderMarkersFromBuffer () = trim (). replaceFirst ( \"^([\\\\W]+)<\" , \"<\" ) Try following code when reading file and compare it without using option to clear buffer","title":"Execution"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_1","text":"No characters were deleted, XML has the same length with and without suggested \"fix\"","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#checking-hex-code-of-provided-raport","text":"","title":"Checking hex code of provided raport"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_3","text":"XML report hex code was checked to make sure that no unnecessary characters are added to the file.","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#execution_2","text":"The report was parsed to hex representation of chars and then in revert side. Used websites (https://tomeko.net/online_tools/file_to_hex.php, https://tomeko.net/online_tools/hex_to_file.php) The file was checked using online hex viewer","title":"Execution"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_2","text":"No unnecessary characters found on file.","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#compare-reading-report-with-using-kotlin-file-loading","text":"","title":"Compare reading report with using Kotlin file loading"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_4","text":"By using java.nio.File(\"JUnitReport.xml\") methods readLines() / length() / readText() checking is provided file is different than generated similar one (also with clearing byte order markers)","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_3","text":"No issues found in the report","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#checking-git-history","text":"","title":"Checking git history"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_5","text":"Checked git history of XML reporting code.","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_4","text":"No breaking changes were introduced between Flank 8.0.1 and 20.06.2","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#ask-user-about-teamcity-configuration","text":"","title":"Ask user about TeamCity configuration"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#description_6","text":"User was asked about agents count and configuration","title":"Description"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#results_5","text":"All agents are docker's containers so they are always the same and the problem isn't here. User have 2 types of tests Using MockWebServer and have ~140 tests executed on multiple shards Using real backend, not many tests executed on single shards - this type of tests sometime falling When user back to old configuration, he can still reproduce problem Between versions user changed config from: num-uniform-shards: 1 to num-test-runs: 1 But when user back to old configuration, he can still reproduce problem.","title":"Results"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#current-situation","text":"","title":"Current situation"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#issue-reported-to-teamcity-httpsyoutrackjetbrainscomissuetw-66753","text":"TeamCity team cannot reproduce the problem, thay asked about additional logs, but User could deliver them in next week (starting 13-07-2020), until that we must wait.","title":"Issue reported to TeamCity https://youtrack.jetbrains.com/issue/TW-66753"},{"location":"bugs/872_error_when_parsing_Junit_raport_using_TeamCityCI/#user-is-testing-teamcity-ci-with-flank-8-to-make-sure-that-the-problem-is-related-to-flank","text":"","title":"User is testing TeamCity CI with Flank 8 to make sure that the problem is related to Flank"},{"location":"bugs/891-rate-limit-exceeded/","text":"Rate limit exceeded #891 Reported description: After bump from 20.05.2 to 20.06.2 I started to see some issues related to rate limit. Also, there are no significant changes on the shard size or test amount. We are running another 7 flank execution in parallel. Total ~6k tests. Stack trace java.io.IOException: Request failed at ftl.http.ExecuteWithRetryKt$executeWithRetry$$inlined$withRetry$1.invokeSuspend(ExecuteWithRetry.kt:36) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at ftl.http.ExecuteWithRetryKt.executeWithRetry(ExecuteWithRetry.kt:41) at ftl.gc.GcToolResults.getStepResult(GcToolResults.kt:98) at ftl.json.SavedMatrix.finished(SavedMatrix.kt:90) at ftl.json.SavedMatrix.update(SavedMatrix.kt:65) at ftl.json.MatrixMapKt.update(MatrixMap.kt:47) at ftl.run.NewTestRunKt$newTestRun$2$2.invokeSuspend(NewTestRun.kt:24) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.ResumeModeKt.resumeUninterceptedMode(ResumeMode.kt:45) at kotlinx.coroutines.internal.ScopeCoroutine.afterCompletionInternal(Scopes.kt:32) at kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310) at kotlinx.coroutines.JobSupport.tryFinalizeSimpleState(JobSupport.kt:276) at kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:807) at kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.ResumeModeKt.resumeUninterceptedMode(ResumeMode.kt:45) at kotlinx.coroutines.internal.ScopeCoroutine.afterCompletionInternal(Scopes.kt:32) at kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310) at kotlinx.coroutines.JobSupport.tryFinalizeFinishingState(JobSupport.kt:236) at kotlinx.coroutines.JobSupport.tryMakeCompletingSlowPath(JobSupport.kt:849) at kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:811) at kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at ftl.cli.firebase.test.android.AndroidRunCommand.run(AndroidRunCommand.kt:48) at picocli.CommandLine.executeUserObject(CommandLine.java:1769) at picocli.CommandLine.access$900(CommandLine.java:145) at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2150) at picocli.CommandLine$RunLast.handle(CommandLine.java:2144) at picocli.CommandLine$RunLast.handle(CommandLine.java:2108) at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:1975) at picocli.CommandLine.execute(CommandLine.java:1904) at ftl.Main$Companion$main$1.invoke(Main.kt:53) at ftl.Main$Companion$main$1.invoke(Main.kt:43) at ftl.util.Utils.withGlobalExceptionHandling(Utils.kt:130) at ftl.Main$Companion.main(Main.kt:49) at ftl.Main.main(Main.kt) C used by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests { \"code\" : 429, \"errors\" : [ { \"domain\" : \"global\", \"message\" : \"Quota exceeded for quota group 'default' and limit 'Queries per user per 100 seconds' of service 'toolresults.googleapis.com' for consumer 'project_number:x'.\", \"reason\" : \"rateLimitExceeded\" } ], \"message\" : \"Quota exceeded for quota group 'default' and limit 'Queries per user per 100 seconds' of service 'toolresults.googleapis.com' for consumer 'project_number:x'.\", \"status\" : \"RESOURCE_EXHAUSTED\" } at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150) at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:443) at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1092) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:541) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:474) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:591) at ftl.http.ExecuteWithRetryKt$executeWithRetry$$inlined$withRetry$1.invokeSuspend(ExecuteWithRetry.kt:41) ... 52 more Reported flank config flank : max-test-shards : 15 shard-time : 160 num-test-runs : 1 smart-flank-gcs-path : gs://x/unit-tests.xml smart-flank-disable-upload : false files-to-download : test-targets-always-run : disable-sharding : false project : x local-result-dir : results full-junit-result : true # Android Flank Yml keep-file-path : false additional-app-test-apks : run-timeout : -1 legacy-junit-result : false ignore-failed-tests : false output-style : multi RunTests Smart Flank cache hit : 100% (88 / 88) Shard times : 93s, 93s Uploadingx-app-debug.apk . Uploading x-androidTest.apk . 88 tests / 2 shards To Reproduce shell script Flank.jar firebase test android run --num-flaky-test-attempts=0 --full-junit-result=true ## API usage outline Real examples simplified to pseudo code that outlines only API call usage #### Gcloud Case for disabled sharding * [MonitorTestExecutionProgress](https://github.com/Flank/gcloud_cli/blob/137d864acd5928baf25434cf59b0225c4d1f9319/google-cloud-sdk/lib/googlecloudsdk/api_lib/firebase/test/matrix_ops.py#L164) while (matrix status is not completed) { _client.projects_testMatrices.Get( TestingProjectsTestMatricesGetRequest(projectId, testMatrixId) ) wait(6s) } * [MonitorTestMatrixProgress](https://github.com/Flank/gcloud_cli/blob/137d864acd5928baf25434cf59b0225c4d1f9319/google-cloud-sdk/lib/googlecloudsdk/api_lib/firebase/test/matrix_ops.py#L248) Case for enabled sharding while (matrix status is not completed) { _client.projects_testMatrices.Get( TestingProjectsTestMatricesGetRequest(projectId, testMatrixId) ) wait(6s) } #### Flank v20.06.2 * [pollMatrices](https://github.com/Flank/flank/blob/6ee6939263923953edf67afa7218cf2c496c2ef2/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L19) forEach(test matrix) { async while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } * [SavedMatrix.finished](https://github.com/Flank/flank/blob/c88cb2786de67c0a114fc31a7b25917a035e145b/test_runner/src/main/kotlin/ftl/json/SavedMatrix.kt#L75) forEach(test matrix) { sync forEach(matrix test execution) { GcToolResults.listTestCases(toolResultsStep) GcToolResults.getStepResult(toolResultsStep) } sync forEach(matrix test execution) { GcToolResults.getExecutionResult(testExecution) GcToolResults.getStepResult(toolResultsStep) } } #### Flank v20.05.2 * [pollMatrices](https://github.com/Flank/flank/blob/accca3b941874e9556eea6616b34a9f4319c8746/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L15) // Only the first matrix is getting status updates // the others are blocked until the first is getting completed. // As a result, it reduces the amount of requests to 1 per 5 secs. // That is why this version of the flank is not getting limit exceeded. forEach(test matrix) { sync while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } #### Flank v20.06.2 * [pollMatrices](https://github.com/Flank/flank/blob/6ee6939263923953edf67afa7218cf2c496c2ef2/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L19) forEach(test matrix) { async while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } * [SavedMatrix.finished](https://github.com/Flank/flank/blob/c88cb2786de67c0a114fc31a7b25917a035e145b/test_runner/src/main/kotlin/ftl/json/SavedMatrix.kt#L75) forEach(test matrix) { sync forEach(matrix test execution) { GcToolResults.getStepResult(toolResultsStep) } } ## API calls usage comparision table Following table should compare API calls complexity. | | execution status updates | |:-------:|:------------------------:| | Gcloud | 1 * r / 6s + 1 | | 20.05.2 | 1 * r / 5s + (E * r) | | 20.06.2 | M * r / 5s + (M * E * 4 * r) | r - request s - second M - count of matrixes E - count of test executions in matrix scope ## Conclusions #### Gclud Because of single matrix run gives only 1 request per 6 seconds #### Flank v20.05.2 Gets matrix status updates in synchronize way so only the first matrix is getting status updates, the others are blocked until the first is getting completed, so the amount of requests is reduced to 1 per 5 secs. Plus additional number of requests for each execution in one matrix scope. That is why this version of the flank is not getting a limit exceeded. #### Flank v20.06.2 Is polling results asynchronously for each matrix. At last, it is doing a request for each test execution for each matrix, and this is the place where flank is getting a rate limit exceeded. It is visible on stack trace. at ftl.gc.GcToolResults.getStepResult(GcToolResults.kt:98) at ftl.json.SavedMatrix.finished(SavedMatrix.kt:90)","title":"Rate limit exceeded [#891](https://github.com/Flank/flank/issues/891)"},{"location":"bugs/891-rate-limit-exceeded/#rate-limit-exceeded-891","text":"Reported description: After bump from 20.05.2 to 20.06.2 I started to see some issues related to rate limit. Also, there are no significant changes on the shard size or test amount. We are running another 7 flank execution in parallel. Total ~6k tests.","title":"Rate limit exceeded #891"},{"location":"bugs/891-rate-limit-exceeded/#stack-trace","text":"java.io.IOException: Request failed at ftl.http.ExecuteWithRetryKt$executeWithRetry$$inlined$withRetry$1.invokeSuspend(ExecuteWithRetry.kt:36) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at ftl.http.ExecuteWithRetryKt.executeWithRetry(ExecuteWithRetry.kt:41) at ftl.gc.GcToolResults.getStepResult(GcToolResults.kt:98) at ftl.json.SavedMatrix.finished(SavedMatrix.kt:90) at ftl.json.SavedMatrix.update(SavedMatrix.kt:65) at ftl.json.MatrixMapKt.update(MatrixMap.kt:47) at ftl.run.NewTestRunKt$newTestRun$2$2.invokeSuspend(NewTestRun.kt:24) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.ResumeModeKt.resumeUninterceptedMode(ResumeMode.kt:45) at kotlinx.coroutines.internal.ScopeCoroutine.afterCompletionInternal(Scopes.kt:32) at kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310) at kotlinx.coroutines.JobSupport.tryFinalizeSimpleState(JobSupport.kt:276) at kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:807) at kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.ResumeModeKt.resumeUninterceptedMode(ResumeMode.kt:45) at kotlinx.coroutines.internal.ScopeCoroutine.afterCompletionInternal(Scopes.kt:32) at kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310) at kotlinx.coroutines.JobSupport.tryFinalizeFinishingState(JobSupport.kt:236) at kotlinx.coroutines.JobSupport.tryMakeCompletingSlowPath(JobSupport.kt:849) at kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:811) at kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36) at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source) at ftl.cli.firebase.test.android.AndroidRunCommand.run(AndroidRunCommand.kt:48) at picocli.CommandLine.executeUserObject(CommandLine.java:1769) at picocli.CommandLine.access$900(CommandLine.java:145) at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2150) at picocli.CommandLine$RunLast.handle(CommandLine.java:2144) at picocli.CommandLine$RunLast.handle(CommandLine.java:2108) at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:1975) at picocli.CommandLine.execute(CommandLine.java:1904) at ftl.Main$Companion$main$1.invoke(Main.kt:53) at ftl.Main$Companion$main$1.invoke(Main.kt:43) at ftl.util.Utils.withGlobalExceptionHandling(Utils.kt:130) at ftl.Main$Companion.main(Main.kt:49) at ftl.Main.main(Main.kt) C used by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests { \"code\" : 429, \"errors\" : [ { \"domain\" : \"global\", \"message\" : \"Quota exceeded for quota group 'default' and limit 'Queries per user per 100 seconds' of service 'toolresults.googleapis.com' for consumer 'project_number:x'.\", \"reason\" : \"rateLimitExceeded\" } ], \"message\" : \"Quota exceeded for quota group 'default' and limit 'Queries per user per 100 seconds' of service 'toolresults.googleapis.com' for consumer 'project_number:x'.\", \"status\" : \"RESOURCE_EXHAUSTED\" } at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150) at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:443) at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1092) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:541) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:474) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:591) at ftl.http.ExecuteWithRetryKt$executeWithRetry$$inlined$withRetry$1.invokeSuspend(ExecuteWithRetry.kt:41) ... 52 more","title":"Stack trace"},{"location":"bugs/891-rate-limit-exceeded/#reported-flank-config","text":"flank : max-test-shards : 15 shard-time : 160 num-test-runs : 1 smart-flank-gcs-path : gs://x/unit-tests.xml smart-flank-disable-upload : false files-to-download : test-targets-always-run : disable-sharding : false project : x local-result-dir : results full-junit-result : true # Android Flank Yml keep-file-path : false additional-app-test-apks : run-timeout : -1 legacy-junit-result : false ignore-failed-tests : false output-style : multi RunTests Smart Flank cache hit : 100% (88 / 88) Shard times : 93s, 93s Uploadingx-app-debug.apk . Uploading x-androidTest.apk . 88 tests / 2 shards","title":"Reported flank config"},{"location":"bugs/891-rate-limit-exceeded/#to-reproduce","text":"shell script Flank.jar firebase test android run --num-flaky-test-attempts=0 --full-junit-result=true ## API usage outline Real examples simplified to pseudo code that outlines only API call usage #### Gcloud Case for disabled sharding * [MonitorTestExecutionProgress](https://github.com/Flank/gcloud_cli/blob/137d864acd5928baf25434cf59b0225c4d1f9319/google-cloud-sdk/lib/googlecloudsdk/api_lib/firebase/test/matrix_ops.py#L164) while (matrix status is not completed) { _client.projects_testMatrices.Get( TestingProjectsTestMatricesGetRequest(projectId, testMatrixId) ) wait(6s) } * [MonitorTestMatrixProgress](https://github.com/Flank/gcloud_cli/blob/137d864acd5928baf25434cf59b0225c4d1f9319/google-cloud-sdk/lib/googlecloudsdk/api_lib/firebase/test/matrix_ops.py#L248) Case for enabled sharding while (matrix status is not completed) { _client.projects_testMatrices.Get( TestingProjectsTestMatricesGetRequest(projectId, testMatrixId) ) wait(6s) } #### Flank v20.06.2 * [pollMatrices](https://github.com/Flank/flank/blob/6ee6939263923953edf67afa7218cf2c496c2ef2/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L19) forEach(test matrix) { async while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } * [SavedMatrix.finished](https://github.com/Flank/flank/blob/c88cb2786de67c0a114fc31a7b25917a035e145b/test_runner/src/main/kotlin/ftl/json/SavedMatrix.kt#L75) forEach(test matrix) { sync forEach(matrix test execution) { GcToolResults.listTestCases(toolResultsStep) GcToolResults.getStepResult(toolResultsStep) } sync forEach(matrix test execution) { GcToolResults.getExecutionResult(testExecution) GcToolResults.getStepResult(toolResultsStep) } } #### Flank v20.05.2 * [pollMatrices](https://github.com/Flank/flank/blob/accca3b941874e9556eea6616b34a9f4319c8746/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L15) // Only the first matrix is getting status updates // the others are blocked until the first is getting completed. // As a result, it reduces the amount of requests to 1 per 5 secs. // That is why this version of the flank is not getting limit exceeded. forEach(test matrix) { sync while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } #### Flank v20.06.2 * [pollMatrices](https://github.com/Flank/flank/blob/6ee6939263923953edf67afa7218cf2c496c2ef2/test_runner/src/main/kotlin/ftl/run/common/PollMatrices.kt#L19) forEach(test matrix) { async while(matrix status is not completed) { GcTestMatrix.refresh(testMatrixId, projectId) wait(5s) } } * [SavedMatrix.finished](https://github.com/Flank/flank/blob/c88cb2786de67c0a114fc31a7b25917a035e145b/test_runner/src/main/kotlin/ftl/json/SavedMatrix.kt#L75) forEach(test matrix) { sync forEach(matrix test execution) { GcToolResults.getStepResult(toolResultsStep) } } ## API calls usage comparision table Following table should compare API calls complexity. | | execution status updates | |:-------:|:------------------------:| | Gcloud | 1 * r / 6s + 1 | | 20.05.2 | 1 * r / 5s + (E * r) | | 20.06.2 | M * r / 5s + (M * E * 4 * r) | r - request s - second M - count of matrixes E - count of test executions in matrix scope ## Conclusions #### Gclud Because of single matrix run gives only 1 request per 6 seconds #### Flank v20.05.2 Gets matrix status updates in synchronize way so only the first matrix is getting status updates, the others are blocked until the first is getting completed, so the amount of requests is reduced to 1 per 5 secs. Plus additional number of requests for each execution in one matrix scope. That is why this version of the flank is not getting a limit exceeded. #### Flank v20.06.2 Is polling results asynchronously for each matrix. At last, it is doing a request for each test execution for each matrix, and this is the place where flank is getting a rate limit exceeded. It is visible on stack trace. at ftl.gc.GcToolResults.getStepResult(GcToolResults.kt:98) at ftl.json.SavedMatrix.finished(SavedMatrix.kt:90)","title":"To Reproduce"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/","text":"Outcome incorrectly set by Flank #914 Changelog Date Who? Action 31th July 2020 pawelpasterz created 1st October 2020 adamfilipow92 update Description [task 2020-07-23T20:29:15.152Z] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 [task 2020-07-23T20:29:15.153Z] \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST DETAILS \u2502 [task 2020-07-23T20:29:15.153Z] \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 [task 2020-07-23T20:29:15.153Z] \u2502 flaky \u2502 matrix-1vj2pt9wih182 \u2502 1 test cases failed, 106 passed, 2 flaky \u2502 [task 2020-07-23T20:29:15.153Z] \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 FTL confirmed the matrix outcome is a failure. Flank reported flaky. Steps to reproduce So far no reliable way to reproduce it (with 10/10 rate) found. One can simulate error with the test: SavedMatrixTest#`savedMatrix should have failed outcome when at least one test is failed and the last one is flaky` Due to the incorrectly implemented logic regarding the status update, there were cases when flaky tests obscure failed results. The following must occur: 1. The matrix needs to have both flaky and failed tests (at least one of each) 2. TestExecutions need to be returned in a specific order: 1. Failed execution must be consumed by flank before flaky one 2. The following flaky execution must be actually failed but the outcome of all re-runs is success 3. Then flank does what is the following: 1. The outcome is set to failure when failed test/shards/executions are processed kotlin private fun Outcome?.updateOutcome( when { ... else -> this?.summary // at this point SavedMatrix.outcome == success, flank changes it to failure ... }) 2. When flank reaches case where step summary is failure but execution is success it sets SavedMatrix outcome to flaky kotlin updateOutcome(flakyOutcome = it.step.outcome?.summary != this?.summary) // flakyOutcome == true 3. Due to incorrect order in when condition failure check is never reached kotlin private fun Outcome?.updateOutcome( flakyOutcome: Boolean ) { outcome = when { flakyOutcome -> flaky // flank should escape here with failure status persisted, but since flakyOutcome == true SavedMatrix.outcome is changed to flaky outcome == failure || outcome == inconclusive -> return outcome == flaky -> this?.summary?.takeIf { it == failure || it == inconclusive } else -> this?.summary } ?: outcome } Proposed fix Change order in when condition to always check for failure first kotlin private fun Outcome?.updateOutcome( flakyOutcome: Boolean ) { outcome = when { outcome == failure || outcome == inconclusive -> return // escape when failure/inconclusive outcome is set flakyOutcome -> flaky outcome == flaky -> this?.summary?.takeIf { it == failure || it == inconclusive } else -> this?.summary } ?: outcome } The test below reflects potential but rare behavior. ``kotlin @Test fun savedMatrix should have flaky outcome when at least one test is flaky`() { val expectedOutcome = \"flaky\" val successStepExecution = createStepExecution(1) // success // https://github.com/Flank/flank/issues/918 // This test covers edge case where summary for both step and execution is null and outcome of // saved matrix was not changed and is set to success val malformed = createStepExecution(stepId = -666, executionId = -666) // flaky // below order in the list matters ! val executions = listOf ( successStepExecution , successStepExecution , malformed ) val testMatrix = testMatrix (). apply { testMatrixId = \"123\" state = FINISHED resultStorage = createResultsStorage () testExecutions = executions } val savedMatrix = createSavedMatrix ( testMatrix ) assertEquals ( expectedOutcome , savedMatrix . outcome ) } ``` This issue is not deterministic and really difficult to reproduce by manual tests. The community not reporting this issue for some time.","title":"Outcome incorrectly set by Flank [#914](https://github.com/Flank/flank/issues/914)"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#outcome-incorrectly-set-by-flank-914","text":"","title":"Outcome incorrectly set by Flank #914"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#changelog","text":"Date Who? Action 31th July 2020 pawelpasterz created 1st October 2020 adamfilipow92 update","title":"Changelog"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#description","text":"[task 2020-07-23T20:29:15.152Z] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 [task 2020-07-23T20:29:15.153Z] \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST DETAILS \u2502 [task 2020-07-23T20:29:15.153Z] \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 [task 2020-07-23T20:29:15.153Z] \u2502 flaky \u2502 matrix-1vj2pt9wih182 \u2502 1 test cases failed, 106 passed, 2 flaky \u2502 [task 2020-07-23T20:29:15.153Z] \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 FTL confirmed the matrix outcome is a failure. Flank reported flaky.","title":"Description"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#steps-to-reproduce","text":"So far no reliable way to reproduce it (with 10/10 rate) found. One can simulate error with the test: SavedMatrixTest#`savedMatrix should have failed outcome when at least one test is failed and the last one is flaky` Due to the incorrectly implemented logic regarding the status update, there were cases when flaky tests obscure failed results. The following must occur: 1. The matrix needs to have both flaky and failed tests (at least one of each) 2. TestExecutions need to be returned in a specific order: 1. Failed execution must be consumed by flank before flaky one 2. The following flaky execution must be actually failed but the outcome of all re-runs is success 3. Then flank does what is the following: 1. The outcome is set to failure when failed test/shards/executions are processed kotlin private fun Outcome?.updateOutcome( when { ... else -> this?.summary // at this point SavedMatrix.outcome == success, flank changes it to failure ... }) 2. When flank reaches case where step summary is failure but execution is success it sets SavedMatrix outcome to flaky kotlin updateOutcome(flakyOutcome = it.step.outcome?.summary != this?.summary) // flakyOutcome == true 3. Due to incorrect order in when condition failure check is never reached kotlin private fun Outcome?.updateOutcome( flakyOutcome: Boolean ) { outcome = when { flakyOutcome -> flaky // flank should escape here with failure status persisted, but since flakyOutcome == true SavedMatrix.outcome is changed to flaky outcome == failure || outcome == inconclusive -> return outcome == flaky -> this?.summary?.takeIf { it == failure || it == inconclusive } else -> this?.summary } ?: outcome }","title":"Steps to reproduce"},{"location":"bugs/914_falsy_positive_outcome_for_flaky_tests/#proposed-fix","text":"Change order in when condition to always check for failure first kotlin private fun Outcome?.updateOutcome( flakyOutcome: Boolean ) { outcome = when { outcome == failure || outcome == inconclusive -> return // escape when failure/inconclusive outcome is set flakyOutcome -> flaky outcome == flaky -> this?.summary?.takeIf { it == failure || it == inconclusive } else -> this?.summary } ?: outcome } The test below reflects potential but rare behavior. ``kotlin @Test fun savedMatrix should have flaky outcome when at least one test is flaky`() { val expectedOutcome = \"flaky\" val successStepExecution = createStepExecution(1) // success // https://github.com/Flank/flank/issues/918 // This test covers edge case where summary for both step and execution is null and outcome of // saved matrix was not changed and is set to success val malformed = createStepExecution(stepId = -666, executionId = -666) // flaky // below order in the list matters ! val executions = listOf ( successStepExecution , successStepExecution , malformed ) val testMatrix = testMatrix (). apply { testMatrixId = \"123\" state = FINISHED resultStorage = createResultsStorage () testExecutions = executions } val savedMatrix = createSavedMatrix ( testMatrix ) assertEquals ( expectedOutcome , savedMatrix . outcome ) } ``` This issue is not deterministic and really difficult to reproduce by manual tests. The community not reporting this issue for some time.","title":"Proposed fix"},{"location":"bugs/986_test_count_and_smart_sharding_%20count_dont_match/","text":"Test count and smart sharding count don't match Bug reported in this issue The problem Flank does not support parameterized tests sharding. Every class with parameterized is considered as one test during shard calculation. Flank is using DEX parser to decompile apks and gather info about all the tests inside. As for now, Flank is unable to determine how many times a test in a parameterized class is invoked. Due to this fact scans apks for any class with an annotation that contains JUnitParamsRunner or Parameterized : @RunWith ( JUnitParamsRunner :: class ) ... @RunWith ( Parameterized :: class ) Solution Flank knows how many tests and classes are being sent to Firebase. So we can inform the user of how many classes we have. Example: Smart Flank cache hit: 0% (0 / 9) Shard times: 240s, 240s, 240s, 360s Uploading app-debug.apk . Uploading app-multiple-flaky-debug-androidTest.apk . 5 tests + 4 parameterized classes / 4 shards Default test time for classes should be different from the default time for test You can set default test time for class with --default-class-test-time command If you did not set this time, the default value is 240s","title":"Test count and smart sharding count don't match"},{"location":"bugs/986_test_count_and_smart_sharding_%20count_dont_match/#test-count-and-smart-sharding-count-dont-match","text":"Bug reported in this issue","title":"Test count and smart sharding count don't match"},{"location":"bugs/986_test_count_and_smart_sharding_%20count_dont_match/#the-problem","text":"Flank does not support parameterized tests sharding. Every class with parameterized is considered as one test during shard calculation. Flank is using DEX parser to decompile apks and gather info about all the tests inside. As for now, Flank is unable to determine how many times a test in a parameterized class is invoked. Due to this fact scans apks for any class with an annotation that contains JUnitParamsRunner or Parameterized : @RunWith ( JUnitParamsRunner :: class ) ... @RunWith ( Parameterized :: class )","title":"The problem"},{"location":"bugs/986_test_count_and_smart_sharding_%20count_dont_match/#solution","text":"Flank knows how many tests and classes are being sent to Firebase. So we can inform the user of how many classes we have. Example: Smart Flank cache hit: 0% (0 / 9) Shard times: 240s, 240s, 240s, 360s Uploading app-debug.apk . Uploading app-multiple-flaky-debug-androidTest.apk . 5 tests + 4 parameterized classes / 4 shards Default test time for classes should be different from the default time for test You can set default test time for class with --default-class-test-time command If you did not set this time, the default value is 240s","title":"Solution"},{"location":"bugs/993_multiple_identical_lines_printing/","text":"Avoid multiple identical lines printing Related to #993 Sometimes Flank prints identical lines multiple times. This bug occurs rarely, there is no clear way to reproduce or force this by code changes. What was checked: [X] launched flank with a different configuration, different count of matrices [X] on PollMatrices.kt onEach { printMatrixStatus ( it ) } It always executes on the same thread so it is not a concurrency issue [X] on ExecutionStatusPrinter.kt -> MultiLinePrinter try to force remove less lines than output.size but in this case, this line does not update status, so the behaviour is different than on screen [X] on ExecutionStatusPrinter.kt -> MultiLinePrinter try to force add to output two same ExecutionStatus but no effect [X] on GcTestMatrix.kt -> refresh() try to add testSpecyfication with same id but without effect","title":"Avoid multiple identical lines printing"},{"location":"bugs/993_multiple_identical_lines_printing/#avoid-multiple-identical-lines-printing","text":"Related to #993 Sometimes Flank prints identical lines multiple times. This bug occurs rarely, there is no clear way to reproduce or force this by code changes. What was checked: [X] launched flank with a different configuration, different count of matrices [X] on PollMatrices.kt onEach { printMatrixStatus ( it ) } It always executes on the same thread so it is not a concurrency issue [X] on ExecutionStatusPrinter.kt -> MultiLinePrinter try to force remove less lines than output.size but in this case, this line does not update status, so the behaviour is different than on screen [X] on ExecutionStatusPrinter.kt -> MultiLinePrinter try to force add to output two same ExecutionStatus but no effect [X] on GcTestMatrix.kt -> refresh() try to add testSpecyfication with same id but without effect","title":"Avoid multiple identical lines printing"},{"location":"bugs/deviceUsageDuration_always_null/","text":"deviceUsageDuration always null deviceUsageDuration description How much the device resource is used to perform the test. This is the device usage used for billing purpose, which is different from the run_duration, for example, infrastructure failure won't be charged for device usage. PRECONDITION_FAILED will be returned if one attempts to set a device_usage on a step which already has this field set. In response: present if previously set. - In create request: optional - In update request: optional @return value or {@code null} for none Problem description Problem found on pull request: Flank needs to respect the timeout value as that's a cap for billing purposes. #865 deviceUsageDuration still is null even if we testing on blaze plan with free quota spent Next steps In future we should check problem status and if problem is fixed on testlab we should implement it on Flank","title":"deviceUsageDuration always null"},{"location":"bugs/deviceUsageDuration_always_null/#deviceusageduration-always-null","text":"","title":"deviceUsageDuration always null"},{"location":"bugs/deviceUsageDuration_always_null/#deviceusageduration-description","text":"How much the device resource is used to perform the test. This is the device usage used for billing purpose, which is different from the run_duration, for example, infrastructure failure won't be charged for device usage. PRECONDITION_FAILED will be returned if one attempts to set a device_usage on a step which already has this field set. In response: present if previously set. - In create request: optional - In update request: optional @return value or {@code null} for none","title":"deviceUsageDuration description"},{"location":"bugs/deviceUsageDuration_always_null/#problem-description","text":"Problem found on pull request: Flank needs to respect the timeout value as that's a cap for billing purposes. #865 deviceUsageDuration still is null even if we testing on blaze plan with free quota spent","title":"Problem description"},{"location":"bugs/deviceUsageDuration_always_null/#next-steps","text":"In future we should check problem status and if problem is fixed on testlab we should implement it on Flank","title":"Next steps"},{"location":"bugs/null_no_tests_found/","text":"The problem One or more shards failed because there are no test cases inside. This problem encounter a couple times on multi-module tests which use a lot of additional test apks. It was noticed only with the submodule tests. Stack trace java.lang.ClassNotFoundException: Invalid name: no tests found at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:324) at androidx.test.internal.runner.TestLoader.doCreateRunner(TestLoader.java:72) at androidx.test.internal.runner.TestLoader.getRunnersFor(TestLoader.java:104) at androidx.test.internal.runner.TestRequestBuilder.build(TestRequestBuilder.java:793) at androidx.test.runner.AndroidJUnitRunner.buildRequest(AndroidJUnitRunner.java:547) at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:390) at com.work.android.test.view.ViewTestRunner.onStart(ViewTestRunner.kt:25) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1879) Firebase screens References github issue https://github.com/Flank/flank/issues/818 reported for Flank v20.05.2 Reported flank config flank : additional-app-test-apks : - test : feature/1/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/2/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/3/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/4/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/5/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/6/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/7/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/8/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/9/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/10/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/11/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/12/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/13/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/14/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/15/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/16/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/17/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/18/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/19/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/20/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/21/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/22/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/23/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/24/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/25/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/26/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/27/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk legacy-junit-result : 'false' local-result-dir : sub-modules-flank-results max-test-shards : 5 project : project-id-12345 run-timeout : 20m smart-flank-gcs-path : gs://project-id-12345-flank/submodules-timing.xml gcloud : app : ./sample-app/build/outputs/apk/debug/sample-app-tester-debug.apk async : false auto-google-login : false device : - model : NexusLowRes version : 23 environment-variables : clearPackageData : true performance-metrics : false record-video : false results-bucket : sub-modules-flank-results test : feature/28/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk test-targets : - notAnnotation org.junit.Ignore use-orchestrator : true Reported flade config fladle { serviceAccountCredentials = file ( \"pathToCredentials\" ) useOrchestrator = true environmentVariables = [ \"clearPackageData\" : \"true\" ] timeoutMin = 30 recordVideo = true performanceMetrics = false devices = [ [ \"model\" : \"Pixel2\" , \"version\" : \"27\" , \"orientation\" : \"portrait\" , \"locale\" : \"en\" ] ] projectId = \"id\" flankVersion = \"20.05.2\" testShards = 5 flakyTestAttempts = 0 } What we know from report issue Firebase shows error null no tests found Same error occurs with and without test-targets: - notAnnotation org.junit.Ignore With test-targets: - notAnnotation org.junit.Ignore firebase additionally shows One or more shards failed because there are no test cases inside. Please check your sharding configuration. Occurs with custom annotation-based filter. It was noticed only with the multi-module tests. If number of shards > 2 , there is a high chance one of the shards will have no tests. test-targets-always-run is not the case. Duplicated apk names probably are not the case, but we should ensure. How to reproduce it Use additional-test-app-apk option and set different apks with same names from different directories. Apks will overlap on bucket because of names collision. This should give similar result to reported. Proposals ~~Drop ignored tests before shard calculation and use them only for results. https://github.com/Flank/flank/pull/853~~ ~~Apks uploaded to bucket could overlap if has same names, fixing this could help. https://github.com/Flank/flank/pull/854~~ Create multi-module project which will provide many test apks and try to reproduce issue. Ensure that our knowledge about issue in What we know from report issue is correct. Based on: https://stackoverflow.com/questions/39241640/android-instrumented-test-no-tests-found try to make real multi dex app and try trun tests on it Play with @BefeoreClass and @Before annotations (exception on @BeforeClass produce null without test cases on test-lab).","title":"The problem"},{"location":"bugs/null_no_tests_found/#the-problem","text":"One or more shards failed because there are no test cases inside. This problem encounter a couple times on multi-module tests which use a lot of additional test apks. It was noticed only with the submodule tests.","title":"The problem"},{"location":"bugs/null_no_tests_found/#stack-trace","text":"java.lang.ClassNotFoundException: Invalid name: no tests found at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:324) at androidx.test.internal.runner.TestLoader.doCreateRunner(TestLoader.java:72) at androidx.test.internal.runner.TestLoader.getRunnersFor(TestLoader.java:104) at androidx.test.internal.runner.TestRequestBuilder.build(TestRequestBuilder.java:793) at androidx.test.runner.AndroidJUnitRunner.buildRequest(AndroidJUnitRunner.java:547) at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:390) at com.work.android.test.view.ViewTestRunner.onStart(ViewTestRunner.kt:25) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1879)","title":"Stack trace"},{"location":"bugs/null_no_tests_found/#firebase-screens","text":"","title":"Firebase screens"},{"location":"bugs/null_no_tests_found/#references","text":"github issue https://github.com/Flank/flank/issues/818 reported for Flank v20.05.2","title":"References"},{"location":"bugs/null_no_tests_found/#reported-flank-config","text":"flank : additional-app-test-apks : - test : feature/1/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/2/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/3/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/4/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/5/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/6/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/7/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/8/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/9/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/10/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/11/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/12/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/13/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/14/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/15/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/16/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/17/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/18/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/19/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/20/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/21/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/22/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/23/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/24/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/25/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/26/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk - test : feature/27/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk legacy-junit-result : 'false' local-result-dir : sub-modules-flank-results max-test-shards : 5 project : project-id-12345 run-timeout : 20m smart-flank-gcs-path : gs://project-id-12345-flank/submodules-timing.xml gcloud : app : ./sample-app/build/outputs/apk/debug/sample-app-tester-debug.apk async : false auto-google-login : false device : - model : NexusLowRes version : 23 environment-variables : clearPackageData : true performance-metrics : false record-video : false results-bucket : sub-modules-flank-results test : feature/28/build/outputs/apk/androidTest/debug/android-debug-androidTest.apk test-targets : - notAnnotation org.junit.Ignore use-orchestrator : true","title":"Reported flank config"},{"location":"bugs/null_no_tests_found/#reported-flade-config","text":"fladle { serviceAccountCredentials = file ( \"pathToCredentials\" ) useOrchestrator = true environmentVariables = [ \"clearPackageData\" : \"true\" ] timeoutMin = 30 recordVideo = true performanceMetrics = false devices = [ [ \"model\" : \"Pixel2\" , \"version\" : \"27\" , \"orientation\" : \"portrait\" , \"locale\" : \"en\" ] ] projectId = \"id\" flankVersion = \"20.05.2\" testShards = 5 flakyTestAttempts = 0 }","title":"Reported flade config"},{"location":"bugs/null_no_tests_found/#what-we-know-from-report-issue","text":"Firebase shows error null no tests found Same error occurs with and without test-targets: - notAnnotation org.junit.Ignore With test-targets: - notAnnotation org.junit.Ignore firebase additionally shows One or more shards failed because there are no test cases inside. Please check your sharding configuration. Occurs with custom annotation-based filter. It was noticed only with the multi-module tests. If number of shards > 2 , there is a high chance one of the shards will have no tests. test-targets-always-run is not the case. Duplicated apk names probably are not the case, but we should ensure.","title":"What we know from report issue"},{"location":"bugs/null_no_tests_found/#how-to-reproduce-it","text":"Use additional-test-app-apk option and set different apks with same names from different directories. Apks will overlap on bucket because of names collision. This should give similar result to reported.","title":"How to reproduce it"},{"location":"bugs/null_no_tests_found/#proposals","text":"~~Drop ignored tests before shard calculation and use them only for results. https://github.com/Flank/flank/pull/853~~ ~~Apks uploaded to bucket could overlap if has same names, fixing this could help. https://github.com/Flank/flank/pull/854~~ Create multi-module project which will provide many test apks and try to reproduce issue. Ensure that our knowledge about issue in What we know from report issue is correct. Based on: https://stackoverflow.com/questions/39241640/android-instrumented-test-no-tests-found try to make real multi dex app and try trun tests on it Play with @BefeoreClass and @Before annotations (exception on @BeforeClass produce null without test cases on test-lab).","title":"Proposals"},{"location":"bugs/smart_flank_upload_results_rules/","text":"Smart Flank rules of validation result types Flank trying to avoid override smart-flank-gcs-path by different JUnit report type. That's means: If user select in smart-flank-gcs-path command FulJUnitResult.xml and flag --full-junit-result is not set Flank fail with the message smart-flank-gcs-path is set with FullJUnitReport.xml but in this run --full-junit-result is disabled, please set --full-junit-result flag If user set in smart-flank-gcs-path command JUnitResult.xml and flag --full-junit-result is set Flank fails with message smart-flank-gcs-path is set with JUnitReport.xml but in this run --full-junit-result enabled, please turn off --full-junit-result flag If smart-flank-gcs-path is set to a different name than JUnitReport.xml and FullJUnitReport.xml flank not validating report type Flank not validating report type if flag --smart-flank-disable-upload set","title":"Smart Flank rules of validation result types"},{"location":"bugs/smart_flank_upload_results_rules/#smart-flank-rules-of-validation-result-types","text":"Flank trying to avoid override smart-flank-gcs-path by different JUnit report type. That's means: If user select in smart-flank-gcs-path command FulJUnitResult.xml and flag --full-junit-result is not set Flank fail with the message smart-flank-gcs-path is set with FullJUnitReport.xml but in this run --full-junit-result is disabled, please set --full-junit-result flag If user set in smart-flank-gcs-path command JUnitResult.xml and flag --full-junit-result is set Flank fails with message smart-flank-gcs-path is set with JUnitReport.xml but in this run --full-junit-result enabled, please turn off --full-junit-result flag If smart-flank-gcs-path is set to a different name than JUnitReport.xml and FullJUnitReport.xml flank not validating report type Flank not validating report type if flag --smart-flank-disable-upload set","title":"Smart Flank rules of validation result types"},{"location":"feature/ios_test_plans/","text":"Flow Flow starts by parsing .xctestrun file. Search for: __xctestrun_metadata__ key. <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 1 </integer> </dict> FormatVersion: 1 - old version of .xctestrun FormatVersion: 2 - the newest version with test plans If format is different than 1 or 2 throw an error. FormatVersion: 1 Any other key than metadata should have corresponding TestTarget dictionary. In example below EarlGreyExampleSwiftTests has a TestTarget dictionary. <plist version= \"1.0\" > <dict> <key> EarlGreyExampleSwiftTests </key> <dict> <!-- TestTarget --> <key> BlueprintName </key> <string> EarlGreyExampleSwiftTests </string> ... </dict> <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 1 </integer> </dict> </dict> </plist> FormatVersion: 2 In this version, XML contains two keys: TestConfigurations and TestPlan in addition to __xctestrun_metadata__ . TestPlan is just a dictionary containing basic informations about current TestPlan. We can ignore it. So it's excluded from example xml below. TestConfigurations is an array of different test configurations. Test configuration contains name property and array of TestTargets. <plist version= \"1.0\" > <dict> <key> Name </key> <string> pl </string> <!-- Name property --> <key> TestTargets </key> <array> <!-- TestConfigurations --> <dict> <key> BlueprintName </key> <string> UITests </string> <!-- Test target and Test configuration properties go here --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <!-- Test target and Test configuration properties go here --> </dict> </array> </dict> </plist> Each configuration may contain different Environment Variables, languages, regions or any other properties. Those properties are stored under TestTarget. Currently FTL doesn't support specifying TestConfiguration for test execution. If there is more than one configuration FTL will probably choose one arbitrarily. For now Flank will allow specifying which test configuration should run with only-test-configuration argument. Running test plan locally Build Xcode project To build example project run command below. xcodebuild build-for-testing \\ -allowProvisioningUpdates \\ -project \"FlankMultiTestTargetsExample.xcodeproj\" \\ -scheme \"AllTests\" \\ #Scheme should have test plans enabled -derivedDataPath \"build_testplan_device\" \\ -sdk iphoneos | xcpretty This command will generate directory: Debug-iphoneos containing binaries and .xctestrun file for each TestPlan. In this example scheme AllTests has have only one test plan: AllTests with two test configurations: pl and en . Test Plan contains two Test Targets: UITests and SecondUITests Outputted .xctestrun should looks like this: <plist version= \"1.0\" > <dict> <key> TestConfigurations </key> <array> <dict> <key> Name </key> <string> en </string> <key> TestTargets </key> <!-- Test Targets for `en` test configuration --> <array> <dict> <key> BlueprintName </key> <string> UITests </string> <key> TestLanguage </key> <string> en </string> <key> TestRegion </key> <string> GB </string> <!-- Language and region --> <!-- ... Other properties --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <key> TestLanguage </key> <string> en </string> <key> TestRegion </key> <string> GB </string> <!-- Language and region --> <!-- ... Other properties --> </dict> </array> </dict> <dict> <key> Name </key> <string> pl </string> <key> TestTargets </key> <!-- Test Targets for `pl` test configuration --> <array> <dict> <key> BlueprintName </key> <string> UITests </string> <key> TestLanguage </key> <string> pl </string> <key> TestRegion </key> <string> PL </string> <!-- Language and region --> <!-- ... Other properties --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <key> TestLanguage </key> <string> pl </string> <key> TestRegion </key> <string> PL </string> <!-- Language and region --> <!-- ... Other properties --> </dict> </array> </dict> </array> <key> TestPlan </key> <dict> <key> IsDefault </key> <true/> <key> Name </key> <string> AllTests </string> </dict> <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 2 </integer> </dict> </dict> </plist> Running tests on a local device After generating binaries and .xctestrun file we can run tests using command. xcodebuild test-without-building \\ -xctestrun \"build_testplan_device/Build/Products/testrun.xctestrun\" \\ -destination \"platform=iOS,id=00008030-000209DC1A50802E\" \\ -only-test-configuration pl | xcpretty Option: -only-test-configuration pl allows to specify which test configuration should Xcode run.","title":"Flow"},{"location":"feature/ios_test_plans/#flow","text":"Flow starts by parsing .xctestrun file. Search for: __xctestrun_metadata__ key. <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 1 </integer> </dict> FormatVersion: 1 - old version of .xctestrun FormatVersion: 2 - the newest version with test plans If format is different than 1 or 2 throw an error.","title":"Flow"},{"location":"feature/ios_test_plans/#formatversion-1","text":"Any other key than metadata should have corresponding TestTarget dictionary. In example below EarlGreyExampleSwiftTests has a TestTarget dictionary. <plist version= \"1.0\" > <dict> <key> EarlGreyExampleSwiftTests </key> <dict> <!-- TestTarget --> <key> BlueprintName </key> <string> EarlGreyExampleSwiftTests </string> ... </dict> <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 1 </integer> </dict> </dict> </plist>","title":"FormatVersion: 1"},{"location":"feature/ios_test_plans/#formatversion-2","text":"In this version, XML contains two keys: TestConfigurations and TestPlan in addition to __xctestrun_metadata__ . TestPlan is just a dictionary containing basic informations about current TestPlan. We can ignore it. So it's excluded from example xml below. TestConfigurations is an array of different test configurations. Test configuration contains name property and array of TestTargets. <plist version= \"1.0\" > <dict> <key> Name </key> <string> pl </string> <!-- Name property --> <key> TestTargets </key> <array> <!-- TestConfigurations --> <dict> <key> BlueprintName </key> <string> UITests </string> <!-- Test target and Test configuration properties go here --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <!-- Test target and Test configuration properties go here --> </dict> </array> </dict> </plist> Each configuration may contain different Environment Variables, languages, regions or any other properties. Those properties are stored under TestTarget. Currently FTL doesn't support specifying TestConfiguration for test execution. If there is more than one configuration FTL will probably choose one arbitrarily. For now Flank will allow specifying which test configuration should run with only-test-configuration argument.","title":"FormatVersion: 2"},{"location":"feature/ios_test_plans/#running-test-plan-locally","text":"","title":"Running test plan locally"},{"location":"feature/ios_test_plans/#build-xcode-project","text":"To build example project run command below. xcodebuild build-for-testing \\ -allowProvisioningUpdates \\ -project \"FlankMultiTestTargetsExample.xcodeproj\" \\ -scheme \"AllTests\" \\ #Scheme should have test plans enabled -derivedDataPath \"build_testplan_device\" \\ -sdk iphoneos | xcpretty This command will generate directory: Debug-iphoneos containing binaries and .xctestrun file for each TestPlan. In this example scheme AllTests has have only one test plan: AllTests with two test configurations: pl and en . Test Plan contains two Test Targets: UITests and SecondUITests Outputted .xctestrun should looks like this: <plist version= \"1.0\" > <dict> <key> TestConfigurations </key> <array> <dict> <key> Name </key> <string> en </string> <key> TestTargets </key> <!-- Test Targets for `en` test configuration --> <array> <dict> <key> BlueprintName </key> <string> UITests </string> <key> TestLanguage </key> <string> en </string> <key> TestRegion </key> <string> GB </string> <!-- Language and region --> <!-- ... Other properties --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <key> TestLanguage </key> <string> en </string> <key> TestRegion </key> <string> GB </string> <!-- Language and region --> <!-- ... Other properties --> </dict> </array> </dict> <dict> <key> Name </key> <string> pl </string> <key> TestTargets </key> <!-- Test Targets for `pl` test configuration --> <array> <dict> <key> BlueprintName </key> <string> UITests </string> <key> TestLanguage </key> <string> pl </string> <key> TestRegion </key> <string> PL </string> <!-- Language and region --> <!-- ... Other properties --> </dict> <dict> <key> BlueprintName </key> <string> SecondUITests </string> <key> TestLanguage </key> <string> pl </string> <key> TestRegion </key> <string> PL </string> <!-- Language and region --> <!-- ... Other properties --> </dict> </array> </dict> </array> <key> TestPlan </key> <dict> <key> IsDefault </key> <true/> <key> Name </key> <string> AllTests </string> </dict> <key> __xctestrun_metadata__ </key> <dict> <key> FormatVersion </key> <integer> 2 </integer> </dict> </dict> </plist>","title":"Build Xcode project"},{"location":"feature/ios_test_plans/#running-tests-on-a-local-device","text":"After generating binaries and .xctestrun file we can run tests using command. xcodebuild test-without-building \\ -xctestrun \"build_testplan_device/Build/Products/testrun.xctestrun\" \\ -destination \"platform=iOS,id=00008030-000209DC1A50802E\" \\ -only-test-configuration pl | xcpretty Option: -only-test-configuration pl allows to specify which test configuration should Xcode run.","title":"Running tests on a local device"},{"location":"feature/summary_output/","text":"Formatted summary output \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-35czp85w4h3a7 \u2502 greatqlte-26-en-portrait \u2502 20 test cases passed \u2502 \u2502 failure \u2502 matrix-35czp85w4h3a7 \u2502 Nexus6P-26-en-portrait \u2502 1 test cases failed, 16 passed, 3 flaky \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 User scenario As a user I want to see finely formatted summary result at the end of execution output. Motivation Gcloud prints summary output in the table. It looks nice and is readable. Why we wouldn't have same in flank? Possible outputs Numbers are representing OUTCOME column, points are representing TEST DETAILS column. 1. success | flaky * ${1} test cases passed | ${2} skipped | ${3} flakes | (Native crash) | --- 2. failure * ${1} test cases failed | ${2} errors | ${3} passed | ${4} skipped | ${4} flakes | (Native crash) * Application crashed | (Native crash) * Test timed out | (Native crash) * App failed to install | (Native crash) * Unknown failure | (Native crash) 3. inconclusive * Infrastructure failure * Test run aborted by user * Unknown reason 4. skipped * Incompatible device/OS combination * App does not support the device architecture * App does not support the OS version * Unknown reason Implementation details Outcome calculation v2 #919 Because of rate limit issue , the new implementation of test details calculation is based on gcloud's CreateMatrixOutcomeSummaryUsingEnvironments . which was already described in Outcome calculation v1 section. The activity diagram for gcloud The activity diagram for flank Both diagrams are showing slightly different aspects of flow but the main difference between gcloud and flank is that the flank is calculating billable minutes in addition. The billable minutes are able to calculate from list of steps , so while gcloud is fetching steps only when environments are corrupted, the flank always required at least steps . Drawbacks Calculating outcome details basing on steps may not return info about flaky tests. But it is possible to reuse data that is collecting for JUnitReport as alternative way for generating outcome details. It should be delivered in dedicated pull request. Flank is not displaying info about the environment in outcome table this problem is described in #983 issue. Outcome calculation v1 It should be mentioned there are some crucial differences how flank and gcloud calculates outcome value. Gcloud is using following API calls 1. self._client.projects_histories_executions_environments.List(request) 2. self._client.projects_histories_executions_steps.List(request) The first one is default, but if returns any environment without environmentResult.outcome , the second one will be used to obtain steps . Both environemnts and steps can provide outcome . The difference between them is the steps returns success event if tests are flaky . Currently, we don't know why self._client.projects_histories_executions_environments.List(request) may return empty environmentResult.outcome . In difference to gcloud flank uses 3 api call to obtain necessary data 1. TestMatrix - GcTesting.get.projects().testMatrices().get(projectId, testMatrixId) 2. Step - toolsResults.projects().histories().executions().steps().get(projectId, historyId, executionId, stepId) 3. ListTestCasesResponse - toolsResults.projects().histories().executions().steps().testCases().get(projectId, historyId, executionId, stepId) TestMatrix from first call provides ToolResultsStep through TestExecution which is used to obtain arguments for next two calls. This is part of flank legacy. Those api calls provides data for JUnitResult . As JUnitResult contains all data required to generate table output, we can reuse it. In result, we are forced to calculate flaky outcomes on flank site because of step . Probably it is place for little improvement in the future. Test details calculation When flank and gcloud implementations can be slightly different because of programming languages, the logic behind is the mainly same.","title":"Formatted summary output"},{"location":"feature/summary_output/#formatted-summary-output","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 OUTCOME \u2502 MATRIX ID \u2502 TEST AXIS VALUE \u2502 TEST DETAILS \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 success \u2502 matrix-35czp85w4h3a7 \u2502 greatqlte-26-en-portrait \u2502 20 test cases passed \u2502 \u2502 failure \u2502 matrix-35czp85w4h3a7 \u2502 Nexus6P-26-en-portrait \u2502 1 test cases failed, 16 passed, 3 flaky \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Formatted summary output"},{"location":"feature/summary_output/#user-scenario","text":"As a user I want to see finely formatted summary result at the end of execution output.","title":"User scenario"},{"location":"feature/summary_output/#motivation","text":"Gcloud prints summary output in the table. It looks nice and is readable. Why we wouldn't have same in flank?","title":"Motivation"},{"location":"feature/summary_output/#possible-outputs","text":"Numbers are representing OUTCOME column, points are representing TEST DETAILS column. 1. success | flaky * ${1} test cases passed | ${2} skipped | ${3} flakes | (Native crash) | --- 2. failure * ${1} test cases failed | ${2} errors | ${3} passed | ${4} skipped | ${4} flakes | (Native crash) * Application crashed | (Native crash) * Test timed out | (Native crash) * App failed to install | (Native crash) * Unknown failure | (Native crash) 3. inconclusive * Infrastructure failure * Test run aborted by user * Unknown reason 4. skipped * Incompatible device/OS combination * App does not support the device architecture * App does not support the OS version * Unknown reason","title":"Possible outputs"},{"location":"feature/summary_output/#implementation-details","text":"","title":"Implementation details"},{"location":"feature/summary_output/#outcome-calculation-v2-919","text":"Because of rate limit issue , the new implementation of test details calculation is based on gcloud's CreateMatrixOutcomeSummaryUsingEnvironments . which was already described in Outcome calculation v1 section. The activity diagram for gcloud The activity diagram for flank Both diagrams are showing slightly different aspects of flow but the main difference between gcloud and flank is that the flank is calculating billable minutes in addition. The billable minutes are able to calculate from list of steps , so while gcloud is fetching steps only when environments are corrupted, the flank always required at least steps .","title":"Outcome calculation v2 #919"},{"location":"feature/summary_output/#drawbacks","text":"Calculating outcome details basing on steps may not return info about flaky tests. But it is possible to reuse data that is collecting for JUnitReport as alternative way for generating outcome details. It should be delivered in dedicated pull request. Flank is not displaying info about the environment in outcome table this problem is described in #983 issue.","title":"Drawbacks"},{"location":"feature/summary_output/#outcome-calculation-v1","text":"It should be mentioned there are some crucial differences how flank and gcloud calculates outcome value. Gcloud is using following API calls 1. self._client.projects_histories_executions_environments.List(request) 2. self._client.projects_histories_executions_steps.List(request) The first one is default, but if returns any environment without environmentResult.outcome , the second one will be used to obtain steps . Both environemnts and steps can provide outcome . The difference between them is the steps returns success event if tests are flaky . Currently, we don't know why self._client.projects_histories_executions_environments.List(request) may return empty environmentResult.outcome . In difference to gcloud flank uses 3 api call to obtain necessary data 1. TestMatrix - GcTesting.get.projects().testMatrices().get(projectId, testMatrixId) 2. Step - toolsResults.projects().histories().executions().steps().get(projectId, historyId, executionId, stepId) 3. ListTestCasesResponse - toolsResults.projects().histories().executions().steps().testCases().get(projectId, historyId, executionId, stepId) TestMatrix from first call provides ToolResultsStep through TestExecution which is used to obtain arguments for next two calls. This is part of flank legacy. Those api calls provides data for JUnitResult . As JUnitResult contains all data required to generate table output, we can reuse it. In result, we are forced to calculate flaky outcomes on flank site because of step . Probably it is place for little improvement in the future.","title":"Outcome calculation v1"},{"location":"feature/summary_output/#test-details-calculation","text":"When flank and gcloud implementations can be slightly different because of programming languages, the logic behind is the mainly same.","title":"Test details calculation"},{"location":"onboarding/1_environment_setup/","text":"Environment setup This document may be incomplete now or in the future, so if you faced any problems ask the team for help. Mac Install a brew, it's not mandatory but may be convenient for installing other software. Currently, the zsh is the default shell on a mac. If you prefer bash use chsh -s /bin/bash . Env config Bunch of useful exports. You can paste them to your .bashrc FLANK_REPO = \"type path to your local flank repository\" export PATH = $PATH : $HOME / $FLANK_REPO /flank/test_runner/bash export PATH = $PATH : $HOME / $FLANK_REPO /flank/test_projects/android/bash export PATH = $PATH : $HOME /Library/Android/sdk/platform-tools export PATH = $PATH : $HOME /Library/Python/2.7/bin #export PATH=$PATH:$HOME/\"path to your local gcloud repository\"/gcloud_cli/google-cloud-sdk/bin export FLANK_PROJECT_ID = flank-open-source export GOOGLE_CLOUD_PROJECT = flank-open-source export GITHUB_TOKEN = \"type your gihub token here\" Common Ask for access to internal slack channels Ask for an invitation to firebase slack Ask for access to GitHub repo Ask for access to test bucket on a google cloud platform Install zenhub extension to get access to our board. Install Oracle JDK 8 Unfortunately there is no official way to download installer without a login account. Unfortunately unofficial instruction from here sometimes isn't working. Use JetBrains Toolbox to install IDE. Install JetBrains Toolbox download from website or brew cask install jetbrains-toolbox Install IntelliJ idea (community may be enough) Install the Android studio. Setup local flank repository Clone the repo git clone --recursive https://github.com/Flank/flank.git Init submodule git submodule update --init --recursive updates the submodules Build flank running ./update_flank.sh Auth google account Run flank auth login [ ./flank ]. Click on the link and authenticate the account in a browser. Flank will save the credential to ~/.flank. Install gcloud. Be aware gcloud requires a python environment. You can clone https://github.com/Flank/gcloud_cli Or follow official instruction https://cloud.google.com/sdk/docs/quickstarts Don't forget about exports for python and gcloud Ensure Git hooks is configured correctly so detekt is run on a commit Navigate to the .githooks folder Execute linkHooks.sh or linkHooks.bat (depends on the current development OS) Attempting a git commit ... should now trigger a detekt check and will not allow committing until it is successful","title":"Environment setup"},{"location":"onboarding/1_environment_setup/#environment-setup","text":"This document may be incomplete now or in the future, so if you faced any problems ask the team for help.","title":"Environment setup"},{"location":"onboarding/1_environment_setup/#mac","text":"Install a brew, it's not mandatory but may be convenient for installing other software. Currently, the zsh is the default shell on a mac. If you prefer bash use chsh -s /bin/bash .","title":"Mac"},{"location":"onboarding/1_environment_setup/#env-config","text":"Bunch of useful exports. You can paste them to your .bashrc FLANK_REPO = \"type path to your local flank repository\" export PATH = $PATH : $HOME / $FLANK_REPO /flank/test_runner/bash export PATH = $PATH : $HOME / $FLANK_REPO /flank/test_projects/android/bash export PATH = $PATH : $HOME /Library/Android/sdk/platform-tools export PATH = $PATH : $HOME /Library/Python/2.7/bin #export PATH=$PATH:$HOME/\"path to your local gcloud repository\"/gcloud_cli/google-cloud-sdk/bin export FLANK_PROJECT_ID = flank-open-source export GOOGLE_CLOUD_PROJECT = flank-open-source export GITHUB_TOKEN = \"type your gihub token here\"","title":"Env config"},{"location":"onboarding/1_environment_setup/#common","text":"Ask for access to internal slack channels Ask for an invitation to firebase slack Ask for access to GitHub repo Ask for access to test bucket on a google cloud platform Install zenhub extension to get access to our board. Install Oracle JDK 8 Unfortunately there is no official way to download installer without a login account. Unfortunately unofficial instruction from here sometimes isn't working. Use JetBrains Toolbox to install IDE. Install JetBrains Toolbox download from website or brew cask install jetbrains-toolbox Install IntelliJ idea (community may be enough) Install the Android studio. Setup local flank repository Clone the repo git clone --recursive https://github.com/Flank/flank.git Init submodule git submodule update --init --recursive updates the submodules Build flank running ./update_flank.sh Auth google account Run flank auth login [ ./flank ]. Click on the link and authenticate the account in a browser. Flank will save the credential to ~/.flank. Install gcloud. Be aware gcloud requires a python environment. You can clone https://github.com/Flank/gcloud_cli Or follow official instruction https://cloud.google.com/sdk/docs/quickstarts Don't forget about exports for python and gcloud Ensure Git hooks is configured correctly so detekt is run on a commit Navigate to the .githooks folder Execute linkHooks.sh or linkHooks.bat (depends on the current development OS) Attempting a git commit ... should now trigger a detekt check and will not allow committing until it is successful","title":"Common"},{"location":"onboarding/2_contribution/","text":"Contribution process This document describes contribution details for full-time contributors. Daily community monitoring Check if there are any new critical issues in reported issues . Check if there are any new issues on the flank slack channel . This is good practice to check out those points once a day by someone from the team. Looking for a new task Perform daily monitoring if needed. Ask the team if someone needs help. We prefer teamwork over solo-work, our goal is quality so verification is important. Check flank zenhub board . Typically, the most important issues are prioritized descending in the Ranked column. Estimating tasks In the flank, we are using 3 points scale for estimates tasks complexity (snake, tiger, dragon). For more details see estimation.md Working on a new task Perform estimation if needed. If the task looks complex it's good practice asking a team for help. Typically, estimates should be already done on a weekly review, but issues reported after review may have a lack of estimation. According to task complexity choose the best strategy. Snakes Snake is easy and don't need any explanations. So if the task requires some implementation, everything that you need is a new branch, pull request with proper description and some tests. After you finished don't forget to mention someone from the team for help. Tigers If you are starting work on a tiger, always consider sharing some work with someone from the team, for more details see Buddy system section. Typically, every tiger brings some common things to do like: * Pull request with proper description which typically may contain the following information: * description * user story * definition of done * how to reproduce * Documentation. for more details see the Documentation section. * Unit tests * Dedicated YAML config with required assets for manual or integration testing Dragons Dragon requires research, so you should always start from writing some documentations draft. This draft should contain any important pieces of information about the task and should be synchronized with the status of knowledge according to any progression. The dragon task may be started by one developer but shouldn't be handle alone too long. The task could be a dragon as long as it's unknown and mysterious when there is a plan on how to deal with it, it's automatically become one or many tigers. So the main goal when deal with a dragon is to prepare documentation on how to solve the problem. Documentation Be aware, some tasks sometimes couldn't be resolved for many reasons, so it's really important to have documentation always up to date. Having documentation up to date gives the ability to drop work at any moment and back to in the future without loss of any information. Buddy system See buddy_system.md to read about team work in a flank.","title":"Contribution"},{"location":"onboarding/2_contribution/#contribution-process","text":"This document describes contribution details for full-time contributors.","title":"Contribution process"},{"location":"onboarding/2_contribution/#daily-community-monitoring","text":"Check if there are any new critical issues in reported issues . Check if there are any new issues on the flank slack channel . This is good practice to check out those points once a day by someone from the team.","title":"Daily community monitoring"},{"location":"onboarding/2_contribution/#looking-for-a-new-task","text":"Perform daily monitoring if needed. Ask the team if someone needs help. We prefer teamwork over solo-work, our goal is quality so verification is important. Check flank zenhub board . Typically, the most important issues are prioritized descending in the Ranked column.","title":"Looking for a new task"},{"location":"onboarding/2_contribution/#estimating-tasks","text":"In the flank, we are using 3 points scale for estimates tasks complexity (snake, tiger, dragon). For more details see estimation.md","title":"Estimating tasks"},{"location":"onboarding/2_contribution/#working-on-a-new-task","text":"Perform estimation if needed. If the task looks complex it's good practice asking a team for help. Typically, estimates should be already done on a weekly review, but issues reported after review may have a lack of estimation. According to task complexity choose the best strategy.","title":"Working on a new task"},{"location":"onboarding/2_contribution/#snakes","text":"Snake is easy and don't need any explanations. So if the task requires some implementation, everything that you need is a new branch, pull request with proper description and some tests. After you finished don't forget to mention someone from the team for help.","title":"Snakes"},{"location":"onboarding/2_contribution/#tigers","text":"If you are starting work on a tiger, always consider sharing some work with someone from the team, for more details see Buddy system section. Typically, every tiger brings some common things to do like: * Pull request with proper description which typically may contain the following information: * description * user story * definition of done * how to reproduce * Documentation. for more details see the Documentation section. * Unit tests * Dedicated YAML config with required assets for manual or integration testing","title":"Tigers"},{"location":"onboarding/2_contribution/#dragons","text":"Dragon requires research, so you should always start from writing some documentations draft. This draft should contain any important pieces of information about the task and should be synchronized with the status of knowledge according to any progression. The dragon task may be started by one developer but shouldn't be handle alone too long. The task could be a dragon as long as it's unknown and mysterious when there is a plan on how to deal with it, it's automatically become one or many tigers. So the main goal when deal with a dragon is to prepare documentation on how to solve the problem.","title":"Dragons"},{"location":"onboarding/2_contribution/#documentation","text":"Be aware, some tasks sometimes couldn't be resolved for many reasons, so it's really important to have documentation always up to date. Having documentation up to date gives the ability to drop work at any moment and back to in the future without loss of any information.","title":"Documentation"},{"location":"onboarding/2_contribution/#buddy-system","text":"See buddy_system.md to read about team work in a flank.","title":"Buddy system"},{"location":"onboarding/3_estimation/","text":"Estimating Eng Work: Snakes, Tigers, Dragons Introduction This document describes a model for estimating the amount of work an engineering task requires. It is not about estimating how long that work will take \u2014 that is a different discussion for a different time. It is not about how to arrive at these estimates \u2014 that is also a different discussion for a different time. The primary aim of estimating size is to increase the velocity of those doing the work, i.e. engineering team. These estimates might give some amount of signal to other stakeholders, e.g. product management or eng leadership, but that is not the goal. Those stakeholders must be informed by different means. Because these estimates serve the needs of the team, they need to be arrived at by the engineers who are going to be doing the work. Any observer who wishes to adjust the team-arrived estimates needs to sign up for executing those tasks. Snakes A \u201csnake\u201d is a small task that can be executed without interrupting or requiring attention from any other member of the team. Small wordsmithing updates to a design doc, or a runbook are good examples of a snake. Creating a new diff to fix a bug that one noticed while working on something unrelated \u2014 not a snake \u2014 because a diff review will require attention from another team member. Drive-by fixes inside a diff that\u2019s already going out for review can sometimes be a snake, depending on the complexity of the fix. The bottom line is, tasks that are sized as a \u201csnake\u201d do not need to be talked about, they just need to be done. As a corollary, there is no task that is 2 snakes or 7 snakes. It\u2019s either a snake or one of the creatures below. The idea of a \u201csnake\u201d comes from US Army Ranger training. They\u2019re told that if, when moving through an area, they see a snake that presents a danger, they should just kill the snake. They should not yell \u201ccheck this out, SNAKE!\u201d Jim Barksdale , who created two industries , making our jobs today possible, also used the analogy. \u201cDo not have a meeting about a snake. Do not form a committee to discuss a snake. Do not send out a survey about a snake. Just kill it.\u201d Tigers One \u201ctiger\u201d is an amount of work that is well understood by the team. It is a real creature with shared characteristics, so, once you have dispatched one, you have an idea of what it\u2019s like to dispatch another one. The team has to agree on what constitutes one tiger and comparing tigers across teams is a counterproductive exercise. E.g. 1 Tiger is: fixing an NPE (or language-equivalent), writing a unit test to ensure that the fix actually fixed something, reviewing that, landing the diff, following through to ensure it makes it to production. Tasks can be 1, 2, 3, 5, 8, or 13 tigers \u2014 nothing in between. If a task is estimated to be bigger than 13 tigers, it needs to be broken up. Dragons A dragon is a mythical creature; each one is unlike any other encountered before. Not enough is known at estimation time to meaningfully discuss the task and assign a number of tigers. When a team identifies a dragon, the next step is to create a task that generates information required to turn that dragon into tiger-sized tasks (often that de-dragonning task is best done as a time-boxed exercise, but that\u2019s a different discussion for a different time). posted on internal slack channel by @bootstraponline","title":"Estimation"},{"location":"onboarding/3_estimation/#estimating-eng-work-snakes-tigers-dragons","text":"","title":"Estimating Eng Work: Snakes, Tigers, Dragons"},{"location":"onboarding/3_estimation/#introduction","text":"This document describes a model for estimating the amount of work an engineering task requires. It is not about estimating how long that work will take \u2014 that is a different discussion for a different time. It is not about how to arrive at these estimates \u2014 that is also a different discussion for a different time. The primary aim of estimating size is to increase the velocity of those doing the work, i.e. engineering team. These estimates might give some amount of signal to other stakeholders, e.g. product management or eng leadership, but that is not the goal. Those stakeholders must be informed by different means. Because these estimates serve the needs of the team, they need to be arrived at by the engineers who are going to be doing the work. Any observer who wishes to adjust the team-arrived estimates needs to sign up for executing those tasks.","title":"Introduction"},{"location":"onboarding/3_estimation/#snakes","text":"A \u201csnake\u201d is a small task that can be executed without interrupting or requiring attention from any other member of the team. Small wordsmithing updates to a design doc, or a runbook are good examples of a snake. Creating a new diff to fix a bug that one noticed while working on something unrelated \u2014 not a snake \u2014 because a diff review will require attention from another team member. Drive-by fixes inside a diff that\u2019s already going out for review can sometimes be a snake, depending on the complexity of the fix. The bottom line is, tasks that are sized as a \u201csnake\u201d do not need to be talked about, they just need to be done. As a corollary, there is no task that is 2 snakes or 7 snakes. It\u2019s either a snake or one of the creatures below. The idea of a \u201csnake\u201d comes from US Army Ranger training. They\u2019re told that if, when moving through an area, they see a snake that presents a danger, they should just kill the snake. They should not yell \u201ccheck this out, SNAKE!\u201d Jim Barksdale , who created two industries , making our jobs today possible, also used the analogy. \u201cDo not have a meeting about a snake. Do not form a committee to discuss a snake. Do not send out a survey about a snake. Just kill it.\u201d","title":"Snakes"},{"location":"onboarding/3_estimation/#tigers","text":"One \u201ctiger\u201d is an amount of work that is well understood by the team. It is a real creature with shared characteristics, so, once you have dispatched one, you have an idea of what it\u2019s like to dispatch another one. The team has to agree on what constitutes one tiger and comparing tigers across teams is a counterproductive exercise. E.g. 1 Tiger is: fixing an NPE (or language-equivalent), writing a unit test to ensure that the fix actually fixed something, reviewing that, landing the diff, following through to ensure it makes it to production. Tasks can be 1, 2, 3, 5, 8, or 13 tigers \u2014 nothing in between. If a task is estimated to be bigger than 13 tigers, it needs to be broken up.","title":"Tigers"},{"location":"onboarding/3_estimation/#dragons","text":"A dragon is a mythical creature; each one is unlike any other encountered before. Not enough is known at estimation time to meaningfully discuss the task and assign a number of tigers. When a team identifies a dragon, the next step is to create a task that generates information required to turn that dragon into tiger-sized tasks (often that de-dragonning task is best done as a time-boxed exercise, but that\u2019s a different discussion for a different time). posted on internal slack channel by @bootstraponline","title":"Dragons"},{"location":"onboarding/4_buddy_system/","text":"Buddy System Every task should have at least two people. Two people working on the task ensures the task gets to Done quickly. There's no single point of failure. Cross training is another valuable aspect of the buddy system. Code reviews will be faster as the buddy doesn't have to context switch to review the code. Having a buddy working on a task provides emotional support and psychological safety as there's two people responsible for completing the work. Work on one task at a time. Stop starting, start finishing. WIP limits encourage us to finish work that\u2019s already in process before introducing more work into the system. The more work teams try to juggle at once, the harder it is for them to take work to the finish line. https://www.planview.com/resources/articles/wip-limits/ Buddies are empowered to determine how to best collaborate on each task. Ideas: - One writes code, the other reviews. - One writes the business logic, the other writes tests - Pair programming. One writes code, the other observes and guides. Roles can be switched anytime. - Visual Studio Code Live Share - One designs the APIs, the other implements We believe that people thrive on being trusted, on freedom, and on being able to make a difference. So we foster freedom and empowerment wherever we can. https://jobs.netflix.com/culture source https://github.com/platform-platform/monorepo/blob/master/docs/11_collaboration.md","title":"Buddy system"},{"location":"onboarding/4_buddy_system/#buddy-system","text":"Every task should have at least two people. Two people working on the task ensures the task gets to Done quickly. There's no single point of failure. Cross training is another valuable aspect of the buddy system. Code reviews will be faster as the buddy doesn't have to context switch to review the code. Having a buddy working on a task provides emotional support and psychological safety as there's two people responsible for completing the work. Work on one task at a time. Stop starting, start finishing. WIP limits encourage us to finish work that\u2019s already in process before introducing more work into the system. The more work teams try to juggle at once, the harder it is for them to take work to the finish line. https://www.planview.com/resources/articles/wip-limits/ Buddies are empowered to determine how to best collaborate on each task. Ideas: - One writes code, the other reviews. - One writes the business logic, the other writes tests - Pair programming. One writes code, the other observes and guides. Roles can be switched anytime. - Visual Studio Code Live Share - One designs the APIs, the other implements We believe that people thrive on being trusted, on freedom, and on being able to make a difference. So we foster freedom and empowerment wherever we can. https://jobs.netflix.com/culture source https://github.com/platform-platform/monorepo/blob/master/docs/11_collaboration.md","title":"Buddy System"},{"location":"onboarding/5_code_review/","text":"Code review Do your best and do not forget about some good practices, formalized and described below. Be aware Good code review requires a good understood of a problem. Code review is an important part of the development process and delivery. Do code review by steps Read the issue related to the pull request. Read the description of the pull request. Make sure the description is clear for you. Make sure the description is sufficient for you. Read the committed changes Make sure you are ok with implementation. Make sure the result of what changed is clear for you. Make sure the pull request really solves the related issue in the desired way. Make sure the testing scenario is clear for you. Do the tests step by step and collect the output. Make sure the result of tests is equal to expected Attach report about test results under pull request. Notify about fail If any step above will fail for you, try to reproduce it for sure and notify about a faced problem using pull request comment. Additionally, you may attach any of: - Github commitable suggestion if possible, and you already know it. - Description of what is unclear for you. - Description of what should be changed or what is missing. - Any additional information important for final quality. Some tips Make sure you clearly understand what you are reviewing. Don't be afraid of paying attention to details if feel they are important. It's always a good idea to open IDE, try to identify the root, and trace the implementation if the pull request is not trivial. Ask if you are not sure, suggest if you are sure.","title":"Code review"},{"location":"onboarding/5_code_review/#code-review","text":"Do your best and do not forget about some good practices, formalized and described below.","title":"Code review"},{"location":"onboarding/5_code_review/#be-aware","text":"Good code review requires a good understood of a problem. Code review is an important part of the development process and delivery.","title":"Be aware"},{"location":"onboarding/5_code_review/#do-code-review-by-steps","text":"Read the issue related to the pull request. Read the description of the pull request. Make sure the description is clear for you. Make sure the description is sufficient for you. Read the committed changes Make sure you are ok with implementation. Make sure the result of what changed is clear for you. Make sure the pull request really solves the related issue in the desired way. Make sure the testing scenario is clear for you. Do the tests step by step and collect the output. Make sure the result of tests is equal to expected Attach report about test results under pull request.","title":"Do code review by steps"},{"location":"onboarding/5_code_review/#notify-about-fail","text":"If any step above will fail for you, try to reproduce it for sure and notify about a faced problem using pull request comment. Additionally, you may attach any of: - Github commitable suggestion if possible, and you already know it. - Description of what is unclear for you. - Description of what should be changed or what is missing. - Any additional information important for final quality.","title":"Notify about fail"},{"location":"onboarding/5_code_review/#some-tips","text":"Make sure you clearly understand what you are reviewing. Don't be afraid of paying attention to details if feel they are important. It's always a good idea to open IDE, try to identify the root, and trace the implementation if the pull request is not trivial. Ask if you are not sure, suggest if you are sure.","title":"Some tips"}]}